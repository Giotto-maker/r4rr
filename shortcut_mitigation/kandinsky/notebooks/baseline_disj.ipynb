{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "MODEL_PARAMETER_NAME = 'ltn'\n",
    "GPU_ID = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_mnist_add_utils' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import setproctitle, socket, uuid\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "from numpy import float32, zeros\n",
    "from datasets import get_dataset\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from datasets.utils.base_dataset import BaseDataset\n",
    "from torchvision import datasets, transforms\n",
    "from utils import fprint\n",
    "from utils.train import train\n",
    "from utils.test import test\n",
    "from utils.preprocess_resnet import preprocess\n",
    "from utils.conf import *\n",
    "from utils.args import *\n",
    "from utils.status import progress_bar\n",
    "from utils.checkpoint import save_model, create_load_ckpt\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.metrics import (\n",
    "    evaluate_metrics,\n",
    "    evaluate_mix,\n",
    "    mean_entropy,\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from protonet_kand_modules.arguments import args_dpl, args_sl, args_ltn\n",
    "from protonet_kand_modules.utility_modules.check_gpu import my_gpu_info\n",
    "from protonet_kand_modules.data_modules.proto_data_creation import get_support_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save paths: ['../outputs/kand/baseline-kandinsky-single-disj/ltn']\n"
     ]
    }
   ],
   "source": [
    "if MODEL_PARAMETER_NAME == 'dpl':   args = args_dpl\n",
    "elif MODEL_PARAMETER_NAME == 'sl':  args = args_sl\n",
    "else:                               args = args_ltn\n",
    "\n",
    "# saving\n",
    "save_folder = \"kand\" \n",
    "save_model_name = MODEL_PARAMETER_NAME\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\", \"outputs\", \n",
    "    save_folder, \n",
    "    \"baseline-kandinsky-single-disj\", \n",
    "    save_model_name\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "print(f\"Save paths: {str(save_paths)}\")\n",
    "\n",
    "if args.model in ['prokandsl', 'prokandltn', 'prokanddpl'] or args.prototypes:\n",
    "    raise ValueError(\"This experiment is NOT meant for pNet based models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.13.0+cu117\n",
      "CUDA version:  11.7\n",
      "Number of GPUs available: 1\n",
      "Device 0: NVIDIA TITAN Xp\n",
      "  Memory Allocated: 0 bytes\n",
      "  Memory Cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "my_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add uuid, timestamp and hostname for logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "args.GPU_ID = GPU_ID\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypical data loaded\n",
      "Images:  torch.Size([1467, 3, 64, 64])\n",
      "Labels:  torch.Size([1467, 2])\n"
     ]
    }
   ],
   "source": [
    "proto_images = torch.load('../data/kand_annotations/pnet_proto/concept_prototypes.pt')\n",
    "proto_labels = torch.load('../data/kand_annotations/pnet_proto/labels_prototypes.pt')\n",
    "print(\"Prototypical data loaded\")\n",
    "print(\"Images: \", proto_images.shape)\n",
    "print(\"Labels: \", proto_labels.shape)\n",
    "\n",
    "support_loader = get_support_loader(proto_images, proto_labels, query_batch_size=32, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimitivesDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): Tensor of shape [N, 3, 64, 64]\n",
    "            labels (Tensor): Tensor of shape [N, 2] where:\n",
    "                             - labels[:, 0] is the shape label  (0: square, 1: circle, 2: triangle)\n",
    "                             - labels[:, 1] is the colour label (0: red, 1: yellow, 2: blue)\n",
    "            transform: Optional transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels  # shape [N, 2]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        # Return shape label and colour label separately\n",
    "        shape_label = self.labels[index, 0].long()\n",
    "        color_label = self.labels[index, 1].long()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, shape_label.squeeze(), color_label.squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class FixedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset_size, batch_size, iterations):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.iterations):\n",
    "            start = (i * self.batch_size) % self.dataset_size\n",
    "            end = start + self.batch_size\n",
    "            if end <= self.dataset_size:\n",
    "                yield list(range(start, end))\n",
    "            else:\n",
    "                # wrap around if needed\n",
    "                part1 = list(range(start, self.dataset_size))\n",
    "                part2 = list(range(0, end - self.dataset_size))\n",
    "                yield part1 + part2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler(object):\n",
    "    \"\"\"\n",
    "    Yields a batch of indices for episodic training.\n",
    "    At each iteration, it randomly selects 'classes_per_it' classes and then picks\n",
    "    'num_samples' samples for each selected class.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (array-like): 1D array or list of labels for the target task.\n",
    "                                 This should be either the shape labels or the colour labels.\n",
    "            classes_per_it (int): Number of random classes for each iteration.\n",
    "            num_samples (int): Number of samples per class (support + query) in each episode.\n",
    "            iterations (int): Number of iterations (episodes) per epoch.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # Create an index matrix of shape (num_classes, max_samples_in_class)\n",
    "        max_count = max(self.counts)\n",
    "        self.indexes = np.empty((len(self.classes), max_count), dtype=int)\n",
    "        self.indexes.fill(-1)\n",
    "        self.indexes = torch.LongTensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros(len(self.classes), dtype=torch.long)\n",
    "\n",
    "        # Fill in the matrix with indices for each class.\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            # Find the row corresponding to this label\n",
    "            class_idx = (self.classes == label).nonzero(as_tuple=False).item()\n",
    "            # Find the next available column (where the value is -1)\n",
    "            pos = (self.indexes[class_idx] == -1).nonzero(as_tuple=False)[0].item()\n",
    "            self.indexes[class_idx, pos] = idx\n",
    "            self.numel_per_class[class_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a batch of indices for each episode.\n",
    "        \"\"\"\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            batch = torch.LongTensor(cpi * spc)\n",
    "            # Randomly choose 'classes_per_it' classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "            for i, class_idx in enumerate(c_idxs):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "                # Randomly choose 'num_samples' indices for the class\n",
    "                perm = torch.randperm(self.numel_per_class[class_idx])\n",
    "                sample_idxs = perm[:spc]\n",
    "                batch[s] = self.indexes[class_idx, sample_idxs]\n",
    "            # Shuffle the batch indices\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istantiating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes (shape): 200\n",
      "Number of episodes (color): 200\n"
     ]
    }
   ],
   "source": [
    "# Create PrimitvesDataset instance\n",
    "kand_proto_dataset = PrimitivesDataset(proto_images, proto_labels, transform=None)\n",
    "\n",
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "shape_labels = kand_proto_dataset.labels[:, 0].numpy()  # & ok\n",
    "color_labels = kand_proto_dataset.labels[:, 1].numpy()  # & ok\n",
    "\n",
    "# Create episodes sampler for shapes and colours:\n",
    "shape_sampler = PrototypicalBatchSampler(shape_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "color_sampler = PrototypicalBatchSampler(color_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "\n",
    "# Create dataloaders for each primitve\n",
    "episodic_shape_dataloader = DataLoader(kand_proto_dataset, batch_sampler=shape_sampler)\n",
    "episodic_color_dataloader = DataLoader(kand_proto_dataset, batch_sampler=color_sampler)\n",
    "\n",
    "print(f\"Number of episodes (shape): {len(episodic_shape_dataloader)}\")\n",
    "print(f\"Number of episodes (color): {len(episodic_color_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct shape labels: 3\n",
      "Number of distinct color labels: 3\n",
      "Batch images shape: torch.Size([30, 3, 64, 64])\n",
      "Batch shape labels: [0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2]\n",
      "Shape label distribution in batch: Counter({0: 10, 1: 10, 2: 10})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Color-based episodic batch:\n",
      "Batch images shape: torch.Size([30, 3, 64, 64])\n",
      "Batch color labels: [0, 1, 0, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1, 1, 1, 0, 0, 1, 2, 1, 1, 2, 0, 0, 2, 2, 0, 2, 0, 1]\n",
      "Color label distribution in batch: Counter({0: 10, 1: 10, 2: 10})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "num_distinct_shape_labels = np.unique(shape_labels).size\n",
    "\n",
    "# Should have 3 labels for shapes (0: square, 1: circle, 2: triangle) and 3 labels for colours (0: red, 1: yellow, 2: blue)\n",
    "print(f\"Number of distinct shape labels: {num_distinct_shape_labels}\")\n",
    "num_distinct_color_labels = np.unique(color_labels).size\n",
    "print(f\"Number of distinct color labels: {num_distinct_color_labels}\")\n",
    "\n",
    "# Prototypical networks expects nunpy arrays for labels\n",
    "assert isinstance(shape_labels, np.ndarray), \"shape labels should be a numpy.ndarray\"\n",
    "assert isinstance(color_labels, np.ndarray), \"color labels should be a numpy.ndarray\"\n",
    "\n",
    "# Check tensor shapes and values\n",
    "assert kand_proto_dataset.images.shape == (shape_labels.size, 3, 64, 64), \\\n",
    "    \"The shape of kand_proto_dataset.images should be (number of shape labels, 3, 64, 64)\"\n",
    "assert kand_proto_dataset.images.shape == (color_labels.size, 3, 64, 64), \\\n",
    "    \"The shape of kand_proto_dataset.images should be (number of color labels, 3, 64, 64)\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of shape labels, 1)\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of color labels, 1)\"\n",
    "assert kand_proto_dataset.images.min() >= 0 and kand_proto_dataset.images.max() <= 1, \\\n",
    "    \"The values of kand_proto_dataset.images should be between 0 and 1\"\n",
    "assert np.all(np.isin(shape_labels, [0, 1, 2])), \"Shape labels should only contain values 0, 1, or 2\"\n",
    "assert np.all(np.isin(color_labels, [0, 1, 2])), \"Color labels should only contain values 0, 1, or 2\"    \n",
    "\n",
    "\n",
    "for batch in episodic_shape_dataloader:\n",
    "    images, shape_labels_batch, _ = batch\n",
    "    shape_labels_list = shape_labels_batch.tolist()\n",
    "    label_counts = Counter(shape_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch shape labels:\", shape_labels_list)\n",
    "    print(\"Shape label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Shape Label: {shape_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Inspect one batch from the color dataloader\n",
    "print(\"\\nColor-based episodic batch:\")\n",
    "for batch in episodic_color_dataloader:\n",
    "    images, _, color_labels_batch = batch\n",
    "    # We only need the color labels for the color network\n",
    "    color_labels_list = color_labels_batch.tolist()\n",
    "    label_counts = Counter(color_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch color labels:\", color_labels_list)\n",
    "    print(\"Color label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Color Label: {color_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL & UNSUPERVISED DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "kand says Namespace(GPU_ID='1', and_op='Godel', backbone='conceptizer', batch_size=64, beta=0.99, boia_model='ce', boia_ood_knowledge=False, c_sup=0.0, c_sup_ltn=0, checkin=None, checkout=False, classes_per_it=3, concept_extractor_path='ultralytics/finetuned/kand_best_100.pt', conf_host='pssr', conf_jobnum='1b297267-898b-4735-9ad8-f9b3d49b6751', conf_timestamp='2025-09-01 13:39:17.679722', count=30, dataset='kandinsky', debug=False, device=device(type='cuda'), embedding_dim=1024, entity='', entropy=False, exp_decay=0.9, extractor_training_epochs=20, gamma=0.001, imp_op='Prod', iterations=200, joint=False, lr=0.001, model='kandltnsingledisj', n_epochs=40, n_support=75, non_verbose=False, notes=None, num_distinct_labels=3, num_query=5, num_samples=10, num_support=5, or_op='Prod', p=8, patience=5, posthoc=False, preprocess=False, proj_name='', project='Reasoning-Shortcuts', proto_epochs=10, proto_lr=0.001, prototypes=False, prototypical_batch_size=32, prototypical_loss_weight=[1.0], retrain_extractor=False, seeds=[0, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768], splitted=False, task='patterns', tuning=False, use_ood=False, val_metric='accuracy', validate=False, w_c=1, w_h=0.8, w_rec=1, w_sl=10, wandb=None, warmup_steps=0, weight_decay=0.001, which_c=[-1]) None\n",
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "Using Dataset:  <datasets.kandinsky.Kandinsky object at 0x7fe4b05ce910>\n",
      "Number of images:  3\n",
      "Using backbone:  (SingleDisjMLP(\n",
      "  (backbone): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=12288, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      "), SingleDisjMLP(\n",
      "  (backbone): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=12288, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      "))\n",
      "Using Model:  KandLTNSingleDisj(\n",
      "  (encoder): ModuleList(\n",
      "    (0): SingleDisjMLP(\n",
      "      (backbone): Sequential(\n",
      "        (0): Flatten(start_dim=1, end_dim=-1)\n",
      "        (1): Linear(in_features=12288, out_features=256, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=128, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SingleDisjMLP(\n",
      "      (backbone): Sequential(\n",
      "        (0): Flatten(start_dim=1, end_dim=-1)\n",
      "        (1): Linear(in_features=12288, out_features=256, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=128, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Using Loss:  KAND_SAT_AGG()\n",
      "Working with taks:  patterns\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Number of images: \", n_images)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "print(\"Working with taks: \", args.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = os.path.join(os.getcwd(), \"../data/kand_config_yolo.yaml\")\n",
    "my_yolo_project_path = f\"ultralytics-4/\"\n",
    "my_yolo_premodel_path = f\"ultralytics-4/pretrained/yolo11n.pt\"\n",
    "args.yolo_folder = my_yolo_project_path\n",
    "\n",
    "yolo = YOLO(my_yolo_premodel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SingleDisjMLP(\n",
       "    (backbone): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=12288, out_features=256, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=128, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): SingleDisjMLP(\n",
       "    (backbone): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=12288, out_features=256, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=128, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.encoder.eval()\n",
    "    correct_shape = 0\n",
    "    correct_color = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, shape_labels, color_labels in data_loader:\n",
    "            images = images.to(model.device)\n",
    "            shape_labels = shape_labels.to(model.device)\n",
    "            color_labels = color_labels.to(model.device)\n",
    "\n",
    "            # Forward through separate backbones\n",
    "            shape_preds = model.encoder[0](images)\n",
    "            color_preds = model.encoder[1](images)\n",
    "\n",
    "            shape_pred_labels = torch.argmax(shape_preds, dim=1)\n",
    "            color_pred_labels = torch.argmax(color_preds, dim=1)\n",
    "\n",
    "            correct_shape += (shape_pred_labels == shape_labels).sum().item()\n",
    "            correct_color += (color_pred_labels == color_labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "    shape_acc = correct_shape / total\n",
    "    color_acc = correct_color / total\n",
    "    overall_acc = (shape_acc + color_acc) / 2\n",
    "\n",
    "    print(f\"Shape Accuracy: {shape_acc:.4f}\")\n",
    "    print(f\"Color Accuracy: {color_acc:.4f}\")\n",
    "    print(f\"Overall Accuracy: {overall_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start of Training ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-01 | 13:39 ] epoch 9: || loss: 0.00073795"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Accuracy: 0.9850\n",
      "Color Accuracy: 1.0000\n",
      "Overall Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "def pre_train(model, train_loader, args, seed: int = 0):\n",
    "    \n",
    "    # Full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Separate optimizers\n",
    "    enc_opt = torch.optim.Adam(\n",
    "        list(model.encoder[0].parameters()) + list(model.encoder[1].parameters()),\n",
    "        lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.encoder.to(model.device)\n",
    "    model.encoder.train()\n",
    "\n",
    "    shape_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    color_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args.proto_epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            images, shape_labels, color_labels = batch\n",
    "            images = images.to(model.device)\n",
    "            shape_labels = shape_labels.to(model.device)\n",
    "            color_labels = color_labels.to(model.device)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            assert images.shape == torch.Size([batch_size, 3, 64, 64]), \\\n",
    "                f\"Expected shape [{batch_size}, 3, 64, 64], but got {images.shape}\"\n",
    "\n",
    "            enc_opt.zero_grad()\n",
    "\n",
    "            # Forward pass through separate backbones\n",
    "            shape_preds = model.encoder[0](images)\n",
    "            color_preds = model.encoder[1](images)\n",
    "\n",
    "            assert shape_preds.shape == (batch_size, 3), \\\n",
    "                f\"Expected shape_preds ({batch_size}, 3), but got {shape_preds.shape}\"\n",
    "            assert color_preds.shape == (batch_size, 3), \\\n",
    "                f\"Expected color_preds ({batch_size}, 3), but got {color_preds.shape}\"\n",
    "\n",
    "            # Compute losses\n",
    "            loss_shape = shape_loss_fn(shape_preds, shape_labels)\n",
    "            loss_color = color_loss_fn(color_preds, color_labels)\n",
    "            loss = loss_shape + loss_color\n",
    "\n",
    "            loss.backward()\n",
    "            enc_opt.step()\n",
    "\n",
    "            progress_bar(i, len(train_loader), epoch, loss.item())\n",
    "\n",
    "    evaluate_model(model, train_loader)\n",
    "\n",
    "pre_train(model, episodic_shape_dataloader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MnistDPL,\n",
    "    dataset: BaseDataset, \n",
    "    concept_extractor,\n",
    "    concept_extractor_training_path,\n",
    "    concept_extractor_project_path,\n",
    "    transform,\n",
    "    _loss: ADDMNIST_DPL,\n",
    "    args,\n",
    "    save_folder: str,\n",
    "    patience: int = 3\n",
    "    ):\n",
    "    \n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0   # for early stopping\n",
    "    yolo_save_dir = None\n",
    "\n",
    "    model.to(model.device)\n",
    "\n",
    "    train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "    dataset.print_stats()\n",
    "    \n",
    "    # Initialize optimizers and schedulers\n",
    "    shape_optimizer = torch.optim.Adam(model.encoder[0].parameters())\n",
    "    color_optimizer = torch.optim.Adam(model.encoder[1].parameters())\n",
    "    shape_lr_scheduler = torch.optim.lr_scheduler.StepLR(shape_optimizer, step_size=10, gamma=0.5)\n",
    "    color_lr_scheduler = torch.optim.lr_scheduler.StepLR(color_optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "\n",
    "    # * Start of training\n",
    "    for epoch in range(args.proto_epochs + 1):  # first epoch is for determining the baseline accuracy\n",
    "        print(f\"Epoch {epoch+1}/{args.proto_epochs + 1}\")\n",
    "\n",
    "        # ^ PHASE 1: Training the Concept Extractor\n",
    "        print('----------------------------------')\n",
    "        print('--- Concept Extractor Training ---')\n",
    "        if epoch == 0:\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "        else:\n",
    "            assert yolo_save_dir is not None\n",
    "            concept_extractor = YOLO(yolo_save_dir)\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "\n",
    "        # ^ PHASE 2: Main Model Training\n",
    "        ys, y_true, cs, cs_true = None, None, None, None\n",
    "        for i, data in enumerate(train_loader):\n",
    "            \n",
    "            if epoch == 0:\n",
    "                model.eval()\n",
    "                assert not model.training, \"Model should **NOT** be in training mode!\"\n",
    "                assert not model.encoder[0].training, \"Encoder should **NOT** be in training mode!\"\n",
    "                assert not model.encoder[1].training, \"Encoder should **NOT** be in training mode!\"\n",
    "            else:    \n",
    "                model.train()\n",
    "                shape_optimizer.zero_grad()\n",
    "                color_optimizer.zero_grad()\n",
    "                assert model.training, \"Model should be in training mode!\"\n",
    "                assert model.encoder[0].training, \"Shape encoder should be in training mode!\"\n",
    "                assert model.encoder[1].training, \"Color encoder should be in training mode!\"\n",
    "                \n",
    "            images, labels, concepts = data\n",
    "            images, labels, concepts = (\n",
    "                images.to(model.device),\n",
    "                labels.to(model.device),\n",
    "                concepts.to(model.device),\n",
    "            )\n",
    "\n",
    "            out_dict = model(images, concept_extractor, transform, args)\n",
    "            out_dict.update({\"LABELS\": labels, \"CONCEPTS\": concepts})\n",
    "            \n",
    "            loss, losses = _loss(out_dict, args)\n",
    "            loss.backward()\n",
    "            \n",
    "            if epoch != 0:\n",
    "                shape_optimizer.step()\n",
    "                color_optimizer.step()  \n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "\n",
    "        # Step the scheduler (if using)\n",
    "        if epoch != 0:\n",
    "            shape_lr_scheduler.step()\n",
    "            color_lr_scheduler.step()\n",
    "\n",
    "        if \"patterns\" in args.task:\n",
    "            y_true = y_true[:, -1]  # it is the last one\n",
    "\n",
    "        model.eval()\n",
    "        tloss, cacc, yacc, f1 = evaluate_metrics(model, val_loader, args, concept_extractor=concept_extractor, transform=transform)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1)\n",
    "        print()\n",
    "\n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            if best_cacc == 0.0:     print(\"Baseline accuracy has been determined.\")\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "                \n",
    "            # Save the best model and the concept extractor\n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            concept_save_path = os.path.join(os.path.dirname(save_folder), f\"best_{SEED}.pt\")\n",
    "            concept_extractor.save(concept_save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "            print()\n",
    "        \n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training model with seed 1\n",
      "Chosen device: cuda\n",
      "Saving model in folder:  ../outputs/kand/baseline-kandinsky-single-disj/ltn/ltn_1.pth\n",
      "Loaded datasets in 4.185626029968262 s.\n",
      "Len loaders: \n",
      " train: 4000 \n",
      " val: 1000\n",
      " len test: 1000\n",
      "## Statistics ##\n",
      "Train samples 4000\n",
      "Validation samples 1000\n",
      "Test samples 1000\n",
      "\n",
      "--- Start of Training ---\n",
      "\n",
      "Epoch 1/11\n",
      "----------------------------------\n",
      "--- Concept Extractor Training ---\n",
      "New https://pypi.org/project/ultralytics/8.3.190 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks/../data/kand_config_yolo.yaml, degrees=0.0, deterministic=True, device=1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=ultralytics-4/pretrained/yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=ultralytics-4/, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=ultralytics-4/train9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 43.217.5 MB/s, size: 0.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/train/labels.cache... 206 images, 0 backgrounds, 0 corrupt: 100%|| 206/206 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 10.55.4 MB/s, size: 0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/val/labels.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to ultralytics-4/train9/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1multralytics-4/train9\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20     0.205G      1.811      3.509      1.158         92         64: 100%|| 13/13 [00:04<00:00,  3.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36       0.02      0.111     0.0118    0.00837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20     0.205G      1.336      3.228      1.023         86         64: 100%|| 13/13 [00:01<00:00,  6.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 17.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36     0.0357      0.222     0.0407     0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20     0.205G      1.385      2.868      0.986         73         64: 100%|| 13/13 [00:01<00:00,  6.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.114          1      0.141     0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20     0.205G      1.365      2.326     0.9567         71         64: 100%|| 13/13 [00:02<00:00,  6.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36     0.0572          1      0.361      0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20     0.205G      1.329      1.794     0.9455         63         64: 100%|| 13/13 [00:02<00:00,  6.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.804      0.228      0.617      0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20     0.205G      1.287      1.399     0.9322         74         64: 100%|| 13/13 [00:02<00:00,  6.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 16.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.956      0.601      0.961      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20     0.205G      1.289      1.137     0.9221         80         64: 100%|| 13/13 [00:02<00:00,  6.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.899      0.987       0.95      0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20     0.205G      1.237     0.9553     0.9226         76         64: 100%|| 13/13 [00:02<00:00,  5.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36          1      0.994      0.995      0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20     0.205G      1.234     0.8487     0.9128         70         64: 100%|| 13/13 [00:01<00:00,  6.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36       0.97          1      0.985      0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20     0.205G      1.124     0.7878     0.9081         77         64: 100%|| 13/13 [00:02<00:00,  6.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.894          1      0.934      0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20     0.205G     0.8714     0.6818     0.8772         42         64: 100%|| 13/13 [00:03<00:00,  3.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20     0.205G     0.7456     0.6099     0.8804         42         64: 100%|| 13/13 [00:02<00:00,  6.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.999          1      0.995      0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20     0.205G     0.6925      0.578     0.8797         42         64: 100%|| 13/13 [00:02<00:00,  6.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.999          1      0.995       0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20     0.205G     0.6902     0.5765     0.8803         42         64: 100%|| 13/13 [00:02<00:00,  6.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20     0.205G     0.7179     0.5753     0.8663         42         64: 100%|| 13/13 [00:02<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20     0.205G     0.5831     0.5563     0.8729         42         64: 100%|| 13/13 [00:02<00:00,  5.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20     0.205G     0.5919     0.5007     0.8577         42         64: 100%|| 13/13 [00:01<00:00,  7.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20     0.205G     0.5457     0.5091      0.867         42         64: 100%|| 13/13 [00:01<00:00,  7.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20     0.205G     0.5049     0.5086     0.8729         42         64: 100%|| 13/13 [00:02<00:00,  5.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20     0.205G     0.5169     0.5044     0.8681         42         64: 100%|| 13/13 [00:02<00:00,  5.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from ultralytics-4/train9/weights/last.pt, 5.4MB\n",
      "Optimizer stripped from ultralytics-4/train9/weights/best.pt, 5.4MB\n",
      "\n",
      "Validating ultralytics-4/train9/weights/best.pt...\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:1 (NVIDIA TITAN Xp, 12190MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.821\n",
      "Speed: 0.0ms preprocess, 1.7ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1multralytics-4/train9\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-01 | 13:43 ] epoch 0: || loss: 0.84301722"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 88.71853298611111   ACC Y 71.69921875 F1 Y 69.12042317574543\n",
      "\n",
      "Saving...\n",
      "Baseline accuracy has been determined.\n",
      "Saved best model with CACC score: 88.71853298611111\n",
      "\n",
      "Epoch 2/11\n",
      "----------------------------------\n",
      "--- Concept Extractor Training ---\n",
      "New https://pypi.org/project/ultralytics/8.3.190 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks/../data/kand_config_yolo.yaml, degrees=0.0, deterministic=True, device=1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=ultralytics-4/train9/weights/last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=ultralytics-4/, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=ultralytics-4/train10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 49.120.4 MB/s, size: 0.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/train/labels.cache... 206 images, 0 backgrounds, 0 corrupt: 100%|| 206/206 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.03.0 MB/s, size: 0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/val/labels.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to ultralytics-4/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1multralytics-4/train10\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20     0.281G     0.9854     0.6401     0.9101         92         64: 100%|| 13/13 [00:04<00:00,  3.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995        0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20     0.281G      1.022     0.6268     0.9121         86         64: 100%|| 13/13 [00:02<00:00,  5.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36       0.94          1      0.981       0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20     0.281G     0.9516     0.5881     0.9025         73         64: 100%|| 13/13 [00:02<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.947      0.995      0.992      0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20     0.281G      1.028     0.5699     0.8995         71         64: 100%|| 13/13 [00:02<00:00,  5.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.994          1      0.995      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20     0.281G     0.9784     0.5636     0.8998         63         64: 100%|| 13/13 [00:02<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.997          1      0.995      0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m save_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEED\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model in folder: \u001b[39m\u001b[38;5;124m\"\u001b[39m, save_folder)\n\u001b[0;32m----> 8\u001b[0m best_cacc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# yolo model\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# yolo training data path\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor_project_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_yolo_project_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# yolo project path\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# resizer     \u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m save_model(model, args, SEED)  \u001b[38;5;66;03m# save the model parameters\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Finished training model with seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEED\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [18], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, concept_extractor, concept_extractor_training_path, concept_extractor_project_path, transform, _loss, args, save_folder, patience)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m yolo_save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     concept_extractor \u001b[38;5;241m=\u001b[39m YOLO(yolo_save_dir)\n\u001b[0;32m---> 46\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mconcept_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcept_extractor_training_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractor_training_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcept_extractor_project_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     yolo_save_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ^ PHASE 2: Main Model Training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/model.py:793\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/trainer.py:212\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/trainer.py:386\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    385\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 386\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/tasks.py:300\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m--> 300\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/tasks.py:115\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/tasks.py:133\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/tasks.py:154\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    155\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:301\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 301\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:301\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 301\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:476\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py:79\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1_scores = dict()\n",
    "print(f\"*** Training model with seed {SEED}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{SEED}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "best_cacc = train(model=model,\n",
    "    dataset=dataset,\n",
    "    concept_extractor=yolo,                              # yolo model\n",
    "    concept_extractor_training_path=yaml_path,           # yolo training data path\n",
    "    concept_extractor_project_path=my_yolo_project_path, # yolo project path\n",
    "    transform=T.Resize((64, 64)),                        # resizer     \n",
    "    _loss=loss,\n",
    "    args=args,\n",
    "    save_folder=save_folder,\n",
    ")\n",
    "save_model(model, args, SEED)  # save the model parameters\n",
    "\n",
    "print(f\"*** Finished training model with seed {SEED}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best CACC score: {best_cacc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
