{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "MODEL_PARAMETER_NAME = 'ltn'\n",
    "GPU_ID = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_mnist_add_utils' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import setproctitle, socket, uuid\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "from numpy import float32, zeros\n",
    "from datasets import get_dataset\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from datasets.utils.base_dataset import BaseDataset\n",
    "from torchvision import datasets, transforms\n",
    "from utils import fprint\n",
    "from utils.train import train\n",
    "from utils.test import test\n",
    "from utils.preprocess_resnet import preprocess\n",
    "from utils.conf import *\n",
    "from utils.args import *\n",
    "from utils.status import progress_bar\n",
    "from utils.checkpoint import save_model, create_load_ckpt\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.metrics import (\n",
    "    evaluate_metrics,\n",
    "    evaluate_mix,\n",
    "    mean_entropy,\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from protonet_kand_modules.arguments import args_dpl, args_sl, args_ltn\n",
    "from protonet_kand_modules.utility_modules.check_gpu import my_gpu_info\n",
    "from protonet_kand_modules.data_modules.proto_data_creation import get_support_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PARAMETER_NAME == 'dpl':   args = args_dpl\n",
    "elif MODEL_PARAMETER_NAME == 'sl':  args = args_sl\n",
    "else:                               args = args_ltn\n",
    "\n",
    "# saving\n",
    "save_folder = \"kand\" \n",
    "save_model_name = MODEL_PARAMETER_NAME\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\", \"outputs\", \n",
    "    save_folder, \n",
    "    \"baseline-kandinsky-single-joint\", \n",
    "    save_model_name\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "print(f\"Save paths: {str(save_paths)}\")\n",
    "\n",
    "if args.model in ['prokandsl', 'prokandltn', 'prokanddpl'] or args.prototypes:\n",
    "    raise ValueError(\"This experiment is NOT meant for pNet based models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add uuid, timestamp and hostname for logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "args.GPU_ID = GPU_ID\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_images = torch.load('../data/kand_annotations/pnet_proto/concept_prototypes.pt')\n",
    "proto_labels = torch.load('../data/kand_annotations/pnet_proto/labels_prototypes.pt')\n",
    "print(\"Prototypical data loaded\")\n",
    "print(\"Images: \", proto_images.shape)\n",
    "print(\"Labels: \", proto_labels.shape)\n",
    "\n",
    "support_loader = get_support_loader(proto_images, proto_labels, query_batch_size=32, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimitivesDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): Tensor of shape [N, 3, 64, 64]\n",
    "            labels (Tensor): Tensor of shape [N, 2] where:\n",
    "                             - labels[:, 0] is the shape label  (0: square, 1: circle, 2: triangle)\n",
    "                             - labels[:, 1] is the colour label (0: red, 1: yellow, 2: blue)\n",
    "            transform: Optional transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels  # shape [N, 2]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        # Return shape label and colour label separately\n",
    "        shape_label = self.labels[index, 0].long()\n",
    "        color_label = self.labels[index, 1].long()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, shape_label.squeeze(), color_label.squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class FixedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset_size, batch_size, iterations):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.iterations):\n",
    "            start = (i * self.batch_size) % self.dataset_size\n",
    "            end = start + self.batch_size\n",
    "            if end <= self.dataset_size:\n",
    "                yield list(range(start, end))\n",
    "            else:\n",
    "                # wrap around if needed\n",
    "                part1 = list(range(start, self.dataset_size))\n",
    "                part2 = list(range(0, end - self.dataset_size))\n",
    "                yield part1 + part2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler(object):\n",
    "    \"\"\"\n",
    "    Yields a batch of indices for episodic training.\n",
    "    At each iteration, it randomly selects 'classes_per_it' classes and then picks\n",
    "    'num_samples' samples for each selected class.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (array-like): 1D array or list of labels for the target task.\n",
    "                                 This should be either the shape labels or the colour labels.\n",
    "            classes_per_it (int): Number of random classes for each iteration.\n",
    "            num_samples (int): Number of samples per class (support + query) in each episode.\n",
    "            iterations (int): Number of iterations (episodes) per epoch.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # Create an index matrix of shape (num_classes, max_samples_in_class)\n",
    "        max_count = max(self.counts)\n",
    "        self.indexes = np.empty((len(self.classes), max_count), dtype=int)\n",
    "        self.indexes.fill(-1)\n",
    "        self.indexes = torch.LongTensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros(len(self.classes), dtype=torch.long)\n",
    "\n",
    "        # Fill in the matrix with indices for each class.\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            # Find the row corresponding to this label\n",
    "            class_idx = (self.classes == label).nonzero(as_tuple=False).item()\n",
    "            # Find the next available column (where the value is -1)\n",
    "            pos = (self.indexes[class_idx] == -1).nonzero(as_tuple=False)[0].item()\n",
    "            self.indexes[class_idx, pos] = idx\n",
    "            self.numel_per_class[class_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a batch of indices for each episode.\n",
    "        \"\"\"\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            batch = torch.LongTensor(cpi * spc)\n",
    "            # Randomly choose 'classes_per_it' classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "            for i, class_idx in enumerate(c_idxs):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "                # Randomly choose 'num_samples' indices for the class\n",
    "                perm = torch.randperm(self.numel_per_class[class_idx])\n",
    "                sample_idxs = perm[:spc]\n",
    "                batch[s] = self.indexes[class_idx, sample_idxs]\n",
    "            # Shuffle the batch indices\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istantiating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PrimitvesDataset instance\n",
    "kand_proto_dataset = PrimitivesDataset(proto_images, proto_labels, transform=None)\n",
    "\n",
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "shape_labels = kand_proto_dataset.labels[:, 0].numpy()  # & ok\n",
    "color_labels = kand_proto_dataset.labels[:, 1].numpy()  # & ok\n",
    "\n",
    "# Create episodes sampler for shapes and colours:\n",
    "shape_sampler = PrototypicalBatchSampler(shape_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "color_sampler = PrototypicalBatchSampler(color_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "\n",
    "# Create dataloaders for each primitve\n",
    "episodic_shape_dataloader = DataLoader(kand_proto_dataset, batch_sampler=shape_sampler)\n",
    "episodic_color_dataloader = DataLoader(kand_proto_dataset, batch_sampler=color_sampler)\n",
    "\n",
    "print(f\"Number of episodes (shape): {len(episodic_shape_dataloader)}\")\n",
    "print(f\"Number of episodes (color): {len(episodic_color_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "num_distinct_shape_labels = np.unique(shape_labels).size\n",
    "\n",
    "# Should have 3 labels for shapes (0: square, 1: circle, 2: triangle) and 3 labels for colours (0: red, 1: yellow, 2: blue)\n",
    "print(f\"Number of distinct shape labels: {num_distinct_shape_labels}\")\n",
    "num_distinct_color_labels = np.unique(color_labels).size\n",
    "print(f\"Number of distinct color labels: {num_distinct_color_labels}\")\n",
    "\n",
    "# Prototypical networks expects nunpy arrays for labels\n",
    "assert isinstance(shape_labels, np.ndarray), \"shape labels should be a numpy.ndarray\"\n",
    "assert isinstance(color_labels, np.ndarray), \"color labels should be a numpy.ndarray\"\n",
    "\n",
    "# Check tensor shapes and values\n",
    "assert kand_proto_dataset.images.shape == (shape_labels.size, 3, 64, 64), \\\n",
    "    \"The shape of kand_proto_dataset.images should be (number of shape labels, 3, 64, 64)\"\n",
    "assert kand_proto_dataset.images.shape == (color_labels.size, 3, 64, 64), \\\n",
    "    \"The shape of kand_proto_dataset.images should be (number of color labels, 3, 64, 64)\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of shape labels, 1)\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of color labels, 1)\"\n",
    "assert kand_proto_dataset.images.min() >= 0 and kand_proto_dataset.images.max() <= 1, \\\n",
    "    \"The values of kand_proto_dataset.images should be between 0 and 1\"\n",
    "assert np.all(np.isin(shape_labels, [0, 1, 2])), \"Shape labels should only contain values 0, 1, or 2\"\n",
    "assert np.all(np.isin(color_labels, [0, 1, 2])), \"Color labels should only contain values 0, 1, or 2\"    \n",
    "\n",
    "\n",
    "for batch in episodic_shape_dataloader:\n",
    "    images, shape_labels_batch, _ = batch\n",
    "    shape_labels_list = shape_labels_batch.tolist()\n",
    "    label_counts = Counter(shape_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch shape labels:\", shape_labels_list)\n",
    "    print(\"Shape label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Shape Label: {shape_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Inspect one batch from the color dataloader\n",
    "print(\"\\nColor-based episodic batch:\")\n",
    "for batch in episodic_color_dataloader:\n",
    "    images, _, color_labels_batch = batch\n",
    "    # We only need the color labels for the color network\n",
    "    color_labels_list = color_labels_batch.tolist()\n",
    "    label_counts = Counter(color_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch color labels:\", color_labels_list)\n",
    "    print(\"Color label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Color Label: {color_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL & UNSUPERVISED DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "loss = model.get_loss(args)\n",
    "model.start_optim(args)\n",
    "\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Number of images: \", n_images)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "print(\"Working with taks: \", args.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = os.path.join(os.getcwd(), \"../data/kand_config_yolo.yaml\")\n",
    "my_yolo_project_path = f\"ultralytics-4/\"\n",
    "my_yolo_premodel_path = f\"ultralytics-4/pretrained/yolo11n.pt\"\n",
    "args.yolo_folder = my_yolo_project_path\n",
    "\n",
    "yolo = YOLO(my_yolo_premodel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.encoder.eval()  # Switch to evaluation mode\n",
    "    correct_shape = 0\n",
    "    correct_color = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, shape_labels, color_labels in data_loader:\n",
    "            images = images.to(model.device)\n",
    "            shape_labels = shape_labels.to(model.device)\n",
    "            color_labels = color_labels.to(model.device)\n",
    "\n",
    "            preds = model.encoder(images)\n",
    "            shape_preds, color_preds = torch.split(preds, [3, 3], dim=1)\n",
    "\n",
    "            # Get predicted class indices\n",
    "            shape_pred_labels = torch.argmax(shape_preds, dim=1)\n",
    "            color_pred_labels = torch.argmax(color_preds, dim=1)\n",
    "\n",
    "            # Compare with ground truth\n",
    "            correct_shape += (shape_pred_labels == shape_labels).sum().item()\n",
    "            correct_color += (color_pred_labels == color_labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "    shape_acc = correct_shape / total\n",
    "    color_acc = correct_color / total\n",
    "    overall_acc = (shape_acc + color_acc) / 2  # average of both\n",
    "\n",
    "    print(f\"Shape Accuracy: {shape_acc:.4f}\")\n",
    "    print(f\"Color Accuracy: {color_acc:.4f}\")\n",
    "    print(f\"Overall Accuracy: {overall_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(model, train_loader, args, seed: int = 0):\n",
    "\n",
    "    # ^ PHASE 0: PreTraining \n",
    "    # Full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Optimizer for encoder\n",
    "    enc_opt = torch.optim.Adam(\n",
    "        model.encoder.parameters(), \n",
    "        lr=args.lr, \n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.encoder.to(model.device)\n",
    "    model.encoder.train()\n",
    "\n",
    "    # Define separate loss functions for shape and color prediction\n",
    "    shape_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    color_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args.proto_epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            images, shape_labels, color_labels = batch\n",
    "            \n",
    "            # Move to device\n",
    "            images = images.to(model.device)\n",
    "            shape_labels = shape_labels.to(model.device)\n",
    "            color_labels = color_labels.to(model.device)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            # Shape checks\n",
    "            assert images.shape == torch.Size([batch_size, 3, 64, 64]), \\\n",
    "                f\"Expected shape [{batch_size}, 3, 64, 64], but got {images.shape}\"\n",
    "            assert shape_labels.shape == torch.Size([batch_size]), \\\n",
    "                f\"Expected shape [{batch_size}], but got {shape_labels.shape}\"\n",
    "            assert color_labels.shape == torch.Size([batch_size]), \\\n",
    "                f\"Expected shape [{batch_size}], but got {color_labels.shape}\"\n",
    "            \n",
    "            # Zero gradients\n",
    "            enc_opt.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model.encoder(images)\n",
    "            \n",
    "            # Example: preds split into shape and color logits\n",
    "            shape_preds, color_preds = torch.split(preds, [3, 3], dim=1)\n",
    "\n",
    "            # Assertions on output shapes\n",
    "            assert shape_preds.shape == (batch_size, 3), \\\n",
    "                f\"Expected shape_preds ({batch_size}, 3), but got {shape_preds.shape}\"\n",
    "            assert color_preds.shape == (batch_size, 3), \\\n",
    "                f\"Expected color_preds ({batch_size}, 3), but got {color_preds.shape}\"\n",
    "\n",
    "            # Compute separate losses\n",
    "            loss_shape = shape_loss_fn(shape_preds, shape_labels)\n",
    "            loss_color = color_loss_fn(color_preds, color_labels)\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_shape + loss_color\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            enc_opt.step()\n",
    "\n",
    "            # Progress update\n",
    "            progress_bar(i, len(train_loader), epoch, loss.item())\n",
    "\n",
    "    evaluate_model(model, train_loader)\n",
    "\n",
    "\n",
    "pre_train(model, episodic_shape_dataloader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MnistDPL,\n",
    "    dataset: BaseDataset, \n",
    "    concept_extractor,\n",
    "    concept_extractor_training_path,\n",
    "    concept_extractor_project_path,\n",
    "    transform,\n",
    "    _loss: ADDMNIST_DPL,\n",
    "    args,\n",
    "    save_folder: str,\n",
    "    patience: int = 3\n",
    "    ):\n",
    "    \n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0   # for early stopping\n",
    "\n",
    "    model.to(model.device)\n",
    "\n",
    "    train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "    dataset.print_stats()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:   w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "\n",
    "    # default for warm-up\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # * Start of training\n",
    "    for epoch in range(args.proto_epochs + 1):  # first epoch is for determining the baseline accuracy\n",
    "        print(f\"Epoch {epoch+1}/{args.proto_epochs + 1}\")\n",
    "\n",
    "        # ^ PHASE 1: Training the Concept Extractor\n",
    "        print('----------------------------------')\n",
    "        print('--- Concept Extractor Training ---')\n",
    "        if epoch == 0:\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "        else:\n",
    "            assert yolo_save_dir is not None\n",
    "            concept_extractor = YOLO(yolo_save_dir)\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "\n",
    "        # ^ PHASE 2: Main Model Training\n",
    "        ys, y_true, cs, cs_true = None, None, None, None\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            if epoch == 0:\n",
    "                model.eval()\n",
    "                assert not model.training, \"Model should **NOT** be in training mode!\"\n",
    "                assert not model.encoder.training, \"Encoder should **NOT** be in training mode!\"\n",
    "            else:    \n",
    "                model.train()\n",
    "                model.opt.zero_grad()\n",
    "                assert model.training, \"Model should be in training mode!\"\n",
    "                assert model.encoder.training, \"Encoder should be in training mode!\"\n",
    "\n",
    "            images, labels, concepts = data\n",
    "            images, labels, concepts = (\n",
    "                images.to(model.device),\n",
    "                labels.to(model.device),\n",
    "                concepts.to(model.device),\n",
    "            )\n",
    "\n",
    "            out_dict = model(images, concept_extractor, transform, args)\n",
    "            out_dict.update({\"LABELS\": labels, \"CONCEPTS\": concepts})\n",
    "            \n",
    "            loss, losses = _loss(out_dict, args)\n",
    "            loss.backward()\n",
    "            \n",
    "            if epoch != 0:\n",
    "                model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "\n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        if \"patterns\" in args.task:\n",
    "            y_true = y_true[:, -1]  # it is the last one\n",
    "\n",
    "        model.eval()\n",
    "        tloss, cacc, yacc, f1 = evaluate_metrics(model, val_loader, args, concept_extractor=concept_extractor, transform=transform)\n",
    "\n",
    "        # update the (warmup) scheduler at end of the epoch\n",
    "        if epoch < args.warmup_steps:\n",
    "            w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1)\n",
    "        print()\n",
    "\n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            if best_cacc == 0.0:     print(\"Baseline accuracy has been determined.\")\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "                \n",
    "            # Save the best model and the concept extractor\n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            concept_save_path = os.path.join(os.path.dirname(save_folder), f\"best_{SEED}.pt\")\n",
    "            concept_extractor.save(concept_save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "            print()\n",
    "        \n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = dict()\n",
    "print(f\"*** Training model with seed {SEED}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{SEED}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "best_cacc = train(model=model,\n",
    "    dataset=dataset,\n",
    "    concept_extractor=yolo,                              # yolo model\n",
    "    concept_extractor_training_path=yaml_path,           # yolo training data path\n",
    "    concept_extractor_project_path=my_yolo_project_path, # yolo project path\n",
    "    transform=T.Resize((64, 64)),                        # resizer     \n",
    "    _loss=loss,\n",
    "    args=args,\n",
    "    save_folder=save_folder,\n",
    ")\n",
    "save_model(model, args, SEED)  # save the model parameters\n",
    "\n",
    "print(f\"*** Finished training model with seed {SEED}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best CACC score: {best_cacc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
