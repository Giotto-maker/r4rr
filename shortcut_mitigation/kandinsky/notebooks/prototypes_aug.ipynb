{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_mnist_add_utils' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from argparse import Namespace\n",
    "from datasets import get_dataset\n",
    "from datasets.utils.base_dataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fetches the bounding boxes of the initial prototypes for cropping the triplets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proto_imgs_bboxes():\n",
    "    all_bboxes = []\n",
    "    for i in range(3):\n",
    "        filename = f\"data/kand_annotations/init/bboxes_init/image_{i}_annotations.json\"    \n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        all_bboxes.append(data[\"boxes\"])\n",
    "\n",
    "    bbox_tensor = torch.tensor(all_bboxes, dtype=torch.float32)  # Shape: (27, 3, 4)\n",
    "    assert bbox_tensor.shape == (3, 3, 4), bbox_tensor.shape\n",
    "    return bbox_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plots the initial prototypes with the bounding boxes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the initial prototypes with their bounding boxes\n",
    "def plot_prototypes(proto_imgs_shapes, proto_labels_shapes, bbox_tensor):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns for 3 images\n",
    "    for idx in range(3):\n",
    "        ax = axs[idx]\n",
    "        img_np = proto_imgs_shapes[idx].permute(1, 2, 0).cpu().numpy()\n",
    "        boxes = bbox_tensor[idx].cpu().numpy()  # Shape: (3, 4)\n",
    "        ax.imshow(img_np)\n",
    "        for box in boxes:\n",
    "            x, y, w, h = box\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        ax.axis('off')        \n",
    "        ax.set_title(f\"{proto_labels_shapes[idx].tolist()}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plots the atomic concept images augmented through anti aliasing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_results_with_antialiasing(original_tensor, augmented_tensor, augmented_labels, white_thresholds):\n",
    "    \"\"\"\n",
    "    Plots the original images and the augmented images for each white threshold.\n",
    "    \n",
    "    The first row contains the original images.\n",
    "    Each subsequent row contains the augmented images for one white threshold.\n",
    "    \n",
    "    Args:\n",
    "        original_tensor (Tensor): Original images of shape (9, 3, 64, 64).\n",
    "        augmented_tensor (Tensor): Augmented images of shape (T*9, 3, 64, 64), where T is the \n",
    "                                     number of white_threshold values.\n",
    "        augmented_labels (Tensor): Corresponding labels of shape (T*9, 2).\n",
    "        white_thresholds (list or iterable): The list of white thresholds used for augmentation.\n",
    "    \"\"\"\n",
    "    T = len(white_thresholds)    # Number of threshold values applied\n",
    "    N = original_tensor.shape[0] # 9 images\n",
    "    total_rows = 1 + T           # First row for originals, then one row per threshold\n",
    "    \n",
    "    fig, axes = plt.subplots(total_rows, N, figsize=(3 * N, 3 * total_rows))\n",
    "    \n",
    "    # Plot the original images (first row)\n",
    "    for j in range(N):\n",
    "        ax = axes[0, j] if total_rows > 1 else axes[j]\n",
    "        img_orig = original_tensor[j].permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_orig)\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Reshape augmented_tensor and augmented_labels from (T*9, 3, 64, 64) to (T, 9, 3, 64, 64)\n",
    "    augmented_tensor = augmented_tensor.view(T, N, 3, 64, 64)\n",
    "    augmented_labels = augmented_labels.view(T, N, 2)\n",
    "    \n",
    "    # Plot each augmentation row\n",
    "    for t in range(T):\n",
    "        for j in range(N):\n",
    "            ax = axes[t + 1, j]\n",
    "            img_aug = augmented_tensor[t, j].permute(1, 2, 0).cpu().numpy()\n",
    "            label = augmented_labels[t, j].tolist()\n",
    "            ax.imshow(img_aug)\n",
    "            ax.set_title(f\"Threshold {white_thresholds[t]}, Labels: {label}\")\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plots the atomic concept images augmented through scalings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_results_with_scaling(original_tensor, augmented_tensor, augmented_labels, scaling_factors):\n",
    "    \"\"\"\n",
    "    Plots the original images and the augmented (scaled) images for each scaling factor.\n",
    "    \n",
    "    The first row contains the original images.\n",
    "    Each subsequent row contains the augmented images for one scaling factor.\n",
    "    \n",
    "    Args:\n",
    "        original_tensor (Tensor): Original images of shape (N, 3, 64, 64).\n",
    "        augmented_tensor (Tensor): Augmented images of shape (T*N, 3, 64, 64), where T is the number of scaling factors.\n",
    "        augmented_labels (Tensor): Corresponding labels of shape (T*N, ?).\n",
    "        scaling_factors (list): List of scaling factors used for augmentation.\n",
    "    \"\"\"\n",
    "    T = len(scaling_factors)    # Number of scaling factors\n",
    "    N = original_tensor.shape[0]  # Number of images\n",
    "    total_rows = 1 + T           # First row for originals, then one row per factor\n",
    "    \n",
    "    fig, axes = plt.subplots(total_rows, N, figsize=(3 * N, 3 * total_rows))\n",
    "    \n",
    "    # Plot the original images (first row)\n",
    "    for j in range(N):\n",
    "        ax = axes[0, j] if total_rows > 1 else axes[j]\n",
    "        img_orig = original_tensor[j].permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_orig)\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Reshape augmented_tensor and augmented_labels from (T*N, 3, 64, 64) to (T, N, 3, 64, 64)\n",
    "    augmented_tensor = augmented_tensor.view(T, N, 3, 64, 64)\n",
    "    # Labels are reshaped accordingly (if they have more than one element, adjust accordingly)\n",
    "    augmented_labels = augmented_labels.view(T, N, -1)\n",
    "    \n",
    "    # Plot each augmentation row\n",
    "    for t in range(T):\n",
    "        for j in range(N):\n",
    "            ax = axes[t + 1, j]\n",
    "            img_aug = augmented_tensor[t, j].permute(1, 2, 0).cpu().numpy()\n",
    "            label = augmented_labels[t, j].tolist()\n",
    "            ax.imshow(img_aug)\n",
    "            ax.set_title(f\"Label: {label}\")\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plots the atomic concept images augmented through translations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_results_with_translation(original_tensor, augmented_tensor, augmented_labels, translation_offsets):\n",
    "    \"\"\"\n",
    "    Plots the original images and the augmented (translated) images for each translation offset.\n",
    "    \n",
    "    The first row contains the original images.\n",
    "    Each subsequent row contains the augmented images for one translation offset.\n",
    "    \n",
    "    Args:\n",
    "        original_tensor (Tensor): Original images of shape (N, 3, 64, 64).\n",
    "        augmented_tensor (Tensor): Augmented images of shape (T*N, 3, 64, 64), where T is the number of translation offsets.\n",
    "        augmented_labels (Tensor): Corresponding labels of shape (T*N, ?).\n",
    "        translation_offsets (list): List of (dx, dy) tuples used for augmentation.\n",
    "    \"\"\"\n",
    "    T = len(translation_offsets)  # Number of translation offsets\n",
    "    N = original_tensor.shape[0]  # Number of images\n",
    "    total_rows = 1 + T           # First row for originals, then one row per offset\n",
    "    \n",
    "    fig, axes = plt.subplots(total_rows, N, figsize=(3 * N, 3 * total_rows))\n",
    "    \n",
    "    # Plot the original images (first row)\n",
    "    for j in range(N):\n",
    "        ax = axes[0, j] if total_rows > 1 else axes[j]\n",
    "        img_orig = original_tensor[j].permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_orig)\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Reshape augmented_tensor and augmented_labels from (T*N, 3, 64, 64) to (T, N, 3, 64, 64)\n",
    "    augmented_tensor = augmented_tensor.view(T, N, 3, 64, 64)\n",
    "    augmented_labels = augmented_labels.view(T, N, -1)\n",
    "    \n",
    "    # Plot each augmentation row\n",
    "    for t in range(T):\n",
    "        for j in range(N):\n",
    "            ax = axes[t + 1, j]\n",
    "            img_aug = augmented_tensor[t, j].permute(1, 2, 0).cpu().numpy()\n",
    "            label = augmented_labels[t, j].tolist()\n",
    "            dx, dy = translation_offsets[t]\n",
    "            ax.imshow(img_aug)\n",
    "            ax.set_title(f\"Trans ({dx}, {dy}), Label: {label}\")\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plots aggregated concepts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_boxes(image, boxes, ax):\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    ax.imshow(image, origin='upper', extent=[0, w, h, 0])\n",
    "    ax.set_xlim(0, w)\n",
    "    ax.set_ylim(h, 0)\n",
    "    \n",
    "    for box in boxes:\n",
    "        x, y, w_box, h_box = box\n",
    "        rect = patches.Rectangle((x, y), w_box, h_box,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translate aggregated concepts and their bounding boxes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_bounding_boxes_and_image(image, boxes, desired_translation):\n",
    "    h, w = image.shape[:2] \n",
    "    allowed_tx_min = []\n",
    "    allowed_tx_max = []\n",
    "    allowed_ty_min = []\n",
    "    allowed_ty_max = []\n",
    "    \n",
    "    for (x, y, w_box, h_box) in boxes:\n",
    "        allowed_tx_min.append(-x)\n",
    "        allowed_tx_max.append(w - (x + w_box))\n",
    "        \n",
    "        allowed_ty_min.append(-y)\n",
    "        allowed_ty_max.append(h - (y + h_box))\n",
    "    \n",
    "    global_tx_min = max(allowed_tx_min)\n",
    "    global_tx_max = min(allowed_tx_max)   \n",
    "    global_ty_min = max(allowed_ty_min)\n",
    "    global_ty_max = min(allowed_ty_max)\n",
    "    \n",
    "    tx_desired, ty_desired = desired_translation\n",
    "    tx = min(max(tx_desired, global_tx_min), global_tx_max)\n",
    "    ty = min(max(ty_desired, global_ty_min), global_ty_max)\n",
    "    \n",
    "    translated_image = np.ones_like(image)\n",
    "    \n",
    "    translated_boxes = []\n",
    "    for (x, y, w_box, h_box) in boxes:\n",
    "        shape_crop = image[y:y + h_box, x:x + w_box]\n",
    "        new_x = x + tx\n",
    "        new_y = y + ty\n",
    "        int_new_x = int(round(new_x))\n",
    "        int_new_y = int(round(new_y))\n",
    "        int_new_x = max(0, min(int_new_x, w - w_box))\n",
    "        int_new_y = max(0, min(int_new_y, h - h_box))\n",
    "        translated_image[int_new_y:int_new_y + h_box, int_new_x:int_new_x + w_box] = shape_crop\n",
    "        translated_boxes.append([int_new_x, int_new_y, w_box, h_box])\n",
    "    \n",
    "    return translated_image, translated_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Iteratively applies the function to augment aggregated concepts through translations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_triplet(proto_imgs, annotations_dir, idx):\n",
    "    image_size = 64\n",
    "    img_tensor = proto_imgs[idx]\n",
    "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    annotation_filename = os.path.join(annotations_dir, f\"image_{idx}_annotations.json\")\n",
    "    with open(annotation_filename, 'r') as f:\n",
    "        ann = json.load(f)\n",
    "    original_boxes = ann[\"boxes\"]\n",
    "    original_labels = ann[\"labels\"]\n",
    "    \n",
    "    scaling_values = [2, 4, 6, 8, 10]\n",
    "    directions = {\n",
    "        \"north\":        (0, -1),\n",
    "        \"north east\":   (1, -1),\n",
    "        \"north west\":   (-1, -1),\n",
    "        \"west\":         (-1, 0),\n",
    "        \"east\":         (1, 0),\n",
    "        \"south\":        (0, 1),\n",
    "        \"south east\":   (1, 1),\n",
    "        \"south west\":   (-1, 1),\n",
    "        \n",
    "        \"north 2\":      (0, -2),\n",
    "        \"south 2\":      (0, 2),\n",
    "        \"west 2\":       (-2, 0),\n",
    "        \"east 2\":       (2, 0),\n",
    "\n",
    "        \"north east 2\": (2, -2),\n",
    "        \"north west 2\": (-2, -2),\n",
    "        \"south east 2\": (2, 2),\n",
    "        \"south west 2\": (-2, 2)\n",
    "    }\n",
    "\n",
    "    direction_order = [\n",
    "        \"north\", \"north east\", \"north west\",\n",
    "        \"west\", \"east\", \"south\",\n",
    "        \"south east\", \"south west\",\n",
    "        \"north 2\", \"south 2\", \"west 2\", \"east 2\",\n",
    "        \"north east 2\", \"north west 2\", \"south east 2\", \"south west 2\"\n",
    "    ]\n",
    "\n",
    "    augmented_images_list = []\n",
    "    augmented_boxes_list = []\n",
    "    \n",
    "    for scale in scaling_values:\n",
    "        for d in direction_order:\n",
    "            multiplier = directions[d]\n",
    "            desired_translation = (scale * multiplier[0], scale * multiplier[1])\n",
    "            \n",
    "            translated_image, translated_boxes = translate_bounding_boxes_and_image(\n",
    "                img_np, original_boxes, desired_translation\n",
    "            )\n",
    "            \n",
    "            translated_image_tensor = translated_image.transpose(2, 0, 1)\n",
    "            \n",
    "            assert translated_image_tensor.shape == (3, image_size, image_size), (\n",
    "                f\"Shape mismatch: {translated_image_tensor.shape}\"\n",
    "            )\n",
    "            assert translated_image_tensor.min() >= 0.0 and translated_image_tensor.max() <= 1.0, (\n",
    "                \"Pixel values out of range\"\n",
    "            )\n",
    "            \n",
    "            augmented_images_list.append(torch.from_numpy(translated_image_tensor))\n",
    "            augmented_boxes_list.append(translated_boxes)\n",
    "    \n",
    "    AUG_proto_imgs = torch.stack(augmented_images_list)\n",
    "    num_augmentations = len(scaling_values) * len(direction_order)\n",
    "    AUG_proto_labels = torch.tensor([original_labels] * num_augmentations)\n",
    "    AUG_bbox_tensor = torch.tensor(augmented_boxes_list)\n",
    "    \n",
    "    return AUG_proto_imgs, AUG_proto_labels, AUG_bbox_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypes setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kand_annotations/init/concepts_init_aggregated.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load initial prototypes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m proto_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/kand_annotations/init/concepts_init_aggregated.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m proto_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kand_annotations/init/labels_init_aggregated.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m bbox_tensor \u001b[38;5;241m=\u001b[39m get_proto_imgs_bboxes()\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/kand_annotations/init/concepts_init_aggregated.pt'"
     ]
    }
   ],
   "source": [
    "# load initial prototypes\n",
    "proto_imgs = torch.load('data/kand_annotations/init/concepts_init_aggregated.pt')\n",
    "proto_labels = torch.load('data/kand_annotations/init/labels_init_aggregated.pt')\n",
    "bbox_tensor = get_proto_imgs_bboxes()\n",
    "\n",
    "print(proto_imgs.shape)\n",
    "print(proto_labels.shape)\n",
    "print(bbox_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prototypes(proto_imgs_shapes=proto_imgs, proto_labels_shapes=proto_labels, bbox_tensor=bbox_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of image indices (0 to 26).\n",
    "image_indices = range(3)\n",
    "\n",
    "# Initialize lists to hold the stacked augmented tensors for all images\n",
    "all_augmented_images = []\n",
    "all_augmented_labels = []\n",
    "all_augmented_boxes = []\n",
    "\n",
    "annotations_dir=f\"data/kand_annotations/init/bboxes_init\"\n",
    "\n",
    "# Loop through each image index and generate augmentations\n",
    "for selected_idx in image_indices:\n",
    "    # Call augment_triplet function to get the augmented data for this image\n",
    "    AUG_proto_imgs, AUG_proto_labels, AUG_bbox_tensor = augment_triplet(proto_imgs, annotations_dir, selected_idx)\n",
    "    \n",
    "    # Add the results to the lists\n",
    "    all_augmented_images.append(AUG_proto_imgs)\n",
    "    all_augmented_labels.append(AUG_proto_labels)\n",
    "    all_augmented_boxes.append(AUG_bbox_tensor)\n",
    "\n",
    "# Stack the lists of augmented data across all images\n",
    "AUG_proto_imgs = torch.stack(all_augmented_images)  # Shape: (27, 40, 3, 64, 64)\n",
    "AUG_proto_labels = torch.stack(all_augmented_labels)  # Shape: (27, 40, 3)\n",
    "AUG_bbox_tensor = torch.stack(all_augmented_boxes)  # Shape: (27, 40, 3, 4)\n",
    "\n",
    "# Randomly select 3 image indices from the range 0 to 26 for visualization\n",
    "random_indices = random.sample(image_indices, 3)\n",
    "\n",
    "# Loop through the randomly selected indices and plot the original image and its 40 augmentations\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Get the original image and annotation for the selected index\n",
    "    img_tensor = proto_imgs[idx]\n",
    "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    annotation_filename = os.path.join(annotations_dir, f\"image_{idx}_annotations.json\")\n",
    "    with open(annotation_filename, 'r') as f:\n",
    "        ann = json.load(f)\n",
    "    original_boxes = ann[\"boxes\"]\n",
    "    \n",
    "    # Plot the original image with bounding boxes\n",
    "    print(f\"IMAGE {i+1} CHECK\")\n",
    "    fig, ax_orig = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    plot_image_with_boxes(img_np, original_boxes, ax_orig)\n",
    "    ax_orig.set_title(f\"Original Image {idx}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot a 5 (scales) x 8 (directions) grid of augmented images.\n",
    "    scaling_values = [2, 4, 6, 8, 10]\n",
    "    direction_order = [\"north\", \"north east\", \"north ovest\", \"left\", \"right\", \"south\", \"south east\", \"south ovest\"]\n",
    "\n",
    "    fig, axes = plt.subplots(len(scaling_values), len(direction_order), figsize=(16, 10))\n",
    "    for row_idx, scale in enumerate(scaling_values):\n",
    "        for col_idx, d in enumerate(direction_order):\n",
    "            i = row_idx * len(direction_order) + col_idx\n",
    "            aug_img = AUG_proto_imgs[idx][i].numpy().transpose(1, 2, 0)   # shape: (H, W, C)\n",
    "            aug_boxes = AUG_bbox_tensor[idx][i].tolist()  # shape: (3, 4)\n",
    "            \n",
    "            ax = axes[row_idx, col_idx]\n",
    "            plot_image_with_boxes(aug_img, aug_boxes, ax)\n",
    "            ax.set_title(f\"{d}\\nScale: {scale}\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if i < 2:  # Only add these checks for the first 3 images\n",
    "        print(f\"FIRST CHECK\" if i == 0 else \"SECOND CHECK\" if i == 1 else \"THIRD CHECK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one dimension to match the augmented images\n",
    "proto_imgs_unsq = proto_imgs.unsqueeze(1)  \n",
    "proto_labels_unsq = proto_labels.unsqueeze(1) \n",
    "bbox_tensor_unsq = bbox_tensor.unsqueeze(1)  \n",
    "\n",
    "print(\"Unsqueezed original images: \", proto_imgs_unsq.shape)\n",
    "print(\"Unsqueezed original labels: \", proto_labels_unsq.shape)\n",
    "print(\"Unsqueezed original bounding boxes: \", bbox_tensor_unsq.shape)\n",
    "print()\n",
    "\n",
    "print(\"Augmented prototypes: \", AUG_proto_imgs.shape)\n",
    "print(\"Augmented labels: \", AUG_proto_labels.shape)\n",
    "print(\"Augmented bounding boxes: \", AUG_bbox_tensor.shape)\n",
    "print()\n",
    "\n",
    "# Stack images, labels and bounding boxes (initial + augmented)\n",
    "final_images = torch.cat((proto_imgs_unsq, AUG_proto_imgs), dim=1)  \n",
    "final_labels = torch.cat((proto_labels_unsq, AUG_proto_labels), dim=1)\n",
    "final_bboxes = torch.cat((bbox_tensor_unsq, AUG_bbox_tensor), dim=1)  \n",
    "\n",
    "final_images_ = final_images.view(-1, 3, 64, 64)\n",
    "final_labels_ = final_labels.view(-1, final_labels.shape[-1])\n",
    "final_bboxes_ = final_bboxes.view(-1, 3, 4)\n",
    "print(\"Final images flattened shape: \", final_images_.shape) \n",
    "print(\"Final labels flattened shape:\", final_labels_.shape) \n",
    "print(\"Final bounding boxes flattened shape: :\", final_bboxes_.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final images shape: \", final_images_.shape)  \n",
    "print(\"Final labels shape: \", final_labels_.shape) \n",
    "print(\"Final bounding boxes shape: \", final_bboxes_.shape) \n",
    "print(\"Range of pixel values in final_images:\", final_images_.min().item(), \"to\", final_images_.max().item())\n",
    "print(\"Dtype of final_images:\", final_images_.dtype)\n",
    "print(\"Dtype of final_labels:\", final_labels_.dtype)\n",
    "print(\"Range of values in final_bboxes:\", final_bboxes_.min().item(), \"to\", final_bboxes_.max().item())\n",
    "print(\"Dtype of final_bboxes:\", final_bboxes_.dtype)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flat_images(images_flat, labels_flat, bboxes_flat):\n",
    "    num_images = images_flat.shape[0]\n",
    "    # Define grid layout: 9 images per row (adjust as needed)\n",
    "    cols = 9\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        ax = axs[idx]\n",
    "        # Convert image from [channels, 64, 64] to [64, 64, channels]\n",
    "        img_np = images_flat[idx].permute(1, 2, 0).cpu().numpy()\n",
    "        # Get bounding boxes (assumed to be in the correct format)\n",
    "        boxes = bboxes_flat[idx].cpu().numpy()\n",
    "        # Use your custom function to plot the image with boxes\n",
    "        plot_image_with_boxes(img_np, boxes, ax)\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{labels_flat[idx].tolist()}\", fontsize=8)\n",
    "\n",
    "    # Turn off any remaining subplots\n",
    "    for j in range(num_images, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_flat_images(final_images_, final_labels_, final_bboxes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_images_, 'data/kand_annotations/yolo_annotations/images.pt')\n",
    "torch.save(final_labels_, 'data/kand_annotations/yolo_annotations/labels.pt')\n",
    "torch.save(final_bboxes_, 'data/kand_annotations/yolo_annotations/bboxes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypes extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shapes(proto_imgs, proto_labels, bbox_tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proto_imgs (Tensor): Shape (3, 3, 64, 64)\n",
    "        proto_labels (Tensor): Shape (3, 6)\n",
    "        bbox_tensor (Tensor): Shape (3, 3, 4) where each box is (x, y, w, h)\n",
    "    \n",
    "    Returns:\n",
    "        shapes_tensor (Tensor): Tensor of cropped and resized shapes with shape (9, 3, 64, 64)\n",
    "        labels_tensor (Tensor): Tensor of corresponding labels with shape (9, 2)\n",
    "    \"\"\"\n",
    "    shape_crops = []\n",
    "    label_pairs = []\n",
    "    \n",
    "    for i in range(proto_imgs.shape[0]):\n",
    "        img = proto_imgs[i]         # (3, 64, 64)\n",
    "        boxes = bbox_tensor[i]      # (3, 4)\n",
    "        labels = proto_labels[i]    # (6,)\n",
    "        \n",
    "        for j in range(boxes.shape[0]):\n",
    "            x, y, w, h = boxes[j].tolist()\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            \n",
    "            crop = img[:, y:y+h, x:x+w]\n",
    "            \n",
    "            # Resize the cropped image to 64x64.\n",
    "            resize_transform = T.Resize((64, 64))\n",
    "            crop_resized = resize_transform(crop)\n",
    "            \n",
    "            shape_crops.append(crop_resized)\n",
    "            \n",
    "            pair = torch.tensor([labels[j], labels[j+3]])\n",
    "            label_pairs.append(pair)\n",
    "    \n",
    "    shapes_tensor = torch.stack(shape_crops, dim=0)\n",
    "    labels_tensor = torch.stack(label_pairs, dim=0) \n",
    "\n",
    "    min_val = min(shape.min().item() for shape in shapes_tensor)\n",
    "    max_val = max(shape.max().item() for shape in shapes_tensor)\n",
    "    assert min_val >= 0 and max_val <= 1, f\"Min: {min_val}, Max: {max_val}\"\n",
    "    assert shapes_tensor.shape == (9, 3, 64, 64), f\"Shapes: {shapes_tensor.shape}, but expected (9, 3, 64, 64)\"\n",
    "    assert labels_tensor.shape == (9, 2), f\"Labels: {labels_tensor.shape}, but expected (9, 2)\"    \n",
    "    return shapes_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_tensor, label_tensor = extract_shapes(proto_imgs, proto_labels, bbox_tensor)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        idx = i * 3 + j\n",
    "        img_np = proto_tensor[idx].permute(1, 2, 0).cpu().numpy()\n",
    "        label = label_tensor[idx].tolist()\n",
    "        \n",
    "        axes[i, j].imshow(img_np)\n",
    "        axes[i, j].axis(\"off\")\n",
    "        axes[i, j].set_title(f\"Labels: {label}\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti-aliasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*antialias_image*: applies the anti alias to a single image with the specified threshold for white pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*augment_images_with_antialiasing*: calls the function above for each image and for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antialias_image(image_tensor, kernel_size, sigma, white_threshold):\n",
    "    \"\"\"\n",
    "    Augment one image by blurring its shape's edges.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (Tensor): A tensor of shape (3, 64, 64) with pixel values in [0,1].\n",
    "        kernel_size (int): Kernel size for the Gaussian blur (should be odd).\n",
    "        sigma (float): Standard deviation for the Gaussian blur.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Augmented image tensor of shape (3, 64, 64) with values in [0,1].\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy image (H, W, C) and scale to [0,255]\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # shape (64, 64, 3)\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    \n",
    "    background_mask = np.all(image_np > white_threshold, axis=-1)  # True for white pixels\n",
    "    shape_mask = (~background_mask).astype(np.float32)\n",
    "    \n",
    "    blurred_mask = cv2.GaussianBlur(shape_mask, (kernel_size, kernel_size), sigma)\n",
    "    blurred_mask = np.clip(blurred_mask, 0, 1)\n",
    "    \n",
    "    white_img = np.full_like(image_np, 255, dtype=np.uint8)\n",
    "    \n",
    "    blended = (image_np.astype(np.float32) * blurred_mask[..., None] +\n",
    "               white_img.astype(np.float32) * (1 - blurred_mask[..., None]))\n",
    "    blended = np.clip(blended, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    blended_tensor = torch.tensor(blended).permute(2, 0, 1).float() / 255.0\n",
    "    return blended_tensor\n",
    "\n",
    "\n",
    "def augment_images_with_antialiasing(shapes_tensor, labels_tensor, kernel_size, sigma, white_thresholds):\n",
    "    \"\"\"\n",
    "    Apply edge anti-aliasing augmentation to a batch of images while maintaining label consistency.\n",
    "\n",
    "    Args:\n",
    "        shapes_tensor (Tensor): Input tensor of shape (N, 3, 64, 64)\n",
    "        labels_tensor (Tensor): Corresponding labels of shape (N, 2)\n",
    "        kernel_size (int): Gaussian blur kernel size.\n",
    "        sigma (float): Gaussian blur sigma.\n",
    "        white_thresholds (list): List of white background thresholds to apply.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Augmented images of shape (N * len(white_thresholds), 3, 64, 64),\n",
    "                               Corresponding labels of shape (N * len(white_thresholds), 2).\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for threshold in white_thresholds:\n",
    "        for i in range(shapes_tensor.shape[0]):\n",
    "            aug_img = antialias_image(shapes_tensor[i], kernel_size=kernel_size, sigma=sigma, white_threshold=threshold)\n",
    "            augmented_images.append(aug_img)\n",
    "            augmented_labels.append(labels_tensor[i])  # Keep the same label\n",
    "\n",
    "    return torch.stack(augmented_images, dim=0), torch.stack(augmented_labels, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of anti aliasing augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_thresholds = list(np.arange(50, 250, 5))\n",
    "augmented_tensor_anti_aliasing, augmented_labels_anti_aliasing = augment_images_with_antialiasing(\n",
    "    proto_tensor, label_tensor, kernel_size=15, sigma=7, white_thresholds=white_thresholds)\n",
    "\n",
    "min_val = min(shape.min().item() for shape in augmented_tensor_anti_aliasing)\n",
    "max_val = max(shape.max().item() for shape in augmented_tensor_anti_aliasing)\n",
    "assert min_val >= 0 and max_val <= 1, f\"Min: {min_val}, Max: {max_val}\"\n",
    "assert augmented_tensor_anti_aliasing.shape == (9*len(white_thresholds), 3, 64, 64), f\"Expected ({9*len(white_thresholds)}, 3, 64, 64), but got {augmented_tensor.shape}\"\n",
    "    \n",
    "# Example usage:\n",
    "plot_augmented_results_with_antialiasing(proto_tensor, augmented_tensor_anti_aliasing, augmented_labels_anti_aliasing, white_thresholds)\n",
    "print(augmented_tensor_anti_aliasing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*scale_image*: applies the anti alias to a single image with the specified threshold for white pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*augment_images_with_scaling*: calls the function above for each image and scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(image_tensor, scale_factor):\n",
    "    \"\"\"\n",
    "    Augment one image by scaling its shape and centering it on a white canvas.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (Tensor): A tensor of shape (3, 64, 64) with pixel values in [0,1].\n",
    "        scale_factor (float): Factor by which to scale the shape.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Augmented image tensor of shape (3, 64, 64) with values in [0,1].\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy image (H, W, C) and scale to [0,255]\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create a mask of the shape (non-white pixels)\n",
    "    shape_mask = np.any(image_np < 255, axis=-1)\n",
    "    \n",
    "    if not np.any(shape_mask):\n",
    "        return image_tensor\n",
    "    \n",
    "    coords = np.argwhere(shape_mask)\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1  # add one to include the last index\n",
    "    shape_crop = image_np[y0:y1, x0:x1]\n",
    "    \n",
    "    new_w = max(1, int(shape_crop.shape[1] * scale_factor))\n",
    "    new_h = max(1, int(shape_crop.shape[0] * scale_factor))\n",
    "    scaled_shape = cv2.resize(shape_crop, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    canvas = np.full_like(image_np, 255, dtype=np.uint8)\n",
    "    canvas_h, canvas_w, _ = canvas.shape\n",
    "    \n",
    "    start_x = (canvas_w - new_w) // 2\n",
    "    start_y = (canvas_h - new_h) // 2\n",
    "\n",
    "    src_x_start = 0\n",
    "    src_y_start = 0\n",
    "    dst_x_start = start_x\n",
    "    dst_y_start = start_y\n",
    "    if dst_x_start < 0:\n",
    "        src_x_start = -dst_x_start\n",
    "        dst_x_start = 0\n",
    "    if dst_y_start < 0:\n",
    "        src_y_start = -dst_y_start\n",
    "        dst_y_start = 0\n",
    "\n",
    "    paste_w = min(new_w - src_x_start, canvas_w - dst_x_start)\n",
    "    paste_h = min(new_h - src_y_start, canvas_h - dst_y_start)\n",
    "    \n",
    "    scaled_shape_cropped = scaled_shape[src_y_start:src_y_start+paste_h, src_x_start:src_x_start+paste_w]\n",
    "    canvas[dst_y_start:dst_y_start+paste_h, dst_x_start:dst_x_start+paste_w] = scaled_shape_cropped\n",
    "    scaled_tensor = torch.tensor(canvas).permute(2, 0, 1).float() / 255.0\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "def augment_images_with_scaling(shapes_tensor, labels_tensor, scaling_factors, record_factor=0.8):\n",
    "    \"\"\"\n",
    "    Apply scaling augmentation to a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        shapes_tensor (Tensor): Input tensor of shape (N, 3, 64, 64)\n",
    "        labels_tensor (Tensor): Corresponding labels tensor of shape (N, ?)\n",
    "        scaling_factors (list): List of scaling factors to apply.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Augmented images of shape (N * len(scaling_factors), 3, 64, 64),\n",
    "                               Corresponding labels of shape (N * len(scaling_factors), ?)\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for factor in scaling_factors:\n",
    "        for i in range(shapes_tensor.shape[0]):\n",
    "            aug_img = scale_image(shapes_tensor[i], scale_factor=factor)\n",
    "            augmented_images.append(aug_img)\n",
    "            augmented_labels.append(labels_tensor[i])  # Keep same label\n",
    "                \n",
    "    return torch.stack(augmented_images, dim=0), torch.stack(augmented_labels, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of scaling augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factors = list(np.arange(0.3, 1.4, 0.05))\n",
    "augmented_tensor_scaling, augmented_labels_scaling = augment_images_with_scaling(proto_tensor, label_tensor, scaling_factors)\n",
    "\n",
    "min_val = min(shape.min().item() for shape in augmented_tensor_scaling)\n",
    "max_val = max(shape.max().item() for shape in augmented_tensor_scaling)\n",
    "assert min_val >= 0 and max_val <= 1, f\"Min: {min_val}, Max: {max_val}\"\n",
    "assert augmented_tensor_scaling.shape == (9*len(scaling_factors), 3, 64, 64), f\"Expected ({9*len(scaling_factors)}, 3, 64, 64), but got {augmented_tensor_scaling.shape}\"\n",
    "\n",
    "plot_augmented_results_with_scaling(proto_tensor, augmented_tensor_scaling, augmented_labels_scaling, scaling_factors)\n",
    "print(\"Scaling augmented tensor shape:\", augmented_tensor_scaling.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*translate_image*: translates a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*augment_images_with_translation*. calles the above function for each image and for each translation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_image(image_tensor, dx, dy):\n",
    "    \"\"\"\n",
    "    Augment one image by translating it.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (Tensor): A tensor of shape (3, 64, 64) with pixel values in [0,1].\n",
    "        dx (int): Translation offset in the x-direction (pixels).\n",
    "        dy (int): Translation offset in the y-direction (pixels).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Augmented image tensor of shape (3, 64, 64) with values in [0,1].\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy image (H, W, C) and scale to [0,255]\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    \n",
    "    h, w, _ = image_np.shape\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    \n",
    "    # Apply translation using warpAffine with white border\n",
    "    translated_np = cv2.warpAffine(image_np, M, (w, h), borderValue=(255, 255, 255))\n",
    "    \n",
    "    # Convert back to tensor in [0,1]\n",
    "    translated_tensor = torch.tensor(translated_np).permute(2, 0, 1).float() / 255.0\n",
    "    return translated_tensor\n",
    "\n",
    "def augment_images_with_translation(shapes_tensor, labels_tensor, translation_offsets):\n",
    "    \"\"\"\n",
    "    Apply translation augmentation to a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        shapes_tensor (Tensor): Input tensor of shape (N, 3, 64, 64).\n",
    "        labels_tensor (Tensor): Corresponding labels tensor of shape (N, ?).\n",
    "        translation_offsets (list): List of (dx, dy) tuples to apply.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Augmented images of shape (N * len(translation_offsets), 3, 64, 64),\n",
    "                               Corresponding labels of shape (N * len(translation_offsets), ?).\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for (dx, dy) in translation_offsets:\n",
    "        for i in range(shapes_tensor.shape[0]):\n",
    "            aug_img = translate_image(shapes_tensor[i], dx=dx, dy=dy)\n",
    "            augmented_images.append(aug_img)\n",
    "            augmented_labels.append(labels_tensor[i])  # Keep same label\n",
    "    \n",
    "    return torch.stack(augmented_images, dim=0), torch.stack(augmented_labels, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of translation augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_offsets = [\n",
    "    (-0.1, 0), (0, -0.1), (0.1, 0), (0, 0.1),\n",
    "    (-0.2, 0), (0, -0.2), (0.2, 0), (0, 0.2),\n",
    "    (-0.3, 0), (0, -0.3), (0.3, 0), (0, 0.3),\n",
    "    (-0.4, 0), (0, -0.4), (0.4, 0), (0, 0.4),\n",
    "    (-0.5, 0), (0, -0.5), (0.5, 0), (0, 0.5),\n",
    "    (-0.6, 0), (0, -0.6), (0.6, 0), (0, 0.6),\n",
    "    (-0.7, 0), (0, -0.7), (0.7, 0), (0, 0.7),\n",
    "    (-0.8, 0), (0, -0.8), (0.8, 0), (0, 0.8),\n",
    "    (-0.9, 0), (0, -0.9), (0.9, 0), (0, 0.9),\n",
    "    (-1, 0), (0, -1), (1, 0), (0, 1),\n",
    "    (-1.1, 0), (0, -1.1), (1.1, 0), (0, 1.1),\n",
    "    (-1.2, 0), (0, -1.2), (1.2, 0), (0, 1.2),\n",
    "    (-1.3, 0), (0, -1.3), (1.3, 0), (0, 1.3),\n",
    "    (-1.4, 0), (0, -1.4), (1.4, 0), (0, 1.4),\n",
    "    (-1.5, 0), (0, -1.5), (1.5, 0), (0, 1.5),\n",
    "    (-1.6, 0), (0, -1.6), (1.6, 0), (0, 1.6),\n",
    "    (-1.7, 0), (0, -1.7), (1.7, 0), (0, 1.7),\n",
    "    (-1.8, 0), (0, -1.8), (1.8, 0), (0, 1.8),\n",
    "    (-1.9, 0), (0, -1.9), (1.9, 0), (0, 1.9),\n",
    "    (-2, 0), (0, -2), (2, 0), (0, 2),\n",
    "    (-2.1, 0), (0, -2.1), (2.1, 0), (0, 2.1),\n",
    "    (-2.2, 0), (0, -2.2), (2.2, 0), (0, 2.2),\n",
    "    (-2.3, 0), (0, -2.3), (2.3, 0), (0, 2.3),\n",
    "    (-2.4, 0), (0, -2.4), (2.4, 0), (0, 2.4),\n",
    "    (-2.5, 0), (0, -2.5), (2.5, 0), (0, 2.5),\n",
    "] \n",
    "augmented_tensor_translation, augmented_labels_translation = augment_images_with_translation(\n",
    "    proto_tensor, label_tensor, translation_offsets)\n",
    "\n",
    "min_val = min(shape.min().item() for shape in augmented_tensor_translation)\n",
    "max_val = max(shape.max().item() for shape in augmented_tensor_translation)\n",
    "assert min_val >= 0 and max_val <= 1, f\"Min: {min_val}, Max: {max_val}\"\n",
    "assert augmented_tensor_translation.shape == (9*len(translation_offsets), 3, 64, 64), f\"Expected ({9*len(translation_offsets)}, 3, 64, 64), but got {augmented_tensor_translation.shape}\"\n",
    "\n",
    "plot_augmented_results_with_translation(proto_tensor, augmented_tensor_translation, augmented_labels_translation, translation_offsets)\n",
    "print(\"Translation augmented tensor shape:\", augmented_tensor_translation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all augmented image tensors\n",
    "final_images = torch.cat([\n",
    "    proto_tensor,\n",
    "    augmented_tensor_anti_aliasing,\n",
    "    augmented_tensor_scaling,\n",
    "    augmented_tensor_translation\n",
    "], dim=0)\n",
    "\n",
    "# Merge all corresponding labels\n",
    "final_labels = torch.cat([\n",
    "    label_tensor,\n",
    "    augmented_labels_anti_aliasing,\n",
    "    augmented_labels_scaling,\n",
    "    augmented_labels_translation\n",
    "], dim=0)\n",
    "\n",
    "# Print final shape to verify\n",
    "print(f\"Final dataset shape: {final_images.shape}\")  # Should be (total_samples, 3, 64, 64)\n",
    "print(f\"Final labels shape: {final_labels.shape}\")  # Should be (total_samples, 6)\n",
    "print(\"Range of pixel values in final_images:\", final_images.min().item(), \"to\", final_images.max().item())\n",
    "print(\"Dtype of final_images:\", final_images.dtype)\n",
    "print(\"Dtype of final_labels:\", final_labels.dtype)\n",
    "print()\n",
    "\n",
    "torch.save(final_images, 'data/kand_annotations/pnet_proto/concept_prototypes.pt')\n",
    "torch.save(final_labels, 'data/kand_annotations/pnet_proto/labels_prototypes.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
