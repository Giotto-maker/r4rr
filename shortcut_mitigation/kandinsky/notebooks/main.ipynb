{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAPERMILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# model's seeds\n",
    "SEED = 128\n",
    "NO_AUGMENTATIONS = False\n",
    "\n",
    "# additional paramters\n",
    "model_parameter_name = 'sl'\n",
    "uns_parameter_percentage = 1.0\n",
    "GPU_ID = '3'\n",
    "HIDE_SHAPES = []\n",
    "HIDE_COLORS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papermill seed parameter is: 128\n",
      "Papermill model name is: sl\n",
      "Papermill uns_parameter_percentage is: 1.0\n",
      "Papermill HIDE_SHAPES is: []\n",
      "Papermill HIDE_COLORS is: []\n",
      "Papermill GPU_ID is: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert isinstance(SEED, int), \"SEED should be an integer\"\n",
    "assert model_parameter_name is not None, \"model_parameter_name should not be None\"\n",
    "assert isinstance(uns_parameter_percentage, float), \"uns_parameter_percentage should be a float\"\n",
    "assert 0.0 <= uns_parameter_percentage <= 1.0, \"uns_parameter_percentage should be in the range [0.0, 1.0]\"\n",
    "assert GPU_ID is not None, \"GPU_ID should not be None\"\n",
    "assert HIDE_SHAPES is not None, \"HIDE_SHAPES should not be None\"\n",
    "assert HIDE_COLORS is not None, \"HIDE_COLORS should not be None\"\n",
    "\n",
    "print(\"Papermill seed parameter is: \" + str(SEED))\n",
    "print(\"Papermill model name is: \" + model_parameter_name)\n",
    "print(\"Papermill uns_parameter_percentage is: \" + str(uns_parameter_percentage))\n",
    "print(\"Papermill HIDE_SHAPES is: \" + str(HIDE_SHAPES))\n",
    "print(\"Papermill HIDE_COLORS is: \" + str(HIDE_COLORS))\n",
    "print(\"Papermill GPU_ID is: \" + GPU_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_mnist_add_utils' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import setproctitle, socket, uuid\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter\n",
    "\n",
    "from utils import fprint\n",
    "from utils.checkpoint import save_model, create_load_ckpt\n",
    "from utils.metrics import evaluate_metrics\n",
    "from utils.status import progress_bar\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from protonet_kand_modules.utility_modules.check_gpu import my_gpu_info\n",
    "from protonet_kand_modules.arguments import args_sl, args_ltn, args_dpl\n",
    "from protonet_kand_modules.utility_modules.visualizers import plot_primitives\n",
    "from protonet_kand_modules.data_modules.proto_data_creation import get_random_classes\n",
    "\n",
    "from backbones.kand_protonet import PrototypicalLoss\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "\n",
    "from datasets import get_dataset\n",
    "from datasets.utils.base_dataset import BaseDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is: sl\n",
      "Uns percentage is: 1.0\n"
     ]
    }
   ],
   "source": [
    "MODEL = model_parameter_name\n",
    "UNS_PERCENTAGE = uns_parameter_percentage\n",
    "\n",
    "print(\"Model is: \" + MODEL)\n",
    "print(\"Uns percentage is: \" + str(UNS_PERCENTAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 128\n",
      "Save path: ../outputs/kand/my_models/sl/[F]-episodic-proto-net-pipeline-1.0-HIDE-SHAPES-[]-HIDE-COLORS-[]\n"
     ]
    }
   ],
   "source": [
    "if MODEL == 'sl':       args = args_sl\n",
    "elif MODEL == 'ltn':    args = args_ltn\n",
    "else:                   args = args_dpl\n",
    "\n",
    "args.seed = SEED\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "args.GPU_ID = GPU_ID\n",
    "args.hide_shapes = HIDE_SHAPES\n",
    "args.hide_colors = HIDE_COLORS\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"kand\" \n",
    "save_model_name = MODEL\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"outputs\", \n",
    "    save_folder, \n",
    "    \"my_models\", \n",
    "    save_model_name,\n",
    "    f\"[F]-episodic-proto-net-pipeline-{UNS_PERCENTAGE}-HIDE-SHAPES-{HIDE_SHAPES}-HIDE-COLORS-{HIDE_COLORS}\",\n",
    ")\n",
    "if NO_AUGMENTATIONS:\n",
    "    save_path += \"-NO-AUGMENTATIONS\"\n",
    "print(f\"Save path: {str(save_path)}\")\n",
    "\n",
    "if args.model in ['kandsl', 'kandltn', 'kanddpl'] or not args.prototypes:\n",
    "    raise ValueError(\"This experiment is NOT meant for baseline models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ultralytics-3/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_yolo_project_path = f\"ultralytics-{args.GPU_ID}/\"\n",
    "my_yolo_premodel_path = f\"ultralytics-{args.GPU_ID}/pretrained/yolo11n.pt\"\n",
    "args.yolo_folder = my_yolo_project_path\n",
    "my_yolo_project_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.13.0+cu117\n",
      "CUDA version:  11.7\n",
      "Number of GPUs available: 1\n",
      "Device 0: NVIDIA TITAN Xp\n",
      "  Memory Allocated: 0 bytes\n",
      "  Memory Cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "my_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "*(only one single training image has been annotated; then, through standard data augmentation we derived the 'YOLO training dataset' whose extracted primitives give the 'Protonet Annotated Dataset' + other data augmentations.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks/../data/kand_config_yolo.yaml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_path = os.path.join(os.getcwd(), \"../data/kand_config_yolo.yaml\")\n",
    "yaml_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protonet Annotated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypical data loaded\n",
      "Images:  torch.Size([1467, 3, 64, 64])\n",
      "Labels:  torch.Size([1467, 2])\n"
     ]
    }
   ],
   "source": [
    "proto_images = torch.load('../data/kand_annotations/pnet_proto/concept_prototypes.pt')\n",
    "proto_labels = torch.load('../data/kand_annotations/pnet_proto/labels_prototypes.pt')\n",
    "print(\"Prototypical data loaded\")\n",
    "print(\"Images: \", proto_images.shape)\n",
    "print(\"Labels: \", proto_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_AUGMENTATIONS:\n",
    "    print(\"Excluding Augmentations...\")\n",
    "    pairs = {(s, c): (proto_labels[:, 0] == s) & (proto_labels[:, 1] == c)\n",
    "             for s in range(3) for c in range(3)}\n",
    "    pair_indices = {k: v.nonzero(as_tuple=True)[0].tolist() for k, v in pairs.items()}\n",
    "    selected_indices = []\n",
    "    for s in range(3):\n",
    "        for c in range(3):\n",
    "            idxs = pair_indices[(s, c)]\n",
    "            assert len(idxs) > 0, f\"No samples found for pair ({s},{c})\"\n",
    "            chosen = random.choice(idxs)\n",
    "            selected_indices.append(chosen)\n",
    "\n",
    "    proto_images = proto_images[selected_indices]\n",
    "    proto_labels = proto_labels[selected_indices]\n",
    "    print(\"Selected indices:\", selected_indices)\n",
    "    print(\"Sampled labels:\\n\", proto_labels)\n",
    "    print(proto_images.shape)\n",
    "    print(proto_labels.shape)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    for i in range(9):\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        img_np = proto_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_np)\n",
    "        ax.set_title(f\"Shape: {proto_labels[i,0].item()}, Color: {proto_labels[i,1].item()}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimitivesDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): Tensor of shape [N, 3, 64, 64]\n",
    "            labels (Tensor): Tensor of shape [N, 2] where:\n",
    "                             - labels[:, 0] is the shape label  (0: square, 1: circle, 2: triangle)\n",
    "                             - labels[:, 1] is the colour label (0: red, 1: yellow, 2: blue)\n",
    "            transform: Optional transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels  # shape [N, 2]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        # Return shape label and colour label separately\n",
    "        shape_label = self.labels[index, 0].long()\n",
    "        color_label = self.labels[index, 1].long()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, shape_label.squeeze(), color_label.squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class FixedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset_size, batch_size, iterations):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.iterations):\n",
    "            start = (i * self.batch_size) % self.dataset_size\n",
    "            end = start + self.batch_size\n",
    "            if end <= self.dataset_size:\n",
    "                yield list(range(start, end))\n",
    "            else:\n",
    "                # wrap around if needed\n",
    "                part1 = list(range(start, self.dataset_size))\n",
    "                part2 = list(range(0, end - self.dataset_size))\n",
    "                yield part1 + part2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler(object):\n",
    "    \"\"\"\n",
    "    Yields a batch of indices for episodic training.\n",
    "    At each iteration, it randomly selects 'classes_per_it' classes and then picks\n",
    "    'num_samples' samples for each selected class.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (array-like): 1D array or list of labels for the target task.\n",
    "                                 This should be either the shape labels or the colour labels.\n",
    "            classes_per_it (int): Number of random classes for each iteration.\n",
    "            num_samples (int): Number of samples per class (support + query) in each episode.\n",
    "            iterations (int): Number of iterations (episodes) per epoch.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # Create an index matrix of shape (num_classes, max_samples_in_class)\n",
    "        max_count = max(self.counts)\n",
    "        self.indexes = np.empty((len(self.classes), max_count), dtype=int)\n",
    "        self.indexes.fill(-1)\n",
    "        self.indexes = torch.LongTensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros(len(self.classes), dtype=torch.long)\n",
    "\n",
    "        # Fill in the matrix with indices for each class.\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            # Find the row corresponding to this label\n",
    "            class_idx = (self.classes == label).nonzero(as_tuple=False).item()\n",
    "            # Find the next available column (where the value is -1)\n",
    "            pos = (self.indexes[class_idx] == -1).nonzero(as_tuple=False)[0].item()\n",
    "            self.indexes[class_idx, pos] = idx\n",
    "            self.numel_per_class[class_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a batch of indices for each episode.\n",
    "        \"\"\"\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            batch = torch.LongTensor(cpi * spc)\n",
    "            # Randomly choose 'classes_per_it' classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "            for i, class_idx in enumerate(c_idxs):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "                # Randomly choose 'num_samples' indices for the class\n",
    "                perm = torch.randperm(self.numel_per_class[class_idx])\n",
    "                sample_idxs = perm[:spc]\n",
    "                batch[s] = self.indexes[class_idx, sample_idxs]\n",
    "            # Shuffle the batch indices\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istantiating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PrimitvesDataset instance\n",
    "kand_proto_dataset = PrimitivesDataset(proto_images, proto_labels, transform=None)\n",
    "\n",
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "shape_labels = kand_proto_dataset.labels[:, 0].numpy()  # & ok\n",
    "color_labels = kand_proto_dataset.labels[:, 1].numpy()  # & ok\n",
    "\n",
    "# Create episodes sampler for shapes and colours:\n",
    "shape_sampler = PrototypicalBatchSampler(shape_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "color_sampler = PrototypicalBatchSampler(color_labels, args.classes_per_it, args.num_samples, args.iterations)\n",
    "\n",
    "# Create dataloaders for each primitve\n",
    "episodic_shape_dataloader = DataLoader(kand_proto_dataset, batch_sampler=shape_sampler)\n",
    "episodic_color_dataloader = DataLoader(kand_proto_dataset, batch_sampler=color_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct shape labels: 3\n",
      "Number of distinct color labels: 3\n",
      "Batch images shape: torch.Size([30, 3, 64, 64])\n",
      "Batch shape labels: [0, 1, 0, 0, 2, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 1, 0, 1, 1, 0]\n",
      "Shape label distribution in batch: Counter({0: 10, 1: 10, 2: 10})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Color-based episodic batch:\n",
      "Batch images shape: torch.Size([30, 3, 64, 64])\n",
      "Batch color labels: [0, 1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 0]\n",
      "Color label distribution in batch: Counter({0: 10, 1: 10, 2: 10})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABb4AAAJSCAYAAAAMOtMPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcvElEQVR4nO3de4xkZ3kn/qdn3NM903Oxx3fvmrE9QLCAQH6OSMArjSGxiTBKkMAJkbKeQfhCYscoXBUigcCJvFyUjGVLFpZY4hVEioYIbViDpSCI5UAURGQghBBfMDYY8Nhjz81znz6/P7zu9XnqTL1V07fqtz8fyZKfOqeqTlW99dTpt2q+71jTNE0AAAAAAEAlViz2AQAAAAAAwFwy8Q0AAAAAQFVMfAMAAAAAUBUT3wAAAAAAVMXENwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFRPfAAAAAABUxcQ3AAAAAABVMfG9hFx22WVx2WWXLfZhDG3btm2xdu3aOb3NpfpcQM2W6vtSj4LlYam+L/UoWB6W6vtSj4LlYam+L/UoTHzPo4cffjiuv/76uOiii2JycjLWr18fl156adx6661x8ODBxT68vi644IJ485vfvNiHMa8+85nPxMUXXxyTk5Pxkpe8JG677bbFPiRYUHrU6Lrjjjviqquuihe96EUxNjYW27ZtW+xDggWnR42mn/zkJ/HRj340XvOa18Rpp50WZ5xxRlx22WXx1a9+dbEPDRaUHjWa9Ch4jh412sxHLZxTFvsAanX33XfHVVddFRMTE3H11VfHK17xijhy5Ej80z/9U7z//e+Pf//3f48777xzsQ9z2fr0pz8d73rXu+Ktb31rvOc974n77rsvbrrppjhw4EB88IMfXOzDg3mnR422j3/847Fv3754zWteEz//+c8X+3BgwelRo+t//+//HR//+MfjLW95S2zdujWOHTsW/+t//a+4/PLL43/+z/8Z73jHOxb7EGHe6VGjS48CPWrUmY9aWCa+58EjjzwSb3/722PTpk3xta99Lc4999yZbTfccEM89NBDcffddy/iEUYcO3YspqenY9WqVYt6HIvh4MGD8Wd/9mdx5ZVXxhe+8IWIiLj22mtjeno6br755rjuuuvitNNOW+SjhPmjR42+e++9d+bX3nP9T/Ng1OlRo+31r399PPbYY3HGGWfMXPaud70rXv3qV8eHP/xhk0pUT48abXoUy50eNdrMRy08USfz4BOf+ETs378/PvOZz7SazPNe/OIXx7vf/e6Z+tixY3HzzTfH5s2bY2JiIi644IL40Ic+FIcPHy7e186dO+Od73xnnH322TE5ORmvetWr4q677mrt8+Mf/zjGxsbiU5/6VGzfvn3mfn7wgx/M6nHed999M/8Uf2JiIs4///z4kz/5kxP+s5kf/ehH8cY3vjGmpqbivPPOi4997GPRNE1rn+np6di+fXu8/OUvj8nJyTj77LPj+uuvj2eeeaZ4PI899lj88Ic/LO739a9/PXbt2hV/9Ed/1Lr8hhtuiGeffXbRPwRgvulRo92jIiI2bdoUY2NjA+0LtdGjRrtHvfzlL29NKEVETExMxJve9Kb46U9/Gvv27SveBixlepQeBaNMjxrtHmU+auH5xfc8+NKXvhQXXXRRvO51rxto/2uuuSbuuuuueNvb3hbvfe9741/+5V/illtuif/4j/+IL37xiye83sGDB+Oyyy6Lhx56KG688ca48MILY8eOHbFt27bYvXt3q5lFRHz2s5+NQ4cOxXXXXRcTExOxcePGWT3OHTt2xIEDB+IP//AP4/TTT49vfetbcdttt8VPf/rT2LFjR2vf48ePx2/91m/Fr//6r8cnPvGJuOeee+IjH/lIHDt2LD72sY/N7Hf99dfHX//1X8c73vGOuOmmm+KRRx6J22+/Pe6///74xje+EePj4yc8nquvvjruvffenuaV3X///RER8au/+qutyy+55JJYsWJF3H///fEHf/AHwz4dsGToUaPdo2C506OWZo/6xS9+EWvWrIk1a9ac1PVhqdCj9CgYZXrUaPco81GLoGFO7dmzp4mI5nd+53cG2v873/lOExHNNddc07r8fe97XxMRzde+9rWZy7Zs2dJs2bJlpt6+fXsTEc3nPve5mcuOHDnSvPa1r23Wrl3b7N27t2mapnnkkUeaiGjWr1/f7Ny5c6Dj2rRpU3PllVf23efAgQM9l91yyy3N2NhY8+ijj85ctnXr1iYimj/+4z+euWx6erq58sorm1WrVjVPPvlk0zRNc9999zUR0Xz+859v3eY999zTc3l+Lp6/bJAhfcMNNzQrV67s3HbmmWc2b3/724u3AUuVHjX6PSqbmppqtm7dOvT1YCnSo5Zej2qapnnwwQebycnJ5r//9/9+UteHpUKP0qNglOlRo9+jzEctPFEnc2zv3r0REbFu3bqB9v/yl78cERHvec97Wpe/973vjYjo+88cvvzlL8c555wTv//7vz9z2fj4eNx0002xf//+uPfee1v7v/Wtb40zzzxzoOMaxOrVq2f+/9lnn42nnnoqXve610XTNDPfYr3QjTfeOPP/Y2NjceONN8aRI0dmVtjesWNHbNiwIS6//PJ46qmnZv675JJLYu3atfH1r3+97/H84z/+40C/ADh48OAJs6QmJydHfoVjmA09avR7FCxnetTS61EHDhyIq666KlavXh3/43/8j6GvD0uJHqVHwSjTo0a/R5mPWniiTubY+vXrIyIGzg579NFHY8WKFfHiF7+4dfk555wTp556ajz66KN9r/uSl7wkVqxof39x8cUXz2x/oQsvvHCgYxrUY489Fh/+8Ifj7//+73syj/bs2dOqV6xYERdddFHrspe+9KUR8VzmU0TEgw8+GHv27Imzzjqr8/527tw5J8e9evXqOHLkSOe2Q4cOtRoo1EaPes4o9yhYzvSo5yyVHnX8+PF4+9vfHj/4wQ/iK1/5Spx33nlzfh8wSvSo5+hRMJr0qOeMco8yH7XwTHzPsfXr18d5550X3//+94e63kIsYjaXb6Djx4/H5ZdfHk8//XR88IMfjJe97GUxNTUVjz/+eGzbti2mp6eHvs3p6ek466yz4vOf/3zn9rn6dvDcc8+N48ePx86dO1tN7ciRI7Fr1y4nRFRNjxr9HgXLmR61tHrUtddeG//n//yf+PznPx9veMMb5vz2YdToUXoUjDI9avR7lPmohWfiex68+c1vjjvvvDP++Z//OV772tf23XfTpk0xPT0dDz744Mw3YxERTzzxROzevTs2bdrU97rf+973Ynp6uvUt2/Mryfa77mz927/9WzzwwANx1113xdVXXz1z+T/8wz907j89PR0/+tGPZr5Vi4h44IEHIiLiggsuiIiIzZs3x1e/+tW49NJL5/Vbrle/+tUREfHtb3873vSmN81c/u1vfzump6dntkOt9Kheo9SjYLnTo3qNYo96//vfH5/97Gdj+/btrX/mDLXTo3rpUTA69Kheo9SjzEctPBnf8+ADH/hATE1NxTXXXBNPPPFEz/aHH344br311oiImYG+ffv21j5/+Zd/GRERV1555Qnv501velP84he/iL/927+duezYsWNx2223xdq1a2PLli2zfSgntHLlyoiIVoZR0zQzj6vL7bff3tr39ttvj/Hx8fiN3/iNiIj43d/93Th+/HjcfPPNPdc9duxY7N69u+8xPfbYYzNNtp83vOENsXHjxrjjjjtal99xxx2xZs2avs851ECP6jYqPQqWOz2q2yj1qE9+8pPxqU99Kj70oQ/Fu9/97oGuA7XQo7rpUTAa9Khuo9KjzEctPL/4ngebN2+Ov/mbv4nf+73fi4svvjiuvvrqeMUrXhFHjhyJb37zm7Fjx47Ytm1bRES86lWviq1bt8add94Zu3fvji1btsS3vvWtuOuuu+Itb3lLvP71rz/h/Vx33XXx6U9/OrZt2xb/+q//GhdccEF84QtfiG984xuxffv2gRc0OJGHHnoo/vzP/7zn8l/5lV+JK664IjZv3hzve9/74vHHH4/169fH3/3d3/VkKz1vcnIy7rnnnti6dWv82q/9WnzlK1+Ju+++Oz70oQ/N/JORLVu2xPXXXx+33HJLfOc734krrrgixsfH48EHH4wdO3bErbfeGm9729tOeLxXX3113HvvvcUFBVavXh0333xz3HDDDXHVVVfFG9/4xrjvvvvic5/7XPzFX/xFbNy4cYhnCZYeParXKPWoiIgvfelL8d3vfjciIo4ePRrf+973Zh7rb//2b8cv//IvF28Dlio9qtco9agvfvGL8YEPfCBe8pKXxMUXXxyf+9znWtsvv/zyOPvss0tPDyxZelQvPQpGhx7Va5R6lPmoRdAwbx544IHm2muvbS644IJm1apVzbp165pLL720ue2225pDhw7N7Hf06NHmox/9aHPhhRc24+Pjzfnnn9/86Z/+aWufpmmaLVu2NFu2bGld9sQTTzTveMc7mjPOOKNZtWpV88pXvrL57Gc/29rnkUceaSKi+eQnPznwsW/atKmJiM7/3vnOdzZN0zQ/+MEPmt/8zd9s1q5d25xxxhnNtdde23z3u99tIqJ1DFu3bm2mpqaahx9+uLniiiuaNWvWNGeffXbzkY98pDl+/HjPfd95553NJZdc0qxevbpZt25d88pXvrL5wAc+0PzsZz/r+1xs2bKlGWZI33nnnc0v/dIvNatWrWo2b97c/NVf/VUzPT098PVhqdOjnjOKPWrr1q0nfHz5+YNa6VHPGbUe9ZGPfOSEjy0imq9//esDP0+wlOlRz9GjYDTpUc8ZtR71wvsxH7UwxppmgJ+eAQAAAADAEiHjGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKqcs9gEAADC8plnsIwBKxsYW+whG13z3sPm4fX2X+TQf/WK2t6mHAUudX3wDAAAAAFAVE98AAAAAAFTFxDcAAAAAAFWR8Q0AQE927bD1ydwHzKVhs2jz/sPWjJbcX6any/uUepKexWIq9ZxBelS+bIWfPgLLjLYHAAAAAEBVTHwDAAAAAFAVE98AAAAAAFRFxjcAwDJUyrrN+bi57sq+nYsccJgrpXzcnHWb60GyceV+L55he1bXZXO9loGeRz+zXYcgK/WwrsvyGNXDgNr5xTcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUBUT3wAAAAAAVMXilgAsslFYCWoUjiEbhdWGRuEYmCuzXczy+PH+2we5jxILw9HPbBeGG/b6K1eW9ymNWQvHzZ3Sc13qaV2X5b42bA/Ts5iNUn8o9bBBxl/pPvQwoHZ+8Q0AAAAAQFVMfAMAAAAAUBUT3wAAAAAAVEXGNwAjphRYeDLbjxf2yUGguZ6PEM/83XOpziGLpe1Z3i60sXazzcPN2beDZHzny+TjMp+GzcddUfjJTyk/t2u8yr+dP6XnP9fHjrXro0d7bzNflq8zbOa3HsZslDK8Sz3slDSbs2pV732Mjw93HwC18YtvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqMr5ZHAsRiCewDJao3B+GrXOed0TEscI+eXuuO8KMZ21lqvNHcq5L+5cyv0sZ4V37sJQMm0Wbs2wPHGjXBw/2rw8d6j2GnJc77DHJy6Wf2ebh5qzb1auHqycmeo8p38ewueL8P6V1B0rbcw/bv7/3Pp59tv8+hw/3v89S3bX2ATyv1C9Kde5ha9f2r7sum5pq1yvz6WUMtx1g1DkVAwAAAACgKia+AQAAAACoiolvAAAAAACqIuObcqDmIAGcpRC+k7nNfttLAWhdl+WAMiGMMKJyP8iBmbnO++d87oiII6k+Wtie64XI+F5VqFOwY/H2ck/LdVfflfG9lAybNZszvY+mt8G+fe366af7188803tMOfd72Ixe+bj0k0/VSnm5+dQvZ3Zv3Dhc3ZV1my9zOjm4YftBqaflvO4nn+y9z3zZU0/1v41he1g+Jnih3B+G/fM097AzzmjXZ57Ze595jK7Kp5cAlXNqBgAAAABAVUx8AwAAAABQFRPfAAAAAABURcY3vUphdV0BnMeO9d9ntnUp4/uUjqGcLxsf718LZYRFkrOmS3Up8zvnd0f0ZnYfTvWhQj0foZ05szvfRynLfLY9S573UjfbfNwj6W2xZ0+7/sUv2vXjj/ffHtGbj1s6hRjkFAOeV8rTztvzqeD69e36v/yXdp3HY87CnZrqPab8PszHJE93cMP2tPznR16nYOfO3vv4yU/613ntgmF7Vj4meKHco4btaevWtevzz2/XXZ+hExPtesOGdt21VBZATcz0AQAAAABQFRPfAAAAAABUxcQ3AAAAAABVkfFNr1KYXVd43dGj/ffJdb7NYescgJbDy0502Qvl0DRgADln+kSXDXMbpYzunH2de1Cuc353RMTBWdbzEdqZg19XF+p8DPl5y7eXP+JzzxvkFCAHP5aCIGe7nRPJ2bddl5U+vnM9bMb3j3/cv47ozced7cc9y1dX7mwpH7e0vMvpp7frPN7y/jkLd+PGwY6TweQeNmxPK2V8P/lk730+9li7fuCB/tcp9ajSnzzwQrlHlXpY3n7qqe06v0e61hTI18mf/f4kBmrnF98AAAAAAFTFxDcAAAAAAFUx8Q0AAAAAQFVkfNMrh4XlsLocDNZ1Wa7zbQybCZ73z2Fka9b0HlMOCszXyUGOwAC68rxLmdu5zhneuT40ZJ0zvXM+d9dl+TaWQsb35JDbh627LsunCbnOfbS0P/Np2Hzc/NFaysf9yU/a9YMP9h5Dvk7p4720BAjLV1d2ds6/LdU57/bss/tvz5ne55zTrvN7JqJ32RlO3mwzvw+mj+68bkFExFNPteuf/axd57UNhu1hMr7pp5TpXar372/XOb/7rLN67/PAgXZtbY1FVGpqXZflD56u61CnfCKUTzhKiwJ0nUgt04VJnKoBAAAAAFAVE98AAAAAAFTFxDcAAAAAAFURvsnwAXpdGd+HD/ev83VyVlUpIzzXOZ87P4aI3gykfJ3JrnxboL+uXLncE4bN6E7hg5GChmP/kPWz0WvYY8r1QmR8zzajeyrVa1O9rlB3XWe2OeMpay6WZ67cUpE//ktLfnRl2c42D1fWKM/riqEsZUDnOt9GPv0sZd3m90DX6SajY5D43GGXFRq2Z+XtxgwvNNvxUBp/XZ+h+tgIKeV3R/SGsufFCvI8C/XKJzF5/iovTLI6/V2WM8AjenPAlwm/+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqiLjm145e6qUtx0RcfBgu87ZVKXM71JGeK4HyfjOmUYTE+16MUL4ugIrYUnpep/kfLrUD3oyuHOG995UPzNkvbtw+xEROQ8v16WM7/kIHk59rCcfe6JQ5/3Xp/q0Ieuuy3IOeM4Az3JuXD5mFtOw+cil7NCuaMrZ5uPmWhbp8jVIxndJXu5FpvfSNmwP68r4LmVyl3Lgh123APoZdkyXPkO7xrw+NkLyC5jnUCIidu1q1z/+cbvevXsuj4hRlk9iTj+9XW/a1K7zidOaNb23KeMbAAAAAACWPhPfAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVLG5J74oXeVWXvOjCvo6F4/bs6b9Pvo28WOWhQ8NtLy1UGdG7GMCqVe06h/3n1UBKC1FaqJJlqWuFnLzgbVrctrh45dOp3pnqJ4esUz+KiN5jLC12meuFWNwyLwS5qrA913lhyjNTfVaq8wKeERG5l+Y6v/75NMJilqOktKBVaSG4YRd1i5j9wnB5f5avQU6zSgvB5VPB0ngbdqG5Qffh5Az7XJZ6WNdlwy7IWxpDFrekn2H7RWlxy1x3LW5ZWrRXz1pAeY7jmfw3UUT853+26699rV3nxS6p1ynp76yXvrRdv/717Tqf9IznvzVPcNky4BffAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVZHzTq5TxvTfn9UZvPtXTKbP3QMr8zZndpTofw+rV7borCDLngOdM7w0b2nUp8EzmN0tSDu4rBfmV9s9Z2RER6f0ZeR2AnF/3VKpzRvcvUv3zVD9R2H+QjO9h647QxFlbmepSxneuc0bbGanOr0t+TF1BpPlx5jr3uXwaMZnq9HnS85jz7Q3SR0/mOsvDsDmdpYzvUl5uPl2ImH0+roxvntd1WjVsNu2wGd+57srLZf7MtocNMj5Kr7mMb+bTbMd4afx19SwZ3iMknzh1rZ3205+26299q11///tze0yMrrxG3f797fplL2vX557brk/L6z8tX37xDQAAAABAVUx8AwAAAABQFRPfAAAAAABURcY3vcFfpYzvriyqnOm9c2e7fvbZ/rc5bD011a7Hc9Ztxz450/tIyrvNoWizzeyW+c1IyMF+OXwy1zm0N29P7+WI6M3oTu//nkzuvD3Xu1KdM8FzZnjuSWlNgYjofVzD1vMRkJjDjPN95KDG/FrkvpfXX8iZ4Dlfu+sx5cd9uLC9lAGe7zMfUz4N6ejlPfs4dZkvpXzc/DHZlRtaygkftmb56jqNypfN9XgrZeHKyh1tg2R8D5sLPtsaXmgUxp8xOkK6FjY5nM69c67znq61jKhSXrMuz6nl+SwL5ZyQX3wDAAAAAFAVE98AAAAAAFTFxDcAAAAAAFURlMncZHw/kzJ3n0yZv/k6+TYPHBiuXreuXa9Z03tMp57a/zaGzfheUfieSKY3IykH+eWM5kOpPlioO97/PRnev0j1zwt1zgjPWdU5yy5vz48hvbcjojdPO9ejECxcyvTO8mubs81LGYB5LHTdRs74zseUjzkfU+6buVevLtQREZOpduoyX0ofY4N8zOV9SjXMp7kY0ywdJ9OjStv1MBaTHrYMeVFhzvnFNwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFUGZy0HO8M5ytnXOvj6ZjO+dO/tfJ+dtP/vscNtzvWFD7zGdcUb/6+THeTxl/pYyvfP2/DzL52IklDK+SxneuU7v9YjozfjOGd4/TfXPUp0zvvMx5dzpXOf9u7KrcxZ1KeO70DfnRL6P0jHm/fP2/Lxk+Xnp2j/ngueM71IWeu6b46len+qcGd7VN52qjCofc4y6YfOZjenlx2vOKNPDgBPyhh+YX3wDAAAAAFAVE98AAAAAAFTFxDcAAAAAAFURnLkU5SzpnNGds6rz9lwfThmuOQs753Pv3dt7THtSLuzu3e16//52nTO8c454qT4lDd18exG9jyMfQ35cuc73MT4+3PZSRjgsiFLGd37v5Pf37lTnPO6Icsb346n+SaqfSnXOfc7HXKpz9nVE7/NQqhdCKeM7y48r57rl65de65y/3XVZWguh5z5zn8unFROpLj3GrmOaLFyH+TJstuig+8BcmW3+rfFat67XV0Yyo0QPA5h/ZuYAAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKrI+F6Kckb3sZSHe/Ro/+25zvnZpUzvnOfddVmuc752zhU/dKj/9lKdH0PE8FnlOZd8MuXKTkz0r3Omd84AjxDERsFc5EyXcqPTeyfS+6Qn43tXqnd23GfO/c6Z3/k6efszqc7HPBf53IuR4T2sfIxdWeUvlPtJ3n/YfO5B9sl52znDO9elfO6VhetHREylOo+PUl/1Hf98OZnMbx+DzMaw48d4o0SmMgtJDwNYeP4aBAAAAACgKia+AQAAAACoiolvAAAAAACqIuN7KWpSDuzxlOuaM75L+dilLOxSNnZEb6Z3zss+cKBdH0nZs/mYS9tPJuM754yXcsnzfebnOYeujY+365wBDkPryqUuZVXn7SnTP/J7Jb2/4+lU5zzuX3Tc51OpzusA5BzxnD2d3mtFSyGveyEMOxZyUGTOyu7aJ62/0DNecj776lSnvthjkIzvdamW8Q0AAECZvwYBAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKrI+F6Kcsb3sZThm/OxD6WM1pyHnbOvS5neOQu767Kc8Z3vM+dl58eQ67z/yWR8lx5XPuZSpvcp6e0zOdmu8+sEQxsk4zvnHeftOT87v1dyZv+uVOeM7593HFPO+M63me8z546XeC/NjVLmd9c+efyk9Rp68txzpncen/n79pzpvbbjmHLOeB4/pYxvpzoAAADLkV98AwAAAABQFRPfAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVrPi0FOVFE/MijKXFLQ+kxcnyoo+lRSBzHVFe3DIvRjmdFjzLj6m0PT/Gk1ncMh/j2rSoWmkxy4m0KFtekBPmRV4ssFQPuzhhXtzyF6nuWtwy94R8m6XFLS1euTgGed5L42d34TZT749Vqc6LWZ7WcQyl8VNa3BIAAIDlyC++AQAAAACoiolvAAAAAACqYuIbAAAAAICqyPiebzmbei7k/OtS3vX+/e0653E//XS7fuaZdp0zvfPtRfTmaedc8XyMpeclb8/10ZQ7m+8vopxlnh/nmjXtekX6Xmh8vF2vXt2u82PMr1PXbc5WziGnMl3vk1Kmd8r8L2Y053zuk8n4zhnMpTofI6OrNH7yGE19MPLnReqzPZneZ3UcQ+7vOePbd/gAAAD08tciAAAAAABVMfENAAAAAEBVTHwDAAAAAFAVGd+jZpBM8OMpHzfnW+e87d272/WulOH71FP9t+eM75ydHTF8hnc27P6lnPOI3uPM2eaTk+161ap2nfO4Jybadc4EX7eu/zF2XVbK6Jbhvcx1jKGey3Leca4PpzrnbecM5vQ+iacLdURvrnPOhc61jO+lI79WOW87j8c8FvLnxempzhnz6fMrInrHbL4P3+EDAADQy1+LAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVZHwvtkGyrfM+OeP7cMrwzRnfOds6Z3o/8US7fjpl+A6S8X00ZfjmLOv8GEqPu7T9ZDK+8+MYH2/XK1e265z5PTXVrtev738M+XXquo/8OGV+L3N53HdlfOdxVcrXzpnMpTrnKeeM8FxHlHPG8zEPmenPIspjML+W+bUujeFhx+Mg++iLAAAA9PKLbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKjK+F9pss60jyhnf+/e3692723XO+N65s12fTMZ3zree60zvbJCM75x1njO9S9asadcbNrTr/DzkY8jH2HWZTG/66sr4zhneeeznDO6c2V3KS863V8oQj+g9zlzn97eM76WrlPldeq1L47Ur4zuP4fwZ5Dt8AAAAevlrEQAAAACAqpj4BgAAAACgKia+AQAAAACoiozvxTZsFnZExLFj7TpnfOds6z172vWuXe36iSfadc4EP5mM75xlPWyGd8kgGd/5OFek73lyVnqu169v1xs39r/9/Drk24voPe58TDnTOz9vMr+Xma6M7/T+78lMzhnJpYzvnLGc63z7+f4jyhnfpcxvRld+rUoZ36XXvpRR35XxXRrTvsMHAACgl78WAQAAAACoiolvAAAAAACqYuIbAAAAAICqyPiebzmjOWc8D5KFXcqzPpjyTvfvb9c5s/vpp9v1U0/1v36u8/1FRBxNua35mOdavv18/xERh7qyYvtcJ2enl7LOc5Z6fl5y5ndEb0b3KektmDO/V67sf32Z35XJ7/+OnPhZZ3znupTpnd9bHe+1nuMu1Swds30tc4/K46uUMR9RHsP6IAAAAL384hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqM7/lWyucu5Ux37fPMM+06Z1Hv2dOuczb1vn3tupThnbOqu/K05zvTu6Tr/vNzmZ/7LOdt5wzv/LyVstOffLL3Pqam2vXkZLtetapdT0z0r3MmOJXpylPOud+5zu/PYTO7S7fflTvO8jVs7x92/HZdlse0PggAAEAvfy0CAAAAAFAVE98AAAAAAFTFxDcAAAAAAFWR8T3fjqf80pwzfehQ/7rrslKm92wzvnOm9yAZ3/lxzrcmZR8PkvE9Ntb/NnJe9mwzvtev7z2m/FyuXduu16xp1/mYx8d7b5NlLueAz3cNs3Ey480YBQAAYHh+8Q0AAAAAQFVMfAMAAAAAUBUT3wAAAAAAVEXG93wrZXwfPNiuc65012XPPNOuS5nfw2Z85wzvXOfs7Ijex5nzs+davv2ujPGcj53l6+T98/Oen8f8vO/a1a6npnrvMz93XdnkL5QzvVev7r8/AAAAAOAX3wAAAAAA1MXENwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFYtbzre8eGFpccu80GRE76KK8724ZT7mvAhk14KMpUUa51pe3LLr/vNCkqXFLLO8uGV+3vLznhezXLOm/+1HRKxI3z2dkt6SeTHLhX6eAQAAAGAJ8otvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqMr7nW86ZLmV85xzpiN5M77nO+M5Z1iU5X3sUnEzueM74zo/rwIF2nZ/HtWvbdc7jnpjovc+c6T0+3q4nJ9v1unXtWsY3AAAAABT5xTcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUJXlnfE9SFZ1aZ/S9mEzvXN+d0TEU0+161272nXO+N6/v10fOtSujx5t16OY2b0Q8uPO+dn5eSq9djmfO+d3d12WrzM11a7za5fHU86Qz7nlue5yMtdhgXS9Fvn7ylzntj5sXbr9Qb4vzT1lmfaYKpX6Q2n7sOO367LSmAUAAAB/LQIAAAAAUBkT3wAAAAAAVMXENwAAAAAAVVneGd9dcu5zqc650Hl7zmQ+cKBdn0zG95NPtuthM76PH++9D3pfu5zxnZ/H/Dzn/O6urOyJiXadM73Xr2/XOVe8lPG9In2XlY8hb+8i43sR5ee+6/XKbTuNqcjv78lCna+f61WF+4+ISH2wmPGd92d05TFZqvOYzdvzeCqNv4jeMbu6cB8AAADgF98AAAAAAFTGxDcAAAAAAFUx8Q0AAAAAQFVkfGfDZnjn7bk+fLhd54zvvXvbdVfGd870zpnfw2Z851xouuXnKT+POZ89Z2N3ZamvTtm0Gza06/za5YzvnDuej3HlynY9SKZ3vg4jpOu1yW07Zybn/Ow1qS5lfOfbGy/UEb254vkYhs0AZ3SUMrxzncds3l7K+M7jM6I30zvXvsMHAACgl78WAQAAAACoiolvAAAAAACqYuIbAAAAAICqyPguyZneObc5Z3rn7UeOtOu5yPjOdc6FznXOGe/Knqb3tc552jlvO2d6l/LdIyKmptr1xo3tupTxncfTsK9lPuaI3sfNCOn6brKU8Z1fz1I+8rCZ310fG3lc5XUE8jHl/Y3BpaOU6Z3HR95+Mhnf+TIZ3wAAAJT5axEAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqM7yznHZcyvEt1znnOGd/79rXrrozvp55q17t2teucA53vIx/DsZy/S6f8POXnMY+NnL+dX5eIiPXr2/WePe162IzvnEOex2+uuzK+V/j+a3Tk12eQjO/Sbcw20zvX44X7j+jN7M61dQaWjtKYzBneuc7jpTS+8niM6B2za1Ld0dcAAABY9sx4AQAAAABQFRPfAAAAAABUxcQ3AAAAAABVkfGd5YzuUqZyzn3O++/d265zpnPenuuuy3IueL7PfEz5mHM2Nc/Jedh5LJTytPPz2pWlnjO882tZ2p7HwmTKvp2Y6F93kfE9wrpem5yhnPOzszwGcl7yVKrXpXpDqk/tuI9DQ9Y541tPGl15vOVM7lJmfN6e1jmItanO+d0RvTn1+Rj0MAAAAHr5axEAAAAAgKqY+AYAAAAAoComvgEAAAAAqMryzvjOGc0RvTnOBw+26wMHhqufeqpdP/NMu86Z388+23tM+RhKGd45WzpnVXc9bnqVMrzz8zqI/Nrl8ZIzvPN4yeMp53OvTXm5a9b0ryMiTlnebWC0jXVclr+vzBnM2Xiq8xjImd4bU312qtOaAhERsa9QZzn/viMPnxFRGj85ozuPp1yfleo83vL+Eb054bln+Q4fAACAXv5aBAAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqLO9V7U5mcct9adG2vDhlXpywtLhl3n///t5jKi1umRdZzHVelNHiloMpLW451rXwYJ/rR5QXt8zjq7S45cq0sGFe2DQfQ94/ImIyLxzHaBv2+8q8OOHqVG9I9empzgvudi1EOZHqfIz5OqmndS7i+UJ61vwoPe8R5cUtT011XqyytFhq3r6+4xjymM2nLoM8DgAAAJYbv/gGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKoi4zsbNuM7ZzDv2tWuh834fjbn6XYcw5Ej7TpnT+c6Z37L+B5Mfp5yXXpe8+sQ0ZvxnV/vPB52727XU1Ptuiuzu9/2iZzFHL2PgxEySHZx/v4yX6eU0ZwzlXPGdxqz0TVe8n3mTO+UZV/MaM49qrSdwQwynoYdP6em+sxUn5Pqs1KdM77XdRxTXodAxjcAAABlfvENAAAAAEBVTHwDAAAAAFAVE98AAAAAAFRleWd8d5ltxveTT7brnPmdM5v37GnX+/f3HlPOhc4Z36Us6kGyp+lVet7GxobbHtH72pXGVx4vkynrdjzl756S3tI50ztnhEfI+B5pXdnFw+ZdD5vxfSjVuV903V/O9E7jOlKfK370yPReGINkY+fXKo+fDanOGd/npfrsVA+S8b26cEwyvhfLIEuGlE5JYBjDjh/jjZLSGNHDmEt6GMDC84tvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqSyvje65Drrpu71jKqs2ZzAcOtOucyZwzu/P2Z59t1zm/uytvuZTJnbOku7KlX2jlyuH2Xy4WIkQt30d+vfN4yOMlj6fVKfs2Z3gfSnnNOcO+6xiGfR6Mnzl0Ms9l6Trp/R4p9z3Wpjr1vJ6M765+dLhQ59zwnAGe5T6YM8RznffvOsbSuM7bFyNUMb+Wpdc2b8+vda7zR37XKUC+zhmpzhneuT4r1TnT+/RU54z5nCEeEbEq1fk7ez1ovpxMtq08UhbSsGNUXvPy0vX6DjtGYD7pYcBJ0wAG5hffAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVllbG91zrysQp5WXnfOxT0lM4Pt6uJyfb9dqUp5szxEt53hERB0v5uEOS0fychchI2rChXZ92Wrten/Juc2b3RMpnzuMtj8cVK/rXXWR8Vya/PmnMRMqJ78lczrrGR870zlnyOZM797l8DKVM8Fzn/Tuy7ItZ5flxday3MOfya1PKrs7bc53z29PnT8/znLd3XXZuqs9L9TmpzhnfOSM89cBYl+p8jBHL/VRllIkWZNTNNj+X+nnNGWV6WGUslgKLwi++AQAAAACoiolvAAAAAACqYuIbAAAAAICq1B2cORf5SDkTea4zvo+nHNmuDOZ8H4dznu4syWh+zkLkaeXXP2d+r0t5t8NmfOfxmcfTIK+1XLHKDJvxneWPia7vS3OGdykfO9/GmlTvS/XeQj1ID8vHVKrzbc7H+2LYDO/8WqT3e8/zmPOzc3573t61z9mpzhnfOQM8758zvlMP7Dnmrtzxuk9VRslcZIeW8kh9xLCQ5OEuL4vxepaWh4IXKo2X0vgxvpa40vpxEb3zR6ee2q7PyOfWVGvVqnad547yWMnzhhrGDL/4BgAAAACgKia+AQAAAACoiolvAAAAAACqUldw5rBBfl375xycYTO+cw7P6pSfOz3d//5yZnNEb67z0aO9+8yG7J/nLEQwYB4POcN7zZr+dSnjO4/HPF691stQKeM7Zyzn7TlzuaNHReprPXnYpazqnDP+dGH/LPfErozxUt/Mx1zKKZ8LpYzv/LhzXXotUw5cnJ7qjR3HlPfJmd2l+qxU5xzCPJ7SZ2bn+KrrVGWUlbJGB1k2Iu9TqmWAcyJzMb6GreXrLm2l17frstLyOHl7qWd13Sc8rzTehh2fg4x5OfQjpJTZHBFx4YXt+r/9t3b9ohfN7TExuvLczubN7fq//td2ndeT68qQX6Z8NAMAAAAAUBUT3wAAAAAAVMXENwAAAAAAVVnawZmzzfQeJEiyFHCZM5ZzPZnzTJOc29O1f87qOXas/20OS9DXcxYiWDTneuXXu7S9lPFdCoIb5LUWsFqZ/JrnTOX8MZB7UM7vznncg9xnzm1O47iYI57HZO6BB1J95MSHdsLbzI8zP4aFeF/k76JzLlt+XvJrmV+bnBuY87ZzPndExDmpzpndZw5Z5/vM4y0/5q7v431GDWrYj/PSKU4pa7RrWZJ8WlPKFpU9yol0jYXSUiZ5+7BLoZROo5hfs+1hpZ4WMfyYGTb3PddOq3mhPN5mW5d6WITP1ZGS/5Y/7bTefV760nad1wjbt29uj4nRlT/EciZ8zvg+9dR23XWivkw5nQMAAAAAoComvgEAAAAAqIqJbwAAAAAAqrK0M76zYTO9u0LXcgjWsEFwObcpy/vnzKau/O582XTOop0lwV/PWYgQvtJ4KtV5/ORM8GHDLLsII6xMfn/nOn//eTKv/1SqT011qWflY8jHmD+qSlnXuzvu42iqDxfqnBN+vOM2Zys/rvz5kev8OHN9eqpzPnfO9M7bu/bJ2YO5Xp/qPBZyfnvp+3afR3Np2GzaUqZ36SMqYvb5uPkYfCQtX4NkfM91Pu7JZNDLqZ8/s8387jrtnW3Gt9eb2Zjtn4KluivXvrTkkzG8gPIA6FrfLec05xfwyCBrGVGF/ObMc43r1/ffPsjczzLhF98AAAAAAFTFxDcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUJW6FrccdmWGrtUf8goRecGBqbRwV2nhyaNpQbXjx4eruy6z0tPSVVo5bNh6zZp2vW5d/+15McyI7tXJWMZKfbTr+9I8rtK4i7xob76PvPBGHpNpEeCeBRTzgot7o1dpMctSPR+LW+ZFOkuLW5bqU1O9cci667INqc6LWebXJr92pcVVWUjDLtJW+gjKi8B1XTbsfVpki+d1jYXZLgxXGtOlReBYXKV+kV+/rj/1TmbRXpgrs128Mn/GlnrYiS5jkeQXo+tEqvT3fp5vol7Dnqg7qT4hbRAAAAAAgKqY+AYAAAAAoComvgEAAAAAqMrSTjHLmTWl7OtBMm9yzlLO+M4ZyqWcnVIGeM7v7spsypfJ+F66SuOlFE6Y959IGb9r17brnEmfx3NEb3icLCj6GiTjO+c+ZznPrnT91Hd7Mr33pXp/x30eSnXO8M7bc70QGd/5/ZkzvPP2XOfnKdc5nzvXXdfJeeo5vz2/Vvkx6SdLSekjaJAs3Nnm4/oI4nldY2G2ebilfFyZ36Nl2DUB8uubT5Mjek+VN6SlLA6lj//8p1yu859yeTu80GzXKcjjNf+pt7rjFLzU9/S1BTTIfJRQdphz3lUAAAAAAFTFxDcAAAAAAFUx8Q0AAAAAQFWWdsZ3SSmwqis/aVXKmu0KynqhHLyVw+RKGd45r3uQjG+WrjwmS+GRpcDVUiZ9Hr9dYYcyvhlK1/jIuc45Bzpvz30171/K9D6Q6pzHnbdHRBwsXKdUz0doZ842L2V457r0PObtpbrrstwz8muZ67pPK0Zd6SMk53rmj5C8jMmZZ7br889v1/kUJyLijDP67zNszfLVdTpSyuQuZXyffnq7zmM6j9+cB51vr+s+8zExuGF7WH7u8+uVe1hExP60DEjO5M5jRA9jLg07pvP29Wl5lk2b2nXXmM+f7bmPzXZtDoBR5xffAAAAAABUxcQ3AAAAAABVMfENAAAAAEBV6kp0KmUT5zztrv27wvteKIdg5Uzlqal2Xcr0LtUnuow65DE4bJ3HY65zZv0g4ZQyvumra3zkrOr80ZKzqXNPy4GYOU/7aKHO++d87ojejO9h64XI+B42k7uUx51fh0HyuPM+Oaw2v/6lmrnS1ZpLebgl+SNiw4Z2fc457Tpn1+ZToIje/NzSUiel7fBCOe922LzcnI/7X/5Lu875uINkfJeOiRObbQ/Lr2/OMj7rrN7r5B6Tl7/Zt6///qWepYfRT2n5plJPW5OWc8mf02ef3Xufue/lz37rEgC1c2oGAAAAAEBVTHwDAAAAAFAVE98AAAAAAFSlrozvklJeckRveF8OvcpBcMNmeGfyu+mnlLddCkfMddftDbIPzOgaH/my0neque/lcMFc51DVnAme65x13XVZDifOedk5Jzzfx1zIjysf07B1vr38OpSe567LfD8+yobNx82nHKWM75xlm/c/9dTe+ziU3jql0yT5uAxj2NOcfBq/OrX6jRv71zK+F9awPS2/Hjnju6uflPrebHuYP+3opzTGS9tL4zfXEb0Z36XpDoDaODUDAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKoy1jSSyABYTPljKOdp55DOXOf9j3Xcx5FZ1vMRPJxDFVNwY08ued6ew2bzsh2ljO+u777zZTL/R1lpWZFSnTO8Dxxo1wcP9q9zFm5ExLH09pvtMcILlZbrKeXj5mzbnPldqvNSP133kY/hlOW1otJQZtsP8vbckw4f7r1Ovixfp9TDSsegh9HPIEuO9due87gn03IvXT0qX5brUpa+HgYsdX7xDQAAAABAVUx8AwAAAABQFRPfAAAAAABURcY3AIssfwzlPO3S9lId0Zv7neujhe0LkfGdQxRLGd75+qUM7xwk2fXdd95HxvcoGzZ7NptOwzpnfpfqfP2uy2Z7jNDPsPm4Ocs25+XmLNu8vSsLt3QMpfzc5Wy2/aG0bkFXjxq2rw3bo/Q0+in1i9L+pczvrjzuUt/Tw4DaaWMAAAAAAFTFxDcAAAAAAFUx8Q0AAAAAQFU6UqAAYDGVcqZLddd3uvmyUp52KWd8LpSOu5TZXaqHfd6oTc7tzNmzpfzjYW+/6zbk4zKX5jsft7T/sPfP7Mx3D+sy2x4Gc6nUg/J4HWQdAn0MWG784hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqMbwBGTCl8cD6yqkcxxHO2j0uII22lXM9h83G79s95uPJxWUzDjvmTyYiWl7tw5iPzu3SbmZ7GfBq2n5xMxvds7xNgqfGLbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoisUtAVhkFnGExTDswn/DLgLXxcJwzKdhF2nL+w9bs7hOZtG+uehjMCpOpkfpY8By4xffAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVZHwDACDPGFjS5BsDAJlffAMAAAAAUBUT3wAAAAAAVMXENwAAAAAAVRlrmqZZ7IMAAAAAAIC54hffAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVTHwDAAAAAFAVE98AAAAAAFTFxDcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUBUT3wAAAAAAVMXENwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFRPfAAAAAABUxcT3EnLZZZfFZZddttiHMbRt27bF2rVr5/Q2l+pzATVbqu9LPQqWh6X6vtSjYHlYqu9LPQqWh6X6vtSjMPE9jx5++OG4/vrr46KLLorJyclYv359XHrppXHrrbfGwYMHF/vw+rrgggvizW9+82Ifxry544474qqrrooXvehFMTY2Ftu2bVvsQ4IFp0eNLj0K9KhRpkeBHjXqPvOZz8TFF18ck5OT8ZKXvCRuu+22xT4kWFB61GjToxbOKYt9ALW6++6746qrroqJiYm4+uqr4xWveEUcOXIk/umf/ine//73x7//+7/HnXfeudiHuWx9/OMfj3379sVrXvOa+PnPf77YhwMLTo8abXoUy50eNdr0KJY7PWq0ffrTn453vetd8da3vjXe8573xH333Rc33XRTHDhwID74wQ8u9uHBvNOjRpsetbBMfM+DRx55JN7+9rfHpk2b4mtf+1qce+65M9tuuOGGeOihh+Luu+9exCOMOHbsWExPT8eqVasW9TgWy7333jvzK6W5/mcvMOr0qNGnR7Gc6VGjT49iOdOjRtvBgwfjz/7sz+LKK6+ML3zhCxERce2118b09HTcfPPNcd1118Vpp522yEcJ80ePGm161MITdTIPPvGJT8T+/fvjM5/5TKvJPO/FL35xvPvd756pjx07FjfffHNs3rw5JiYm4oILLogPfehDcfjw4eJ97dy5M975znfG2WefHZOTk/GqV70q7rrrrtY+P/7xj2NsbCw+9alPxfbt22fu5wc/+MGsHud99903889cJyYm4vzzz48/+ZM/OeE/m/nRj34Ub3zjG2NqairOO++8+NjHPhZN07T2mZ6eju3bt8fLX/7ymJycjLPPPjuuv/76eOaZZ4rH89hjj8UPf/jDgY5906ZNMTY2NtC+UBs9So+CUaZH6VEwyvSo0e5RX//612PXrl3xR3/0R63Lb7jhhnj22WcXfcIP5psepUfR5hff8+BLX/pSXHTRRfG6171uoP2vueaauOuuu+Jtb3tbvPe9741/+Zd/iVtuuSX+4z/+I774xS+e8HoHDx6Myy67LB566KG48cYb48ILL4wdO3bEtm3bYvfu3a1mFhHx2c9+Ng4dOhTXXXddTExMxMaNG2f1OHfs2BEHDhyIP/zDP4zTTz89vvWtb8Vtt90WP/3pT2PHjh2tfY8fPx6/9Vu/Fb/+678en/jEJ+Kee+6Jj3zkI3Hs2LH42Mc+NrPf9ddfH3/9138d73jHO+Kmm26KRx55JG6//fa4//774xvf+EaMj4+f8HiuvvrquPfee3uaF9CmR+lRMMr0KD0KRpkeNdo96v7774+IiF/91V9tXX7JJZfEihUr4v77748/+IM/GPbpgCVDj9KjSBrm1J49e5qIaH7nd35noP2/853vNBHRXHPNNa3L3/e+9zUR0Xzta1+buWzLli3Nli1bZurt27c3EdF87nOfm7nsyJEjzWtf+9pm7dq1zd69e5umaZpHHnmkiYhm/fr1zc6dOwc6rk2bNjVXXnll330OHDjQc9ktt9zSjI2NNY8++ujMZVu3bm0iovnjP/7jmcump6ebK6+8slm1alXz5JNPNk3TNPfdd18TEc3nP//51m3ec889PZfn5+L5y05mSE9NTTVbt24d+nqwFOlRehSMMj1Kj4JRpkeNfo+64YYbmpUrV3ZuO/PMM5u3v/3txduApUqP0qPoJepkju3duzciItatWzfQ/l/+8pcjIuI973lP6/L3vve9ERF9/5nDl7/85TjnnHPi93//92cuGx8fj5tuuin2798f9957b2v/t771rXHmmWcOdFyDWL169cz/P/vss/HUU0/F6173umiaZuZbrBe68cYbZ/5/bGwsbrzxxjhy5Eh89atfjYjnvrHbsGFDXH755fHUU0/N/HfJJZfE2rVr4+tf/3rf4/nHf/xHv1KCAj1Kj4JRpkfpUTDK9KjR71EHDx48YW7w5OTkCWMQoAZ6lB5FL1Enc2z9+vUREbFv376B9n/00UdjxYoV8eIXv7h1+TnnnBOnnnpqPProo32v+5KXvCRWrGh/f3HxxRfPbH+hCy+8cKBjGtRjjz0WH/7wh+Pv//7vezKP9uzZ06pXrFgRF110Ueuyl770pRHxXOZTRMSDDz4Ye/bsibPOOqvz/nbu3DlHRw7Llx71HD0KRpMe9Rw9CkaTHvWcUe5Rq1evjiNHjnRuO3ToUGuyDGqjRz1Hj+KFTHzPsfXr18d5550X3//+94e63kIsEDSXb6Djx4/H5ZdfHk8//XR88IMfjJe97GUxNTUVjz/+eGzbti2mp6eHvs3p6ek466yz4vOf/3zn9rn8dhCWKz1Kj4JRpkfpUTDK9KjR71HnnntuHD9+PHbu3NmawDpy5Ejs2rUrzjvvvDm5HxhFepQeRS8T3/PgzW9+c9x5553xz//8z/Ha1762776bNm2K6enpePDBB2e+GYuIeOKJJ2L37t2xadOmvtf93ve+F9PT061v2Z5fSbbfdWfr3/7t3+KBBx6Iu+66K66++uqZy//hH/6hc//p6en40Y9+NPOtWkTEAw88EBERF1xwQUREbN68Ob761a/GpZde6lsumEd6VC89CkaHHtVLj4LRoUf1GqUe9epXvzoiIr797W/Hm970ppnLv/3tb8f09PTMdqiVHtVLj1reZHzPgw984AMxNTUV11xzTTzxxBM92x9++OG49dZbIyJmBvr27dtb+/zlX/5lRERceeWVJ7yfN73pTfGLX/wi/vZv/3bmsmPHjsVtt90Wa9eujS1btsz2oZzQypUrIyJaGUZN08w8ri633357a9/bb789xsfH4zd+4zciIuJ3f/d34/jx43HzzTf3XPfYsWOxe/fuvsf02GOPzTRZ4MT0qG56FIwGPaqbHgWjQY/qNio96g1veENs3Lgx7rjjjtbld9xxR6xZs6bvcw410KO66VHLl198z4PNmzfH3/zN38Tv/d7vxcUXXxxXX311vOIVr4gjR47EN7/5zdixY0ds27YtIiJe9apXxdatW+POO++M3bt3x5YtW+Jb3/pW3HXXXfGWt7wlXv/615/wfq677rr49Kc/Hdu2bYt//dd/jQsuuCC+8IUvxDe+8Y3Yvn37wAsanMhDDz0Uf/7nf95z+a/8yq/EFVdcEZs3b473ve998fjjj8f69evj7/7u73qylZ43OTkZ99xzT2zdujV+7dd+Lb7yla/E3XffHR/60Idm/snIli1b4vrrr49bbrklvvOd78QVV1wR4+Pj8eCDD8aOHTvi1ltvjbe97W0nPN6rr7467r333oEWFPjSl74U3/3udyMi4ujRo/G9731v5rH+9m//dvzyL/9y8TZgqdKjeulRMDr0qF56FIwOParXKPWo1atXx8033xw33HBDXHXVVfHGN74x7rvvvvjc5z4Xf/EXfxEbN24c4lmCpUeP6qVHLXMN8+aBBx5orr322uaCCy5oVq1a1axbt6659NJLm9tuu605dOjQzH5Hjx5tPvrRjzYXXnhhMz4+3px//vnNn/7pn7b2aZqm2bJlS7Nly5bWZU888UTzjne8oznjjDOaVatWNa985Subz372s619HnnkkSYimk9+8pMDH/umTZuaiOj8753vfGfTNE3zgx/8oPnN3/zNZu3atc0ZZ5zRXHvttc13v/vdJiJax7B169Zmamqqefjhh5srrriiWbNmTXP22Wc3H/nIR5rjx4/33Pedd97ZXHLJJc3q1aubdevWNa985SubD3zgA83Pfvazvs/Fli1bmkGH9NatW0/4+PLzB7XSo56jR8Fo0qOeo0fBaNKjnjOKPer5+/mlX/qlZtWqVc3mzZubv/qrv2qmp6cHvj4sdXrUc/QoxppmgJ91AAAAAADAEiHjGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoCqnLPYBANSqaZpWPTY2tkhHAktDfs9E9L5vvK/mjucShqNHAQAsLX7xDQAAAABAVUx8AwAAAABQFRPfAAAAAABUZazpCqsDoJjTOdc5noPcnuxQRslCv0do06OgPz0KAGB584tvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqMr4BojfnM6KOrE8tntmY7/eAfN3B6VHQS48CAKAfv/gGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpyymIfQH8597FUTxe2w3KTv9vqyqbMl5XqOg2W2znbnlLqYcPfX47HzQ9jbEwfZDZm+/4vXX/42y+N+Vp7lh4FXfQoAEaDdSFYbpbKmPeLbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoyliT08gXVT6U44X6SKoPpvrYrI8IlrbxVE927LMq1SsL9WguWDBbXa2wd3GGYRd6ywvu5jr3tGEXlhvkOiPU4lmCZrv4bWn/3F+6vo8fZJHeYbYvTXoUdNGjAFgaTmYhwKWyeCBLQ2k8zfV4G+T2FmKM+8U3AAAAAABVMfENAAAAAEBVTHwDAAAAAFCVUxb7ANpyzmPO6D6a6j2p3p3qnPkNy81Uqjd07LM+1Tnze9jsyqXp5LKkSlm1pXUJck8r5e32XtY07XpsrJThC8Movf/z9mHrvA5B7j9d9zns9jroUdBFjwJgYQybRbwYmd4jtYQfC2qQsVLaZzEy5Oc7ZzzCL74BAAAAAKiMiW8AAAAAAKpi4hsAAAAAgKqMWMZ3znnMWZPPpvqJVP8k1c/M+ohgaTs91ed37LOycBt5ex3fl81NdlTuWTkv90Cq96Z6f6pznm6uI8prH7SPoWny/jC4sbF8mpD7Qc6/zfvn7blel+q85kBExOrCMZR62NKkR0GZHrXElLJn8/ZSPW2NAJaY/Fk+27p0+8xOPhcrbM+K+3dcf9j7KF6f5WOAsZL3KI6XWfaUgcbzAvStOmawAAAAAADg/zLxDQAAAABAVUx8AwAAAABQlRHL+M7ZkznT++lU/2eq/zXVj836iGBpuyjV/1/HPhOpzhlLeXsdTiYvN2fu5vzcsbGcZbsv1XldgifT7R9Mt3eo4yjyPofT9sNpe1cGLwwq593mfpDrdtZt00y26rGxvP3stL3r+/j2qUrTtN+7vdepI91Qj4JB6FFLSimj+3j6W/Bo6g+HUs85nPrLkFm4sODyZ/vERP961ap2vXJl/5rFlXpaz3otuecN0rMGyAWHTh1/S/RcMuy6AStW9K/nIK/75NY16s8vvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqoxYxnfKPEo5kRF7Uv1IqnPG9w9nfUSwtO1O9Rkd+2xK9ZpUnzpXB7Pk9cZN5Yy1Y6nOPeyZVLfzdMfG9qftuY6IOFCo833mfF0YRv983N5+0a7Hxtam7WvT9pRdGRs6jmFduk4+V+B5ehTLjx61pORs2mPH+tfPpvWe9qV1Cfbu7X/7MGpyJve6df3rUtbtPOTrLic9Gdyl56+Ut50yvMdyT8vrFnT1rNJaCDK/GVTXeC71jFKm93haW6W07sAAPWno9+FJ8ItvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqI5bxnfOJjqf6SKpztuSuVD8RsLydl+quPNb8vsrvu+WRG5azpSIGyZfK18l1ynHrybbNr8fewvauy1L+ZTFPF4ZRysvN/SNnSPfPum2aQ616bCy/Z7puY3n0pEyPgi561JKSs2pz/u2h9vMdu3e36yee6F/n24dRk/NxzzyzXZ99drsufc7nPN2cx0tfQ2cJl/K3c4Z37mkH0jnQ8fx3d8dt5n3S9qaUAc6y0TOeu/pBvqyU0X1KmjJenc67JifzQfS/v473XD7u+cj81hkBAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqMWMY3MLdmn4e0XJxcdlTOUMvZkjlLNOW89WTh5npPx33uK1wn5+nKz2U2cn7u4VTnvNuOrMKW9vftY2P5PZHfMxHlnOrlkWWoR0EXPWpJyVm0B9P7f0/qKT/5Sbv+z/9s1z/8YbvO+bowanIe7ktf2q7zGM6f/bletWpujqtCc5ITXMr0zusUPJvOcfI6BbvSmnSH82dW9I6BQj2Wj0HG9/KV87RzPnfXZbmH5HUI1qS1U04/vV1v2ND/mPLt5QzxDnOR6Z35xTcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUJUllvGds17kF8PseR9F9ObARQySLzXb/NwDqc5ZuDkrNyJib2GffBv5PmAYOXswj+k85rOc45ZPOwbJz833sTyzC/Uo6KJHLSk5izZnfO9N/eOnP23X3/teu/7mN9v1kSMnf2ywENata9c543nt2na9fn27npho11NT7bor33ke8nKXgkFygos54Pn5PJ7WiUg9p9nXPucZe+KJ9v4//nG7PtBxDpT7Yq7zmMm1jO/lK2d8537Rddnq1f3rU09t1/k9kDPDc52PKdcRxR41F3n9fvENAAAAAEBVTHwDAAAAAFAVE98AAAAAAFRliWV8A8yPwbKicmZau+7N4G1nWY6N5SzSnOv2bKpzFm5Eb17unsJ18m3CMI6m+njnXv9P/j59PNWrUl3K5+3KdVue2YV6FHTRo5aU6ZSHnjO5c97t00+368cfb9cPPtiuc9YtjJoNG9r1eee169272/WhtM5AzsmX5zywk1srJck97Gj7M2gs97Bdu9r1Y4+1630da6XsT+dJz6bzpHwfOQPcmFi+cn52zuuOiFizpl3ndQLyOgNnntmu8zoFp53W//bG83nW8E4m0zvzi28AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIrFLQHmSO+CVnmRrbSIU6TFSIoLyUX0LgyX67xIStficzCo6cL2lanOpxUTqZ5MdX5P9C5MNzZWOgYGpUdRHz1qSckLVJVqgDkyFwvk9SwcWVjssmfhybyY5d69vfeRLystdmlxS543yOKWeRHovGBuHtP5NkoL7ubrj8h49ItvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqMr4BTlrOrMo5nzkLNOW+RcrY6snTzXVEOWO3lK8Ls5G/Lx9Pdc7LzWM4j/n8nujNz+19n41GVtzSoEex3OhRVZH5Te2M8UXVpPzhnhzwnE9cyvjOecc5DznndQ+S8Z1zwUuZ3yxfefxOTfXuU8rkzvJtlDLCRyTTO/OLbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKjK+AQAAABaTzO95k/O8Izoyvcs30r/Oecc5Dznncee87ojeTO89e/pfJ9/miGYsswDyeM7jMaKc6b0i/TZ6/fp2ncf08bT2Sb790ntmgfjFNwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFRnfAAAAAFRp6DzvLjmfOOcZ50zlI0fa9cGD7Trnc0f0Znjneu/e/ttZvvIYL+V5R/Rmep+SpojzmM1jupDxnbP1F2sVA7/4BgAAAACgKia+AQAAAACoiolvAAAAAACqIuMbAAAAgCrlrOGIOcj9zreZ845LGd+5jujN/c51KQOc5WuQ8VzK9J6YaNd5jB492q7zmB+RTO/ML74BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKrI+AYAAACgSrPO847ozfTuyA0fav/S9QfdByLmZnzNss73ODYH4zfn85/Me9kvvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqsj4BgAAAACo1Wxz6QvmIEm/9zbnIJ/fL74BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqIqJbwAAAAAAqmLiGwAAAACAqpj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKpyymIfAAAAAAAA82RsbLjtpf2TJt/cUNc+wW027VsdG/KYIvziGwAAAACAypj4BgAAAACgKia+AQAAAACoiolvAAAAAACqYnFLgOhdNCHiZBZOmO3+pXou7hNmozTehh3T7bppem//JNYvqZIeBYPQowDoNSfnUcMu/Ffaf5D79yHDoPJYWdHxO+fSPsPW6fZ6RuscjN+TWcwy84tvAAAAAACqYuIbAAAAAICqmPgGAAAAAKAqMr4B4mSzo0rZoCtTvSrVE6leXagjIqZSfSzVxzuuAydrbarz+Mt1aQznMT/eqsbG8nsmovc7+uWZdahHQRc9CoCyucgJ7rjRdr0yfUasSudVq1f3ryMiptLn1rF0HnXceRQnkPO31+ZzpI7L8ngrjdHx9nlRz5jvyhUvyPn7+b1a2j4Iv/gGAAAAAKAqJr4BAAAAAKiKiW8AAAAAAKoi4xsgerOjIgbJjyrl5+YWm/Nzc67bmlQf7rjPYfNycx5p7+OE/yeP4ZyPm7PiSvm6eUznMd9+TzRNb35u7/sw577l/Xtuogp6FEToUQCcjJM7j+q5QrvOecanpPOqUsZ3zleOGD7TO2csdzxOKpXHY667Mr7Xreu/T67zmM1jOo+/0jF1KL0P5yKf3y++AQAAAACoiolvAAAAAACqYuIbAAAAAICqyPgGiO7sqJwFV86Xyt8l5hY7meqcLXok1Uc77mO6cAw5f1SbZzZK+bnrCttLeboTrWpsrGu8lnLf+m6uhh4FXfQooCLymefNnJxHlfKLc8b3RPszpCfT+2jHedR04TwqZyrn+2T5ypnzXRnyOcO7lPmdbyON6SaNx7F8DOk90nS9x4b+e2Z4fvENAAAAAEBVTHwDAAAAAFAVE98AAAAAAFRFIBDACfTmS+Xcvbw917nFppy3nmzRY6kuZeVGlDN7833KDqSfPIZXp7qUp7u+sD1fP2dKd52W5DFeet8tH3oUy48etaQNm18s75jaGeOLaugs4bx/zjPOeduT6TMk5yeX8rwHuY+cI25MLR+l8bg6nyNF7xgsZX4XMr57xmMh47vrHVcascOvadTLL74BAAAAAKiKiW8AAAAAAKpi4hsAAAAAgKossYzvnP4ivwhmz/soojc7KmKQ/Ki8vZRlW8oiPZm83PFU57zcg6n2etNPKT93TapLebopJ+6k8nPl5UboUfAcPaoqua/JpgXmyUmdR+XtpUzl8XTOkzOWc35yl9Jt5ozlg+k8Sh9dPk4m43tNOk8qZX7nOuXWjw2Z8d2l9D48mUzvzC++AQAAAACoiolvAAAAAACqYuIbAAAAAICqLLGMb4D50ZUdlbPgencp5ee2M9maJmVijZVy3rq+m8xte1Wqcx7p4VTLfaOfPKZzHvOwebq5zmM+j9ecBx3R+z5Ynvm5ehRE6FFL3MqV7TplhfZkiZ51VrvevLld79rVro8cOfljg4WQx3ge02ee2a6nUo9alT5T5yD7drkY7Dyq8HzmHpZejya9vmO5hx061K4PHOi9j7xPrg8f7l+zfOV87ZwH33VZ/hzOueDr17frM85o17lH5Uz6dExdZ/kL0cX84hsAAAAAgKqY+AYAAAAAoComvgEAAAAAqMqIZXzndJeUodSTq5dz+U5LdcrIgmXn1FTnrMuI3vdVft/VmR03SKZbMeetkJebs0LHxs5N23N26KFCHdGbh5vzLHN9NNXyc+knj/k8pnNec65zllwe47nemOqUfdlxDE3T7lG1xlvqUdBFj1rSShnfGza06xe9qF3nLNucRXrs2MkfGyyEPOZzxvf557frU09t13nM5/eUhjNjTs6j8vacoZzyjMfWpXUipqf77t+5LsHRo/3r3Of0PZ6Xx2fuDxG9YzDVTR7TueecluZcU8Z3c0qaYk7HVP7bZX74xTcAAAAAAFUx8Q0AAAAAQFVMfAMAAAAAUJURy/jOGTQpT6Ynw/vFqd6X6v866yOCpe2CVG/u2Ce/r/L7rs7vx+YmX6qUn5uzQPN95u05o+14x32W9mnXTdN1GzCYsbH253Jvdm3+3M6nFaXted2BvHZHRH5fjY3l912deZZ6FJTpUUtMzhtds6b/9ixnjZ53XrvOebowanL+7ca0jsAZZ7TrnHs/kdYpyJm+zJiT86h8G6VM9VJPy9uPd5wD5T5WqBt9j/+rZ8x39Yd8Wc7gzttzz0rrFDSr2mupjBUyvhfLaBwFAAAAAADMERPfAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVRmxxyzwP37WAzAvlxS0nU/2y2R0OLHmnp/r8jn1OTXVeyGn5fj/WNE2r7l0jpfTcpAVoehbRys91XpykiV6lffIx998ObXmQt+vehYJyXVrULW/PpyF58cWI3vcNz9OjWH70qCWttFBcWiQr1uYFdpO80F+jfzDi8pifSvMd69a16zzGx1MPGpGF45aq3vOowoKYpcUD88J+pZ7X1bPyZYV6TN/jRLrGc76sNKZLYzyP6bz/XCwyOwd0SgAAAAAAqmLiGwAAAACAqpj4BgAAAACgKmNNDjZaVMdTfbRQ7y3UB2d9RLC05XzWDR37pCy5nvzKXC/n78tK7bJ/Vm3TtLNvx8aGy8Kdi2OA4ZTycofdv103TbufjI119ZfZHsNyokex3OhRS8r09HD10fS335Ej/etR+rMWuuS825xrn+uc6b1E8nSrMWRP6ckML/W4QW6/tI++x4kM0g9Kmd9ZKQP8ZHrQWD73GjJ7fwDLeQYLAAAAAIAKmfgGAAAAAKAqJr4BAAAAAKjKiGV85yzJUn041SnnrScTHJabnM890bFPypLr+T4sZckt52zKOc+i7X97Xd05R1zlfcoRWCPU8hlBs3t/Dz8eh7+/8n3oUQt1e3oUC0+PWtLm+s/OUfozFgYx3xncMr7n1jz3mK5bz69g3scrTHVkfAMAAAAAwHBMfAMAAAAAUBUT3wAAAAAAVGXEMr6HPZS8f6mG5SbnIXXlIw2bmSRZDAAAAIDnzEU+d+k2TuY+/OIbAAAAAICqmPgGAAAAAKAqJr4BAAAAAKjKiGV8AwAAAADA7PjFNwAAAAAAVTHxDQAAAABAVUx8AwAAAABQFRPfAAAAAABUxcQ3AAAAAABVMfENAAAAAEBVTHwDAAAAAFAVE98AAAAAAFTFxDcAAAAAAFX5/wF6sUDH6Hy9iQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the 1D label arrays from the dataset labels. Note: support_dataset.labels is a tensor of shape [N,2].\n",
    "num_distinct_shape_labels = np.unique(shape_labels).size\n",
    "\n",
    "# Should have 3 labels for shapes (0: square, 1: circle, 2: triangle) and 3 labels for colours (0: red, 1: yellow, 2: blue)\n",
    "print(f\"Number of distinct shape labels: {num_distinct_shape_labels}\")\n",
    "num_distinct_color_labels = np.unique(color_labels).size\n",
    "print(f\"Number of distinct color labels: {num_distinct_color_labels}\")\n",
    "\n",
    "# Prototypical networks expects nunpy arrays for labels\n",
    "assert isinstance(shape_labels, np.ndarray), \"shape labels should be a numpy.ndarray\"\n",
    "assert isinstance(color_labels, np.ndarray), \"color labels should be a numpy.ndarray\"\n",
    "\n",
    "# Check tensor shapes and values\n",
    "assert kand_proto_dataset.images.shape == (shape_labels.size, 3, 64, 64), \\\n",
    "    f\"The shape of kand_proto_dataset.images should be (number of shape labels, 3, 64, 64), but got {kand_proto_dataset.images.shape}\"\n",
    "assert kand_proto_dataset.images.shape == (color_labels.size, 3, 64, 64), \\\n",
    "    f\"The shape of kand_proto_dataset.images should be (number of color labels, 3, 64, 64), but got {kand_proto_dataset.images.shape}\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of shape labels, 1)\"\n",
    "assert kand_proto_dataset.labels.shape == (color_labels.size, 2), \\\n",
    "    \"The shape of mnist_dataset.labels should be (number of color labels, 1)\"\n",
    "assert kand_proto_dataset.images.min() >= 0 and kand_proto_dataset.images.max() <= 1, \\\n",
    "    \"The values of kand_proto_dataset.images should be between 0 and 1\"\n",
    "assert np.all(np.isin(shape_labels, [0, 1, 2])), \"Shape labels should only contain values 0, 1, or 2\"\n",
    "assert np.all(np.isin(color_labels, [0, 1, 2])), \"Color labels should only contain values 0, 1, or 2\"    \n",
    "\n",
    "for batch in episodic_shape_dataloader:\n",
    "    images, shape_labels_batch, _ = batch\n",
    "    shape_labels_list = shape_labels_batch.tolist()\n",
    "    label_counts = Counter(shape_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch shape labels:\", shape_labels_list)\n",
    "    print(\"Shape label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "min_range = min(shape_labels.shape[0], 10)\n",
    "for i in range(min_range):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Shape Label: {shape_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Inspect one batch from the color dataloader\n",
    "print(\"\\nColor-based episodic batch:\")\n",
    "for batch in episodic_color_dataloader:\n",
    "    images, _, color_labels_batch = batch\n",
    "    # We only need the color labels for the color network\n",
    "    color_labels_list = color_labels_batch.tolist()\n",
    "    label_counts = Counter(color_labels_list)\n",
    "    print(\"Batch images shape:\", images.shape)  # Expected: [batch_size, 3, 64, 64]\n",
    "    print(\"Batch color labels:\", color_labels_list)\n",
    "    print(\"Color label distribution in batch:\", label_counts)\n",
    "    break\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "min_range = min(shape_labels.shape[0], 10)\n",
    "for i in range(min_range):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = images[i].permute(1, 2, 0).numpy() # Convert tensor from (3, 64, 64) to (64, 64, 3) for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Color Label: {color_labels_list[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "<datasets.kandinsky.Kandinsky object at 0x7fd38821a4c0>\n"
     ]
    }
   ],
   "source": [
    "unsup_dataset = get_dataset(args)\n",
    "print(unsup_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kand says Namespace(GPU_ID='3', and_op='Prod', backbone='conceptizer', batch_size=32, beta=0.99, boia_model='ce', boia_ood_knowledge=False, c_sup=0.0, c_sup_ltn=0, checkin=None, checkout=False, classes_per_it=3, concept_extractor_path='ultralytics/finetuned/kand_best_100.pt', conf_host='pssr', conf_jobnum='3d10880f-3b76-4012-a6c6-7bdbcf91907a', conf_timestamp='2025-09-29 14:39:24.492627', count=30, dataset='kandinsky', debug=False, device=device(type='cuda'), embedding_dim=1024, entity='', entropy=False, exp_decay=0.9, extractor_training_epochs=20, gamma=0.001, hide_colors=[], hide_shapes=[], imp_op='Prod', iterations=100, joint=False, lr=0.001, model='prokandsl', n_epochs=40, n_support=75, non_verbose=False, notes=None, num_distinct_labels=3, num_query=5, num_samples=10, num_support=5, or_op='Prod', p=2, patience=5, posthoc=False, preprocess=False, proj_name='', project='Reasoning-Shortcuts', proto_epochs=10, proto_lr=0.001, prototypes=True, prototypical_batch_size=32, prototypical_loss_weight=[1.0], retrain_extractor=False, seed=128, seeds=[0, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768], splitted=False, task='patterns', tuning=False, use_ood=False, val_metric='accuracy', validate=False, w_c=1, w_h=1, w_rec=1, w_sl=10.0, wandb=None, warmup_steps=0, weight_decay=0.0001, which_c=[-1], yolo_folder='ultralytics-3/') None\n",
      "[PROTO-INFO] Using Prototypical Networks as backbone\n",
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'bddoiadpldisj', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'shieldedmnist', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "Using Dataset:  <datasets.kandinsky.Kandinsky object at 0x7fd38821a4c0>\n",
      "Number of images:  3\n",
      "Using backbone:  (PrimitivesProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "), PrimitivesProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "Using Model:  ProKandSL(\n",
      "  (encoder): ModuleList(\n",
      "    (0): PrimitivesProtoNet(\n",
      "      (encoder): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PrimitivesProtoNet(\n",
      "      (encoder): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Using Loss:  KANDINSKY_SL()\n",
      "Working with taks:  patterns\n",
      "Shapes encoder:  PrimitivesProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Colors encoder:  PrimitivesProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# & Instatiate the main model\n",
    "n_images, c_split = unsup_dataset.get_split()\n",
    "encoder, decoder = unsup_dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)    \n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(\"Using Dataset: \", unsup_dataset)\n",
    "print(\"Number of images: \", n_images)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "print(\"Working with taks: \", args.task)\n",
    "\n",
    "# & Initialize the two distinct prototypical classifiers\n",
    "print(\"Shapes encoder: \", model.encoder[0].to(args.device))\n",
    "print(\"Colors encoder: \", model.encoder[1].to(args.device))\n",
    "\n",
    "# & Load the pretrained concept extractor\n",
    "yolo = YOLO(my_yolo_premodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder[0].missing_classes = args.hide_shapes\n",
    "model.encoder[0].num_hidden = len(args.hide_shapes)\n",
    "\n",
    "model.encoder[1].missing_classes = args.hide_colors\n",
    "model.encoder[1].num_hidden = len(args.hide_colors)\n",
    "\n",
    "assert model.encoder[0].missing_classes == args.hide_shapes, \"Shape encoder should have hidden classes\"\n",
    "assert model.encoder[1].missing_classes == args.hide_colors, \"Color encoder should have hidden classes\"\n",
    "\n",
    "if args.hide_shapes:\n",
    "    assert model.encoder[0].num_hidden > 0, \"Shape encoder should have hidden classes\"\n",
    "    model.encoder[0].unknown_prototypes = nn.Parameter(torch.randn(model.encoder[0].num_hidden, 1024, requires_grad=True))\n",
    "if args.hide_colors:\n",
    "    assert model.encoder[1].num_hidden > 0, \"Color encoder should have hidden classes\"\n",
    "    model.encoder[1].unknown_prototypes = nn.Parameter(torch.randn(model.encoder[1].num_hidden, 1024, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.state_dict()) > 0, \"Model state dict is empty. Please check the model initialization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder 0 number of parameters: 113088\n",
      "Encoder 1 number of parameters: 113088\n",
      "Total number of parameters: 226176\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "num_params = 0\n",
    "for i, enc in enumerate(model.encoder):\n",
    "    num_params = sum(p.numel() for p in enc.parameters())\n",
    "    total_params += num_params\n",
    "    print(f\"Encoder {i} number of parameters: {num_params}\")\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hidden_classes(images, labels, hide):\n",
    "    \"\"\"\n",
    "    Filters out images and labels corresponding to the classes in the hide list.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): A tensor of shape (batch_size, 3, 64, 64).\n",
    "        labels (torch.Tensor): A tensor of shape (batch_size) with labels 0, 1, or 2.\n",
    "        hide (list): A list of integers representing the classes to hide.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor, torch.Tensor: Filtered images and labels.\n",
    "    \"\"\"\n",
    "    if hide:\n",
    "        # Create a mask for labels not in the hide list\n",
    "        mask = ~torch.isin(labels, torch.tensor(hide, device=labels.device))\n",
    "        \n",
    "        # Apply the mask to filter images and labels\n",
    "        filtered_images = images[mask]\n",
    "        filtered_labels = labels[mask]\n",
    "        \n",
    "        return filtered_images, filtered_labels\n",
    "    else:\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_extractor, concept_extractor_training_path, concept_extractor_project_path, transform,                  \n",
    "        episodic_shape_dataloader, episodic_color_dataloader, unsup_dataset, \n",
    "        _loss, args, seed, save_folder, patience=3):\n",
    "    \n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    yolo_save_dir = None\n",
    "    \n",
    "    model.to(model.device)\n",
    "    \n",
    "    # Initialize optimizers and schedulers\n",
    "    shape_optimizer = torch.optim.Adam(model.encoder[0].parameters())\n",
    "    color_optimizer = torch.optim.Adam(model.encoder[1].parameters())\n",
    "    shape_lr_scheduler = torch.optim.lr_scheduler.StepLR(shape_optimizer, step_size=10, gamma=0.5)\n",
    "    color_lr_scheduler = torch.optim.lr_scheduler.StepLR(color_optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    unsup_train_loader, unsup_val_loader, unsup_test_loader = unsup_dataset.get_data_loaders()\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    for epoch in range(args.proto_epochs + 1):  # first epoch is for determining the baseline accuracy\n",
    "        print(f\"Epoch {epoch+1}/{args.proto_epochs + 1}\")\n",
    "\n",
    "        # ^ PHASE 1: Training the Concept Extractor\n",
    "        print('----------------------------------')\n",
    "        print('--- Concept Extractor Training ---')\n",
    "        if epoch == 0:\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "        else:\n",
    "            assert yolo_save_dir is not None\n",
    "            concept_extractor = YOLO(yolo_save_dir)\n",
    "            results = concept_extractor.train(data=concept_extractor_training_path, \n",
    "                        epochs=args.extractor_training_epochs, \n",
    "                        imgsz=64, \n",
    "                        project=concept_extractor_project_path)\n",
    "            yolo_save_dir = os.path.join(results.save_dir, \"weights\", \"last.pt\")\n",
    "\n",
    "        # ^ PHASE 2: Training the Prototypical Networks\n",
    "        print('----------------------------------')\n",
    "        print('--- Prototypical Networks Training ---')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss_shapes, epoch_train_acc_shapes, epoch_train_loss_colors, epoch_train_acc_colors = [], [], [], []\n",
    "        \n",
    "        # * Under the assumption that both dataloaders yield the same number of episodes per epoch.\n",
    "        pNet_loss = PrototypicalLoss(n_support=args.num_support)\n",
    "        for (shape_batch, color_batch) in tqdm(zip(episodic_shape_dataloader, episodic_color_dataloader), total=args.iterations):\n",
    "            \n",
    "            # ------------------\n",
    "            # & Process shape episode\n",
    "            # ------------------\n",
    "            shape_optimizer.zero_grad()\n",
    "            shape_images, shape_labels, _ = shape_batch\n",
    "            shape_images, shape_labels = filter_hidden_classes(shape_images, shape_labels, args.hide_shapes)\n",
    "            if args.hide_shapes:   assert not any(label in args.hide_shapes for label in shape_labels), \"shape_labels contains hidden classes\"\n",
    "            shape_images, shape_labels = shape_images.to(args.device), shape_labels.to(args.device)            \n",
    "\n",
    "            # Forward pass: compute embeddings for all images in the episode.\n",
    "            shape_embeddings = model.encoder[0](shape_images)\n",
    "\n",
    "            # Compute prototypical loss.\n",
    "            shape_loss, shape_acc = pNet_loss(input=shape_embeddings, target=shape_labels)\n",
    "            shape_loss.backward()\n",
    "            shape_optimizer.step()\n",
    "\n",
    "            epoch_train_loss_shapes.append(shape_loss.item())\n",
    "            epoch_train_acc_shapes.append(shape_acc.item())\n",
    "\n",
    "            # ------------------\n",
    "            # & Process color episode\n",
    "            # ------------------\n",
    "            color_optimizer.zero_grad()\n",
    "            color_images, _, color_labels = color_batch\n",
    "            color_images, color_labels = filter_hidden_classes(color_images, color_labels, args.hide_colors)\n",
    "            if args.hide_colors:   assert not any(label in args.hide_colors for label in color_labels), \"color_labels contains hidden classes\"\n",
    "            color_images, color_labels = color_images.to(args.device), color_labels.to(args.device)\n",
    "            \n",
    "            # Forward pass: compute embeddings for all images in the episode.\n",
    "            color_embeddings = model.encoder[1](color_images)\n",
    "\n",
    "            # Compute prototypical loss.\n",
    "            color_loss, color_acc = pNet_loss(input=color_embeddings, target=color_labels)\n",
    "            color_loss.backward()\n",
    "            color_optimizer.step()\n",
    "\n",
    "            epoch_train_loss_colors.append(color_loss.item())\n",
    "            epoch_train_acc_colors.append(color_acc.item())\n",
    "\n",
    "        avg_loss_shapes = np.mean(epoch_train_loss_shapes)\n",
    "        avg_acc_shapes = np.mean(epoch_train_acc_shapes)\n",
    "        avg_loss_colors = np.mean(epoch_train_loss_colors)\n",
    "        avg_acc_colors = np.mean(epoch_train_acc_colors)\n",
    "        \n",
    "        print(f\"Shapes  - Avg Loss: {avg_loss_shapes:.4f} | Avg Acc: {avg_acc_shapes:.4f}\")\n",
    "        print(f\"Colors  - Avg Loss: {avg_loss_colors:.4f} | Avg Acc: {avg_acc_colors:.4f}\")\n",
    "\n",
    "        # ^ PHASE 3: Training the model with Unsupervised Data\n",
    "        print('----------------------------------')\n",
    "        print(\"--- Training with Unsupervised Data ---\")\n",
    "\n",
    "        # ys are the predictions of the model, y_true are the true labels, cs are the predictions of the concepts, cs_true are the true concepts\n",
    "        ys, y_true, cs, cs_true = None, None, None, None\n",
    "\n",
    "        unknown_init = True if (len(args.hide_shapes) > 0 or len(args.hide_colors)) else False\n",
    "        for i,data in enumerate(unsup_train_loader):\n",
    "            if random.random() > UNS_PERCENTAGE:\n",
    "                continue  # Skip this batch with probability (1 - percentage)\n",
    "\n",
    "            if epoch == 0:\n",
    "                model.eval()\n",
    "                if args.debug:  print(\"Find baseline accuracy, no training.\")\n",
    "                assert not model.training, \"Model should **NOT** be in training mode!\"\n",
    "                assert not model.encoder[0].training, \"Shape encoder should **NOT** be in training mode!\"\n",
    "                assert not model.encoder[1].training, \"Color encoder should **NOT** be in training mode!\"\n",
    "            else:    \n",
    "                shape_optimizer.zero_grad()\n",
    "                color_optimizer.zero_grad()\n",
    "                if args.debug:  print(\"Reset the optimizers.\")\n",
    "                assert model.training, \"Model should be in training mode!\"\n",
    "                assert model.encoder[0].training, \"Shape encoder should be in training mode!\"\n",
    "                assert model.encoder[1].training, \"Color encoder should be in training mode!\"\n",
    "\n",
    "            # load batch\n",
    "            images, labels, concepts = data\n",
    "            images, labels, concepts = (\n",
    "                images.to(model.device),\n",
    "                labels.to(model.device),\n",
    "                concepts.to(model.device),\n",
    "            )\n",
    "            batch_size = images.shape[0]\n",
    "            assert images.shape == (batch_size, 3, 64, 192), f\"Expected shape (B, 3, 64, 192), but got {images.shape}\"\n",
    "            assert labels.shape == (batch_size, 4), f\"Expected shape (B, 4), but got {labels.shape}\"\n",
    "            assert concepts.shape == (batch_size, 3, 6), f\"Expected shape (B, 3, 6), but got {concepts.shape}\"\n",
    "            \n",
    "            # Get a random support set.\n",
    "            if NO_AUGMENTATIONS:\n",
    "                this_support_images = kand_proto_dataset.images\n",
    "                this_support_labels = kand_proto_dataset.labels\n",
    "            else:\n",
    "                # Get a random support set.\n",
    "                this_support_images, this_support_labels = get_random_classes(\n",
    "                    kand_proto_dataset.images, kand_proto_dataset.labels, args.n_support, args.num_distinct_labels)\n",
    "                assert this_support_images.shape == (args.n_support * args.num_distinct_labels, 3, 64, 64), \\\n",
    "                    f\"Support images shape is not ({args.n_support * args.num_distinct_labels}, 3, 64, 64), but {this_support_images.shape}\"\n",
    "                assert this_support_labels.shape == (args.n_support * args.num_distinct_labels, 2), \\\n",
    "                    f\"Support labels shape is not ({args.n_support * args.num_distinct_labels}, 2), but {this_support_labels.shape}\"\n",
    "                \n",
    "            if args.debug:\n",
    "                plot_primitives(this_support_images, this_support_labels)\n",
    "\n",
    "            out_dict = model(images, concept_extractor, transform, this_support_images, this_support_labels, args, unknown_init=unknown_init)\n",
    "            out_dict.update({\"LABELS\": labels, \"CONCEPTS\": concepts})\n",
    "            unknown_init = False\n",
    "\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "            loss.backward()\n",
    "            \n",
    "            if epoch != 0:\n",
    "                if args.debug:  print(\"Update the schedulers.\")\n",
    "                shape_optimizer.step()\n",
    "                color_optimizer.step()  \n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(unsup_train_loader) - 9, epoch, loss.item())\n",
    "\n",
    "        # Step the scheduler (if using)\n",
    "        if epoch != 0:\n",
    "            shape_lr_scheduler.step()\n",
    "            color_lr_scheduler.step()\n",
    "\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(\"End of epoch \", epoch)\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print()\n",
    "\n",
    "        # ^ PHASE 4: Evaluation\n",
    "        print('----------------------------------')\n",
    "        print('--- Evaluation ---')\n",
    "\n",
    "        if ys is None:\n",
    "            # Skip evaluation if no unsupervised data was used for the model  \n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            concept_save_path = os.path.join(os.path.dirname(save_folder), f\"best_{seed}.pt\")\n",
    "            concept_extractor.save(concept_save_path)\n",
    "            print(f\"Saved model after prototypical netwrork training.\")\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "        if \"patterns\" in args.task:\n",
    "            y_true = y_true[:, -1]  # it is the last one\n",
    "\n",
    "        # Get a random support set.\n",
    "        if NO_AUGMENTATIONS:\n",
    "            this_support_images = kand_proto_dataset.images\n",
    "            this_support_labels = kand_proto_dataset.labels\n",
    "        else:\n",
    "            # Get a random support set.\n",
    "            this_support_images, this_support_labels = get_random_classes(\n",
    "                kand_proto_dataset.images, kand_proto_dataset.labels, args.n_support, args.num_distinct_labels)\n",
    "            assert this_support_images.shape == (args.n_support * args.num_distinct_labels, 3, 64, 64), \\\n",
    "                f\"Support images shape is not ({args.n_support * args.num_distinct_labels}, 3, 64, 64), but {this_support_images.shape}\"\n",
    "            assert this_support_labels.shape == (args.n_support * args.num_distinct_labels, 2), \\\n",
    "                f\"Support labels shape is not ({args.n_support * args.num_distinct_labels}, 2), but {this_support_labels.shape}\"\n",
    "            \n",
    "        model.eval()\n",
    "        tloss, cacc, yacc, f1 = evaluate_metrics(model, unsup_val_loader, args, \n",
    "                                support_images=this_support_images, support_labels=this_support_labels,\n",
    "                                concept_extractor=concept_extractor, transform=transform)\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1)\n",
    "        print()\n",
    "        \n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            if best_cacc == 0.0 and args.debug:     print(\"Baseline accuracy has been determined.\")\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "                \n",
    "            # Save the best model and the concept extractor\n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            concept_save_path = os.path.join(os.path.dirname(save_folder), f\"best_{seed}.pt\")\n",
    "            concept_extractor.save(concept_save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "            print()\n",
    "        \n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training model with seed 128\n",
      "Chosen device: cuda\n",
      "Save path for this model:  ../outputs/kand/my_models/sl/[F]-episodic-proto-net-pipeline-1.0-HIDE-SHAPES-[]-HIDE-COLORS-[]\n",
      "Saving in folder:  ../outputs/kand/my_models/sl/[F]-episodic-proto-net-pipeline-1.0-HIDE-SHAPES-[]-HIDE-COLORS-[]/sl_128.pth\n",
      "Loaded datasets in 4.4737207889556885 s.\n",
      "Len loaders: \n",
      " train: 4000 \n",
      " val: 1000\n",
      " len test: 1000\n",
      "\n",
      "--- Start of Training ---\n",
      "\n",
      "Epoch 1/11\n",
      "----------------------------------\n",
      "--- Concept Extractor Training ---\n",
      "New https://pypi.org/project/ultralytics/8.3.203 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks/../data/kand_config_yolo.yaml, degrees=0.0, deterministic=True, device=3, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=ultralytics-3/pretrained/yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train14, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=ultralytics-3/, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=ultralytics-3/train14, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 47.120.2 MB/s, size: 0.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/train/labels.cache... 206 images, 0 backgrounds, 0 corrupt: 100%|| 206/206 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 14.25.2 MB/s, size: 0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/val/labels.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to ultralytics-3/train14/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1multralytics-3/train14\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20     0.109G      1.811      3.509      1.158         92         64: 100%|| 13/13 [00:04<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36       0.02      0.111     0.0118    0.00837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20     0.109G      1.336      3.228      1.023         86         64: 100%|| 13/13 [00:02<00:00,  5.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36     0.0357      0.222     0.0407     0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20     0.109G      1.385      2.868      0.986         73         64: 100%|| 13/13 [00:02<00:00,  5.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.114          1      0.141     0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20     0.109G      1.365      2.326     0.9567         71         64: 100%|| 13/13 [00:02<00:00,  6.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36     0.0572          1      0.361      0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20     0.109G      1.329      1.794     0.9455         63         64: 100%|| 13/13 [00:02<00:00,  5.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.804      0.228      0.617      0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20     0.109G      1.287      1.399     0.9322         74         64: 100%|| 13/13 [00:02<00:00,  5.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.956      0.601      0.961      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20     0.109G      1.289      1.137     0.9221         80         64: 100%|| 13/13 [00:02<00:00,  5.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.899      0.987       0.95      0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20     0.109G      1.237     0.9553     0.9226         76         64: 100%|| 13/13 [00:02<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36          1      0.994      0.995      0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20     0.109G      1.234     0.8487     0.9128         70         64: 100%|| 13/13 [00:02<00:00,  6.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36       0.97          1      0.985      0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20     0.109G      1.124     0.7878     0.9081         77         64: 100%|| 13/13 [00:02<00:00,  6.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.894          1      0.934      0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20     0.109G     0.8714     0.6818     0.8772         42         64: 100%|| 13/13 [00:03<00:00,  3.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20     0.109G     0.7456     0.6099     0.8804         42         64: 100%|| 13/13 [00:02<00:00,  6.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.999          1      0.995      0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20     0.109G     0.6925      0.578     0.8797         42         64: 100%|| 13/13 [00:02<00:00,  5.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.999          1      0.995       0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20     0.109G     0.6902     0.5765     0.8803         42         64: 100%|| 13/13 [00:02<00:00,  6.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20     0.109G     0.7179     0.5753     0.8663         42         64: 100%|| 13/13 [00:02<00:00,  6.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20     0.109G     0.5831     0.5563     0.8729         42         64: 100%|| 13/13 [00:02<00:00,  6.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20     0.109G     0.5919     0.5007     0.8577         42         64: 100%|| 13/13 [00:02<00:00,  6.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20     0.109G     0.5457     0.5091      0.867         42         64: 100%|| 13/13 [00:02<00:00,  6.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20     0.109G     0.5049     0.5086     0.8729         42         64: 100%|| 13/13 [00:01<00:00,  7.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20     0.109G     0.5169     0.5044     0.8681         42         64: 100%|| 13/13 [00:02<00:00,  6.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from ultralytics-3/train14/weights/last.pt, 5.4MB\n",
      "Optimizer stripped from ultralytics-3/train14/weights/best.pt, 5.4MB\n",
      "\n",
      "Validating ultralytics-3/train14/weights/best.pt...\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:3 (NVIDIA TITAN Xp, 12190MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         36      0.998          1      0.995      0.821\n",
      "Speed: 0.0ms preprocess, 1.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1multralytics-3/train14\u001b[0m\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:03<00:00, 28.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes  - Avg Loss: 3.9477 | Avg Acc: 0.8867\n",
      "Colors  - Avg Loss: 1.3423 | Avg Acc: 0.9893\n",
      "----------------------------------\n",
      "--- Training with Unsupervised Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-29 | 14:44 ] epoch 0: || loss: 40.73071289"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "End of epoch  0\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "----------------------------------\n",
      "--- Evaluation ---\n",
      "  ACC C 90.89626736111114   ACC Y 77.05078125 F1 Y 75.47387327660314\n",
      "\n",
      "Saving...\n",
      "Saved best model with CACC score: 90.89626736111114\n",
      "\n",
      "Epoch 2/11\n",
      "----------------------------------\n",
      "--- Concept Extractor Training ---\n",
      "New https://pypi.org/project/ultralytics/8.3.203 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130  Python-3.8.20 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/notebooks/../data/kand_config_yolo.yaml, degrees=0.0, deterministic=True, device=3, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=ultralytics-3/train14/weights/last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train15, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=ultralytics-3/, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=ultralytics-3/train15, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 46.720.5 MB/s, size: 0.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/train/labels.cache... 206 images, 0 backgrounds, 0 corrupt: 100%|| 206/206 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.95.1 MB/s, size: 0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/kandinsky/data/kand_yolo_dataset/val/labels.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to ultralytics-3/train15/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1multralytics-3/train15\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      7.46G     0.9216     0.6405     0.9117         78         64:  46%|     | 6/13 [00:03<00:03,  1.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m save_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving in folder: \u001b[39m\u001b[38;5;124m\"\u001b[39m, save_folder)\n\u001b[0;32m----> 7\u001b[0m best_cacc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# main model\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# yolo model\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# yolo training data path\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_extractor_project_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_yolo_project_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# yolo project path\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# resizer \u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodic_shape_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodic_shape_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# episodic dataloader for shapes\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodic_color_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodic_color_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# episodic dataloader for colors\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43munsup_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsup_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# original weakly supervised dataset\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m save_model(model, args, args\u001b[38;5;241m.\u001b[39mseed)  \u001b[38;5;66;03m# save the model parameters\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest F1 score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_cacc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [23], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, concept_extractor, concept_extractor_training_path, concept_extractor_project_path, transform, episodic_shape_dataloader, episodic_color_dataloader, unsup_dataset, _loss, args, seed, save_folder, patience)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m yolo_save_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     concept_extractor \u001b[38;5;241m=\u001b[39m YOLO(yolo_save_dir)\n\u001b[0;32m---> 42\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mconcept_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcept_extractor_training_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractor_training_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcept_extractor_project_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     yolo_save_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ^ PHASE 2: Training the Prototypical Networks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/model.py:793\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/trainer.py:212\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/ultralytics/engine/trainer.py:395\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    391\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/r4rr/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "print(\"Save path for this model: \", save_path)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving in folder: \", save_folder)\n",
    "best_cacc = train(model=model,                           # main model\n",
    "    concept_extractor=yolo,                              # yolo model\n",
    "    concept_extractor_training_path=yaml_path,           # yolo training data path\n",
    "    concept_extractor_project_path=my_yolo_project_path, # yolo project path\n",
    "    transform=T.Resize((64, 64)),                        # resizer \n",
    "    episodic_shape_dataloader=episodic_shape_dataloader, # episodic dataloader for shapes\n",
    "    episodic_color_dataloader=episodic_color_dataloader, # episodic dataloader for colors\n",
    "    unsup_dataset=unsup_dataset,                         # original weakly supervised dataset\n",
    "    _loss=loss, \n",
    "    args=args,\n",
    "    seed=args.seed,\n",
    "    save_folder=save_folder\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "\n",
    "print(f\"Best F1 score: {best_cacc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
