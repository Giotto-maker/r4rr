{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAPERMILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's seed\n",
    "seed = 0\n",
    "\n",
    "# additional paramters\n",
    "model_parameter_name = 'sl'\n",
    "uns_parameter_percentage = 1.0\n",
    "sup_loss_weight = 1.0\n",
    "GPU_ID = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert seed is not None, \"seed should not be None\"\n",
    "assert isinstance(seed, int), \"seed should be an integer\"\n",
    "assert isinstance(uns_parameter_percentage, float), \"uns_parameter_percentage should be a float\"\n",
    "assert 0.0 <= uns_parameter_percentage <= 1.0, \"uns_parameter_percentage should be in the range [0.0, 1.0]\"\n",
    "assert model_parameter_name is not None, \"model_parameter_name should not be None\"\n",
    "assert sup_loss_weight is not None, \"sup_loss_weight should not be None\"\n",
    "assert isinstance(sup_loss_weight, float), \"sup_loss_weight should be a float\"\n",
    "assert GPU_ID is not None, \"GPU_ID should not be None\"\n",
    "\n",
    "print(\"Papermill seed parameter is: \" + str(seed))\n",
    "print(\"Papermill model name is: \" + model_parameter_name)\n",
    "print(\"Papermill uns_parameter_percentage is: \" + str(uns_parameter_percentage))\n",
    "print(\"Papermill GPU_ID is: \" + GPU_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_mnist_add_utils' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import datetime\n",
    "import importlib\n",
    "import setproctitle, socket, uuid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "from numpy import float32, zeros\n",
    "from datasets import get_dataset\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from datasets.utils.base_dataset import BaseDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils import fprint\n",
    "from utils.train import train\n",
    "from utils.test import test\n",
    "from utils.preprocess_resnet import preprocess\n",
    "from utils.conf import *\n",
    "from utils.args import *\n",
    "from utils.status import progress_bar\n",
    "from utils.checkpoint import save_model, create_load_ckpt\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.metrics import (\n",
    "    evaluate_metrics,\n",
    "    evaluate_mix,\n",
    "    mean_entropy,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from protonet_kand_modules.arguments import args_dpl, args_sl, args_ltn\n",
    "from protonet_kand_modules.utility_modules.check_gpu import my_gpu_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_parameter_name == 'dpl':   args = args_dpl\n",
    "elif model_parameter_name == 'sl':  args = args_sl\n",
    "else:                               args = args_ltn\n",
    "\n",
    "# saving\n",
    "save_folder = \"kandinsky\" \n",
    "save_model_name = model_parameter_name\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\", \"NEW-outputs\", \n",
    "    save_folder, \n",
    "    \"baseline\", \n",
    "    save_model_name,\n",
    "    f\"DEBUG-supervisions-via-augmentations-{uns_parameter_percentage}\",\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "print(f\"Save paths: {str(save_paths)}\")\n",
    "\n",
    "if args.model in ['prokandsl', 'prokandltn', 'prokanddpl'] or args.prototypes:\n",
    "    raise ValueError(\"This experiment is NOT meant for pNet based models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = model_parameter_name\n",
    "UNS_PERCENTAGE = uns_parameter_percentage\n",
    "CONCEPT_LOSS_WEIGHT = sup_loss_weight\n",
    "if CONCEPT_LOSS_WEIGHT > 1.0 and MODEL != 'sl':\n",
    "    raise Exception(\"Concept loss weight should be less than or equal to 1.0 for DPL and LTN\")\n",
    "elif MODEL != 'sl':\n",
    "    assert CONCEPT_LOSS_WEIGHT == 1.0, 'Loss weight greater than 1 is only for SL'\n",
    "\n",
    "print(\"Model: \", MODEL)\n",
    "print(\"Unsupervised Percentage: \", UNS_PERCENTAGE)\n",
    "print(\"Concept Loss Weight: \", CONCEPT_LOSS_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add uuid, timestamp and hostname for logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_images = torch.load('data/kand_annotations/yolo_annotations/images.pt')\n",
    "proto_labels = torch.load('data/kand_annotations/yolo_annotations/labels.pt')\n",
    "print(\"Prototypical data loaded\")\n",
    "print(\"Images: \", proto_images.shape)\n",
    "print(\"Labels: \", proto_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): Tensor of shape [N, 3, 64, 64]\n",
    "            labels (Tensor): Tensor of shape [N, 6] where:\n",
    "                             - labels[:, :3] are the shape labels  (0: square, 1: circle, 2: triangle)\n",
    "                             - labels[:, 3:] are the colour labels (0: red, 1: yellow, 2: blue)\n",
    "            transform: Optional transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels  # shape [N, 6]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index].long()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PrimitvesDataset instance\n",
    "kand_sup_dataset = SupervisedDataset(proto_images, proto_labels, transform=None)\n",
    "sup_train_loader = DataLoader(kand_sup_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "# Plot the first 30 images in kand_proto_dataset\n",
    "fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "axes = axes.flatten()\n",
    "for i in range(30):\n",
    "    image, labels = kand_sup_dataset[i]\n",
    "    axes[i].imshow(image.permute(1, 2, 0))  # Convert from CHW to HWC for plotting\n",
    "    axes[i].set_title(f\"{labels.numpy()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(args)\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders()\n",
    "dataset.print_stats()    \n",
    "n_images, c_split = dataset.get_split()\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "loss = model.get_loss(args)\n",
    "model.start_optim(args)\n",
    "\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Number of images: \", n_images)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "print(\"Working with taks: \", args.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:MnistDPL,\n",
    "        sup_train_loader:DataLoader,\n",
    "        unsup_train_loader:DataLoader,\n",
    "        unsup_val_loader:DataLoader,\n",
    "        _loss: ADDMNIST_DPL, \n",
    "        args,\n",
    "        seed,\n",
    "        save_folder,\n",
    "        sup_loss_weight=1.0,\n",
    "        patience=5,\n",
    "        debug=False):\n",
    "    \n",
    "     # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0   # for early stopping\n",
    "\n",
    "    model.to(model.device)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:   w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "\n",
    "    # default for warm-up\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # & FOR EACH EPOCH\n",
    "    for epoch in range(args.proto_epochs):  # ^ ensure consistency with the number of epochs used for prototypical networks\n",
    "        model.train()\n",
    "\n",
    "        ###############################\n",
    "        # 1. Supervised phase: Teach the model to recognize primitives (merged in triplets)\n",
    "        ###############################\n",
    "        print(\"Start of supervised episodic training.\")\n",
    "        for i, (images, labels) in enumerate(sup_train_loader):\n",
    "            sup_images = images.to(model.device)  # shape: (batch_size, 3, 64, 64)\n",
    "            sup_labels = labels.to(model.device)  # shape: (batch_size, 6)\n",
    "            batch_size = sup_images.size(0)\n",
    "\n",
    "            assert sup_images.dim() == 4 and sup_images.size(1) == 3 \\\n",
    "                and sup_images.size(2) == 64 and sup_images.size(3) == 64, \\\n",
    "                f\"Expected sup_images [B,3,64,64], got {sup_images.shape}\"\n",
    "            assert sup_labels.shape == torch.Size([batch_size, 6]), \\\n",
    "                f\"Expected sup_labels [{batch_size},6], got {sup_labels.shape}\"\n",
    "\n",
    "            # make batch_size divisible by 3 by dropping the extra 1 or 2 samples\n",
    "            if batch_size % 3 != 0:\n",
    "                drop = batch_size % 3\n",
    "                sup_images = sup_images[:-drop]\n",
    "                sup_labels = sup_labels[:-drop]\n",
    "                batch_size -= drop\n",
    "\n",
    "            # now form triplets: 0 with 1 with 2, 3 with 4 with 5, ...\n",
    "            merged_images = torch.cat([\n",
    "                sup_images[0::3],   # first in each triplet\n",
    "                sup_images[1::3],   # second\n",
    "                sup_images[2::3]    # third\n",
    "            ], dim=3)  # concat along width â†’ new width = 64*3 = 192\n",
    "\n",
    "            expected_bs = batch_size // 3\n",
    "            assert merged_images.shape == torch.Size([expected_bs, 3, 64, 192]), \\\n",
    "                f\"Expected merged_images [{expected_bs},3,64,192], got {merged_images.shape}\"\n",
    "\n",
    "            # extract labels for each of the three primitives in the triplet\n",
    "            labels_first  = sup_labels[0::3]  # [bs//3, 6]\n",
    "            labels_second = sup_labels[1::3]  # [bs//3, 6]\n",
    "            labels_third  = sup_labels[2::3]  # [bs//3, 6]\n",
    "\n",
    "            if args.debug:\n",
    "                img = merged_images[0].cpu().permute(1, 2, 0).numpy()\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Labels: {labels_first[0].tolist()} | \"\n",
    "                        f\"{labels_second[0].tolist()} | \"\n",
    "                        f\"{labels_third[0].tolist()}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "            # Forward pass: now feeding the concatenated triplets\n",
    "            out_dict = model(merged_images)\n",
    "            logits = out_dict[\"CS\"]\n",
    "            \n",
    "            B = logits.size(0)\n",
    "            num_objects = 6\n",
    "            num_classes = 3\n",
    "\n",
    "            def triplet_loss(logits_slice, labels_slice):\n",
    "                l = logits_slice.reshape(B, num_objects, num_classes)\n",
    "                l_flat = l.reshape(B * num_objects, num_classes)\n",
    "                t_flat = labels_slice.reshape(B * num_objects)\n",
    "                return F.cross_entropy(l_flat, t_flat)\n",
    "\n",
    "            logits1 = logits[:, 0, :]  # [B, 18]\n",
    "            logits2 = logits[:, 1, :]\n",
    "            logits3 = logits[:, 2, :]\n",
    "\n",
    "            # compute individual losses\n",
    "            loss1 = triplet_loss(logits1, labels_first)\n",
    "            loss2 = triplet_loss(logits2, labels_second)\n",
    "            loss3 = triplet_loss(logits3, labels_third)\n",
    "\n",
    "            # total concept loss (you can average instead of sum if you prefer)\n",
    "            concept_loss = sup_loss_weight * (loss1 + loss2 + loss3)\n",
    "\n",
    "            # backprop\n",
    "            model.opt.zero_grad()\n",
    "            concept_loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Supervised phase, Epoch {epoch}, Batch {i}: \"\n",
    "                    f\"Concept Loss = {concept_loss.item():.4f}\")\n",
    "        \n",
    "        ###############################\n",
    "        # 2. Original unsupervised training phase (sum prediction)\n",
    "        ###############################\n",
    "        # ys are the predictions of the model, y_true are the true labels, cs are the predictions of the concepts, cs_true are the true concepts\n",
    "        ys, y_true, cs, cs_true = None, None, None, None\n",
    "        \n",
    "        # & FOR EACH BATCH\n",
    "        print(\"Start of unsupervised training.\")\n",
    "        for i, data in enumerate(unsup_train_loader):\n",
    "            if random.random() > UNS_PERCENTAGE:\n",
    "                continue  # Skip this batch with probability (1 - percentage)\n",
    "\n",
    "            images, labels, concepts = data\n",
    "            images, labels, concepts = (\n",
    "                images.to(model.device),\n",
    "                labels.to(model.device),\n",
    "                concepts.to(model.device),\n",
    "            )\n",
    "\n",
    "            # ^ baseline model\n",
    "            out_dict = model(images)\n",
    "            out_dict.update({\"LABELS\": labels, \"CONCEPTS\": concepts})\n",
    "            \n",
    "            model.opt.zero_grad()\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(unsup_train_loader) - 9, epoch, loss.item())\n",
    "\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(\"End of epoch \", epoch)\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print()\n",
    "\n",
    "        if UNS_PERCENTAGE == 0.0:\n",
    "            print(\"Saving...\")\n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            print(f\"Saved best model with F1 score: {best_cacc}\")\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        if \"patterns\" in args.task:\n",
    "            y_true = y_true[:, -1]  # it is the last one\n",
    "\n",
    "        model.eval()\n",
    "        tloss, cacc, yacc, f1 = evaluate_metrics(model, unsup_val_loader, args)\n",
    "\n",
    "        # update the (warmup) scheduler at end of the epoch\n",
    "        if epoch < args.warmup_steps:\n",
    "            w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1)\n",
    "        print()\n",
    "\n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "                \n",
    "            # Save the best model and the concept extractor\n",
    "            torch.save(model.state_dict(), save_folder)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "            print()\n",
    "        \n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = dict()\n",
    "print(f\"*** Training model with seed {seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "best_f1 = train(model=model,\n",
    "        sup_train_loader=sup_train_loader,\n",
    "        unsup_train_loader=unsup_train_loader,\n",
    "        unsup_val_loader=unsup_val_loader,\n",
    "        _loss=loss, \n",
    "        args=args,\n",
    "        seed=seed,\n",
    "        save_folder=save_folder,\n",
    "        sup_loss_weight=CONCEPT_LOSS_WEIGHT,\n",
    "        debug=False\n",
    "    )\n",
    "f1_scores[(seed)] = best_f1\n",
    "save_model(model, args, seed)  # save the model parameters\n",
    "\n",
    "print(f\"*** Finished training model with seed {seed}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "best_weight_seed = max(f1_scores, key=f1_scores.get)\n",
    "print(f\"Best weight and seed combination: {best_weight_seed} with F1 score: {f1_scores[best_weight_seed]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
