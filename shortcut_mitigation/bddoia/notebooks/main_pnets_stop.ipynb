{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db624e1e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import setproctitle, socket, uuid\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from datasets import get_dataset\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "from utils import fprint\n",
    "from utils.status import progress_bar\n",
    "from utils.metrics import evaluate_metrics, accuracy_binary\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.checkpoint import save_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from datasets.utils.base_dataset import BaseDataset\n",
    "\n",
    "from backbones.bddoia_protonet import ProtoNetConv1D, PrototypicalLoss\n",
    "\n",
    "from protonet_STOP_bddoia_modules.arguments import args_dpl \n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_helpers import (\n",
    "    assert_inputs,\n",
    "    get_random_classes,\n",
    "    compute_class_logits_per_batch\n",
    ")\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_functions import (\n",
    "    build_stop_filtered_concat_inputs,\n",
    "    get_prototypical_datasets_inputs,\n",
    "    train_my_prototypical_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b846",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3398\n",
    "UNS_PERCENTAGE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0680d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args_dpl\n",
    "args.seed = SEED\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"bddoia\" \n",
    "save_model_name = 'dpl'\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"NEW-outputs\", \n",
    "    save_folder, \n",
    "    \"my_models\", \n",
    "    save_model_name,\n",
    "    f\"episodic-proto-net-pipeline-{UNS_PERCENTAGE}-article-STOP\"\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "print(f\"Save paths: {str(save_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511d01",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c1be3",
   "metadata": {},
   "source": [
    "## Test Set Evaluation (alternative to full fledged notebook eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * helper function for 'plot_multilabel_confusion_matrix'\n",
    "def convert_to_categories(elements):\n",
    "    # Convert vector of 0s and 1s to a single binary representation along the first dimension\n",
    "    binary_rep = np.apply_along_axis(\n",
    "        lambda x: \"\".join(map(str, x)), axis=1, arr=elements\n",
    "    )\n",
    "    return np.array([int(x, 2) for x in binary_rep])\n",
    "\n",
    "\n",
    "# * BBDOIA custom confusion matrix for concepts\n",
    "def plot_multilabel_confusion_matrix(\n",
    "    y_true, y_pred, class_names, title, save_path=None\n",
    "):\n",
    "    y_true_categories = convert_to_categories(y_true.astype(int))\n",
    "    y_pred_categories = convert_to_categories(y_pred.astype(int))\n",
    "\n",
    "    to_rtn_cm = confusion_matrix(y_true_categories, y_pred_categories)\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    num_classes = len(class_names)\n",
    "    num_rows = (num_classes + 4) // 5  # Calculate the number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_rows))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.subplot(num_rows, 5, i + 1)  # Set the subplot position\n",
    "        plt.imshow(cm[i], interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Class: {class_names[i]}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "        plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "        fmt = \".0f\"\n",
    "        thresh = cm[i].max() / 2.0\n",
    "        for j in range(cm[i].shape[0]):\n",
    "            for k in range(cm[i].shape[1]):\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    format(cm[i][j, k], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i][j, k] > thresh else \"black\",\n",
    "                )\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_total.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return to_rtn_cm\n",
    "\n",
    "\n",
    "# * Concept collapse (Soft)\n",
    "def compute_coverage(confusion_matrix):\n",
    "    \"\"\"Compute the coverage of a confusion matrix.\n",
    "\n",
    "    Essentially this metric is\n",
    "    \"\"\"\n",
    "\n",
    "    max_values = np.max(confusion_matrix, axis=0)\n",
    "    clipped_values = np.clip(max_values, 0, 1)\n",
    "\n",
    "    # Redefinition of soft coverage\n",
    "    coverage = np.sum(clipped_values) / len(clipped_values)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# * BDDOIA custom confusion matrix for actions\n",
    "def plot_actions_confusion_matrix(c_true, c_pred, title, save_path=None):\n",
    "\n",
    "    # Define scenarios and corresponding labels\n",
    "    scenarios = {\n",
    "        \"forward\": [slice(0, 3), slice(0, 3)],\n",
    "        \"stop\": [slice(3, 9), slice(3, 9)],\n",
    "        #'forward_stop': [slice(None, 9), slice(None, 9)],\n",
    "        \"left\": [slice(9, 15), slice(9, 15)],\n",
    "        \"right\": [slice(15, 21), slice(15, 21)],\n",
    "    }\n",
    "\n",
    "    to_rtn = {}\n",
    "\n",
    "    # Plot confusion matrix for each scenario\n",
    "    for scenario, indices in scenarios.items():\n",
    "\n",
    "        g_true = convert_to_categories(c_true[:, indices[0]].astype(int))\n",
    "        c_pred_scenario = convert_to_categories(c_pred[:, indices[1]].astype(int))\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(g_true, c_pred_scenario)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{title} - {scenario}\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        n_classes = c_true[:, indices[0]].shape[1]\n",
    "\n",
    "        tick_marks = np.arange(2**n_classes)\n",
    "        plt.xticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "        plt.yticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or show plot\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{scenario}.png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        to_rtn.update({scenario: cm})\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return to_rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04330867",
   "metadata": {},
   "source": [
    "# DATASET & BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert embeddings.shape[0] == labels.shape[0]\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class PrototypicalBatchSampler(object):\n",
    "    \"\"\"\n",
    "    Yields a batch of indices for episodic training.\n",
    "    At each iteration, it randomly selects 'classes_per_it' classes and then picks\n",
    "    'num_samples' samples for each selected class.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (array-like): 1D array or list of labels for the target task.\n",
    "                                 This should be either the shape labels or the colour labels.\n",
    "            classes_per_it (int): Number of random classes for each iteration.\n",
    "            num_samples (int): Number of samples per class (support + query) in each episode.\n",
    "            iterations (int): Number of iterations (episodes) per epoch.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # Create an index matrix of shape (num_classes, max_samples_in_class)\n",
    "        max_count = max(self.counts)\n",
    "        self.indexes = np.empty((len(self.classes), max_count), dtype=int)\n",
    "        self.indexes.fill(-1)\n",
    "        self.indexes = torch.LongTensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros(len(self.classes), dtype=torch.long)\n",
    "\n",
    "        # Fill in the matrix with indices for each class.\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            # Find the row corresponding to this label\n",
    "            class_idx = (self.classes == label).nonzero(as_tuple=False).item()\n",
    "            # Find the next available column (where the value is -1)\n",
    "            pos = (self.indexes[class_idx] == -1).nonzero(as_tuple=False)[0].item()\n",
    "            self.indexes[class_idx, pos] = idx\n",
    "            self.numel_per_class[class_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a batch of indices for each episode.\n",
    "        \"\"\"\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            batch = torch.LongTensor(cpi * spc)\n",
    "            # Randomly choose 'classes_per_it' classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "            for i, class_idx in enumerate(c_idxs):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "\n",
    "                n_avail = self.numel_per_class[class_idx]\n",
    "                if spc <= n_avail:\n",
    "                    # enough examples → sample without replacement\n",
    "                    perm = torch.randperm(n_avail)\n",
    "                    sample_idxs = perm[:spc]\n",
    "                else:\n",
    "                    # too few → sample with replacement\n",
    "                    sample_idxs = torch.randint(0, n_avail, (spc,), dtype=torch.long)\n",
    "\n",
    "                batch[s] = self.indexes[class_idx, sample_idxs]\n",
    "\n",
    "            # Shuffle the batch indices\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e79c9",
   "metadata": {},
   "source": [
    "# UNSUPERVISED DATA AND MODEL LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "# & Main model\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "\n",
    "# & Prototypical Networks\n",
    "traffic_lights_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 3\n",
    "traffic_signs_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 4\n",
    "car_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 5\n",
    "pedestrians_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 6\n",
    "rider_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 7\n",
    "others_model = ProtoNetConv1D(in_dim=3072).to(model.device) # 8\n",
    "\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "print(\"Using Traffic Lights Model: \", traffic_lights_model)\n",
    "print(\"Using Traffic Signs Model: \", traffic_signs_model)\n",
    "print(\"Using Car Model: \", car_model)\n",
    "print(\"Using Pedestrians Model: \", pedestrians_model)\n",
    "print(\"Using Rider Model: \", rider_model)\n",
    "print(\"Using Others Model: \", others_model)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override the default optimizer to include all the expert models\n",
    "model.opt = torch.optim.Adam(\n",
    "    list(model.parameters()) + # the main model\n",
    "    list(traffic_lights_model.parameters()) + # 3\n",
    "    list(traffic_signs_model.parameters()) + # 4\n",
    "    list(car_model.parameters()) + # 5\n",
    "    list(pedestrians_model.parameters()) + # 6\n",
    "    list(rider_model.parameters()) + # 7\n",
    "    list(others_model.parameters()), # 8\n",
    "    args.lr, weight_decay=args.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b138d",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b4d4",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e14769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: MnistDPL, \n",
    "        traffic_lights_model: ProtoNetConv1D,# 3\n",
    "        traffic_signs_model: ProtoNetConv1D, # 4\n",
    "        car_model: ProtoNetConv1D, # 5 \n",
    "        pedestrians_model: ProtoNetConv1D, # 6\n",
    "        rider_model: ProtoNetConv1D, # 7\n",
    "        others_model: ProtoNetConv1D,# 8\n",
    "        _loss: ADDMNIST_DPL,\n",
    "        save_path: str, \n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        args: Namespace,\n",
    "        seed: int = 0,\n",
    "        debug=False,\n",
    "    ) -> float:\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # early stopping\n",
    "    best_f1_stop_concepts = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # scheduler & warmup (not used) for main model\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:\n",
    "        w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    # Optimizers & Schedulers for PNets\n",
    "    traffic_lights_optimizer = torch.optim.Adam(traffic_lights_model.parameters()) # 3\n",
    "    traffic_lights_scheduler = torch.optim.lr_scheduler.StepLR(traffic_lights_optimizer, step_size=10, gamma=0.5)\n",
    "    traffic_signs_model_optimizer = torch.optim.Adam(traffic_signs_model.parameters()) # 4\n",
    "    traffic_signs_model_scheduler = torch.optim.lr_scheduler.StepLR(traffic_signs_model_optimizer, step_size=10, gamma=0.5)\n",
    "    car_model_optimizer = torch.optim.Adam(car_model.parameters()) # 5\n",
    "    car_model_scheduler = torch.optim.lr_scheduler.StepLR(car_model_optimizer, step_size=10, gamma=0.5)\n",
    "    pedestrians_model_optimizer = torch.optim.Adam(pedestrians_model.parameters()) # 6\n",
    "    pedestrians_model_scheduler = torch.optim.lr_scheduler.StepLR(pedestrians_model_optimizer, step_size=10, gamma=0.5)\n",
    "    rider_model_optimizer = torch.optim.Adam(rider_model.parameters()) # 7\n",
    "    rider_model_scheduler = torch.optim.lr_scheduler.StepLR(rider_model_optimizer, step_size=10, gamma=0.5)\n",
    "    others_model_optimizer = torch.optim.Adam(others_model.parameters()) # 8\n",
    "    others_model_scheduler = torch.optim.lr_scheduler.StepLR(others_model_optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.to(model.device)\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # used to fetch the first random batch of data to train the prototypical networks\n",
    "    compute_prototypical_batch = True\n",
    "        \n",
    "    # & Training start\n",
    "    for epoch in range(args.n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{args.n_epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        ys, y_true, cs, cs_true, batch = None, None, None, None, 0\n",
    "        pNet_loss = PrototypicalLoss(n_support=args.num_support)\n",
    "                \n",
    "        execute = True  # when to train the prototypical networks (once per epoch)\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # ------------------ original embeddings\n",
    "            images_embeddings = torch.stack(batch['embeddings']).to(model.device)\n",
    "            attr_labels = torch.stack(batch['attr_labels']).to(model.device)\n",
    "            class_labels = torch.stack(batch['class_labels'])[:,:-1].to(model.device) # exclude the last column\n",
    "            # ------------------ my extracted features\n",
    "            images_embeddings_raw = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "            detected_rois = batch['rois']\n",
    "            detected_rois_feats = batch['roi_feats']\n",
    "            detection_labels = batch['detection_labels']\n",
    "            detection_scores = batch['detection_scores']\n",
    "            assert_inputs(images_embeddings, attr_labels, class_labels,\n",
    "                   detected_rois_feats, detected_rois, detection_labels,\n",
    "                   detection_scores, images_embeddings_raw)\n",
    "            # ------------------ combined object + scene features\n",
    "            \n",
    "            # & Build Prototypical Inputs\n",
    "            inputs = build_stop_filtered_concat_inputs(\n",
    "                images_embeddings_raw, \n",
    "                detected_rois, \n",
    "                detected_rois_feats, \n",
    "                detection_labels, \n",
    "                detection_scores\n",
    "            )\n",
    "\n",
    "            # ? List of length batch size, each tensor of shape (num_rois, 3072).\n",
    "            concat_inputs_traffic_lights = inputs[0] # 3\n",
    "            concat_input_traffic_signs = inputs[1] # 4\n",
    "            concat_input_cars = inputs[2] # 5\n",
    "            concat_input_pedestrians = inputs[3] # 6\n",
    "            concat_input_riders = inputs[4] # 7\n",
    "            concat_input_others = inputs[5] # 8\n",
    "            \n",
    "            # --------------------------------------\n",
    "            # ^ PROTOTYPICAL NETWORK TRAINING\n",
    "            # --------------------------------------\n",
    "            if execute:\n",
    "\n",
    "                if compute_prototypical_batch:\n",
    "                    # & Build Prototypical Datasets\n",
    "                    # * [traffic lights]\n",
    "                    proto_data_traffic_lights, proto_labels_traffic_lights, sz_traffic_lights = get_prototypical_datasets_inputs(\n",
    "                            concat_inputs_traffic_lights, \n",
    "                            3, \n",
    "                            attr_labels, \n",
    "                            model.device\n",
    "                        )\n",
    "                    proto_dataset_traffic_lights = ProtoDataset(proto_data_traffic_lights, proto_labels_traffic_lights)\n",
    "                    if (proto_labels_traffic_lights == 1).sum().item() == 0:    continue\n",
    "\n",
    "                    # * [traffic signs]\n",
    "                    proto_data_traffic_signs, proto_labels_traffic_signs, sz_traffic_signs = get_prototypical_datasets_inputs(\n",
    "                            concat_input_traffic_signs, \n",
    "                            4, \n",
    "                            attr_labels,\n",
    "                            model.device\n",
    "                    )\n",
    "                    proto_dataset_traffic_signs = ProtoDataset(proto_data_traffic_signs, proto_labels_traffic_signs)\n",
    "                    if (proto_labels_traffic_signs == 1).sum().item() == 0:    continue\n",
    "\n",
    "                    # * [cars]\n",
    "                    proto_data_cars, proto_labels_cars, sz_cars = get_prototypical_datasets_inputs(\n",
    "                            concat_input_cars, \n",
    "                            5, \n",
    "                            attr_labels, \n",
    "                            model.device\n",
    "                    )\n",
    "                    proto_dataset_cars = ProtoDataset(proto_data_cars, proto_labels_cars)\n",
    "                    if (proto_labels_cars == 1).sum().item() == 0:    continue\n",
    "                    \n",
    "                    # * [pedestrians]\n",
    "                    proto_data_pedestrians, proto_labels_pedestrians, sz_pedestrians = get_prototypical_datasets_inputs(\n",
    "                            concat_input_pedestrians, \n",
    "                            6, \n",
    "                            attr_labels, \n",
    "                            model.device\n",
    "                    )\n",
    "                    proto_dataset_pedestrians = ProtoDataset(proto_data_pedestrians, proto_labels_pedestrians)\n",
    "                    if (proto_labels_pedestrians == 1).sum().item() == 0:    continue\n",
    "                    \n",
    "                    # * [riders]\n",
    "                    proto_data_riders, proto_labels_riders, sz_riders = get_prototypical_datasets_inputs(\n",
    "                            concat_input_riders, \n",
    "                            7, \n",
    "                            attr_labels, \n",
    "                            model.device\n",
    "                    )\n",
    "                    proto_dataset_riders = ProtoDataset(proto_data_riders, proto_labels_riders)\n",
    "                    if (proto_labels_riders == 1).sum().item() == 0:    continue\n",
    "                    \n",
    "                    # * [others]\n",
    "                    proto_data_others, proto_labels_others, sz_others = get_prototypical_datasets_inputs(\n",
    "                            concat_input_others, \n",
    "                            8, \n",
    "                            attr_labels, \n",
    "                            model.device\n",
    "                    )\n",
    "                    proto_dataset_others = ProtoDataset(proto_data_others, proto_labels_others)\n",
    "                    if (proto_labels_others == 1).sum().item() == 0:    continue\n",
    "                \n",
    "                # & Build Prototypical Batch Samplers and Episodic DataLoaders\n",
    "                # * [traffic lights]\n",
    "                proto_sampler_traffic_lights = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_traffic_lights.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_traffic_lights = DataLoader(proto_dataset_traffic_lights, batch_sampler=proto_sampler_traffic_lights)\n",
    "                \n",
    "                # * [traffic signs]\n",
    "                proto_sampler_traffic_signs = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_traffic_signs.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_traffic_signs = DataLoader(proto_dataset_traffic_signs, batch_sampler=proto_sampler_traffic_signs)\n",
    "                if debug:   print(\"Number of batches in Traffic Signs DataLoader:\", len(proto_loader_traffic_signs))\n",
    "\n",
    "                # * [cars]\n",
    "                proto_sampler_cars = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_cars.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_cars = DataLoader(proto_dataset_cars, batch_sampler=proto_sampler_cars)\n",
    "                if debug:   print(\"Number of batches in Cars DataLoader:\", len(proto_loader_cars))\n",
    "\n",
    "                # * [pedestrians]\n",
    "                proto_sampler_pedestrians = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_pedestrians.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_pedestrians = DataLoader(proto_dataset_pedestrians, batch_sampler=proto_sampler_pedestrians)\n",
    "                if debug:   print(\"Number of batches in Pedestrians DataLoader:\", len(proto_loader_pedestrians))\n",
    "                \n",
    "                # * [riders]\n",
    "                proto_sampler_riders = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_riders.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_riders = DataLoader(proto_dataset_riders, batch_sampler=proto_sampler_riders)\n",
    "                if debug:   print(\"Number of batches in Riders DataLoader:\", len(proto_loader_riders))\n",
    "                \n",
    "                # * [others]\n",
    "                proto_sampler_others = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels_others.cpu().numpy(), \n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "                proto_loader_others = DataLoader(proto_dataset_others, batch_sampler=proto_sampler_others)\n",
    "                if debug:   print(\"Number of batches in Others DataLoader:\", len(proto_loader_others))\n",
    "\n",
    "                print(f\"Number of proto_labels_traffic_lights that are 0: {(proto_labels_traffic_lights == 0).sum().item()}, 1: {(proto_labels_traffic_lights == 1).sum().item()}\")\n",
    "                print(f\"Number of proto_labels_traffic_signs that are 0: {(proto_labels_traffic_signs == 0).sum().item()}, 1: {(proto_labels_traffic_signs == 1).sum().item()}\")\n",
    "                print(f\"Number of proto_labels_pedestrians that are 0: {(proto_labels_pedestrians == 0).sum().item()}, 1: {(proto_labels_pedestrians == 1).sum().item()}\")\n",
    "                print(f\"Number of proto_labels_cars that are 0: {(proto_labels_cars == 0).sum().item()}, 1: {(proto_labels_cars == 1).sum().item()}\")\n",
    "                print(f\"Number of proto_labels_riders that are 0: {(proto_labels_riders == 0).sum().item()}, 1: {(proto_labels_riders == 1).sum().item()}\")\n",
    "                print(f\"Number of proto_labels_others that are 0: {(proto_labels_others == 0).sum().item()}, 1: {(proto_labels_others == 1).sum().item()}\")\n",
    "\n",
    "                # & Train Prototypical Networks\n",
    "                traffic_lights_model.train()\n",
    "                traffic_signs_model.train()\n",
    "                car_model.train()\n",
    "                pedestrians_model.train()\n",
    "                rider_model.train()\n",
    "                others_model.train()\n",
    "                for e in range(args.proto_epochs):\n",
    "                    epoch_loss_traffic_lights, epoch_acc_traffic_lights = train_my_prototypical_network(\n",
    "                        proto_loader_traffic_lights, args.iterations, traffic_lights_model, traffic_lights_optimizer, pNet_loss\n",
    "                    )\n",
    "                    epoch_loss_traffic_signs, epoch_acc_traffic_signs = train_my_prototypical_network(\n",
    "                        proto_loader_traffic_signs, args.iterations, traffic_signs_model, traffic_signs_model_optimizer, pNet_loss\n",
    "                    )\n",
    "                    epoch_loss_cars, epoch_acc_cars = train_my_prototypical_network(\n",
    "                        proto_loader_cars, args.iterations, car_model, car_model_optimizer, pNet_loss\n",
    "                    )\n",
    "                    epoch_loss_pedestrians, epoch_acc_pedestrians = train_my_prototypical_network(\n",
    "                        proto_loader_pedestrians, args.iterations, pedestrians_model, pedestrians_model_optimizer, pNet_loss\n",
    "                    )\n",
    "                    epoch_loss_riders, epoch_acc_riders = train_my_prototypical_network(\n",
    "                        proto_loader_riders, args.iterations, rider_model, rider_model_optimizer, pNet_loss\n",
    "                    )\n",
    "                    epoch_loss_others, epoch_acc_others = train_my_prototypical_network(\n",
    "                        proto_loader_others, args.iterations, others_model, others_model_optimizer, pNet_loss\n",
    "                    )\n",
    "\n",
    "                avg_loss_tf = sum(epoch_loss_traffic_lights) / len(epoch_loss_traffic_lights)\n",
    "                avg_acc_tf  = sum(epoch_acc_traffic_lights)  / len(epoch_acc_traffic_lights)\n",
    "                print(f\"Traffic Lights Features  - Avg Loss: {avg_loss_tf:.4f} | Avg Acc: {avg_acc_tf:.4f}\")\n",
    "\n",
    "                avg_loss_ts = sum(epoch_loss_traffic_signs) / len(epoch_loss_traffic_signs)\n",
    "                avg_acc_ts  = sum(epoch_acc_traffic_signs)  / len(epoch_acc_traffic_signs)\n",
    "                print(f\"Traffic Signs Features  - Avg Loss: {avg_loss_ts:.4f} | Avg Acc: {avg_acc_ts:.4f}\")\n",
    "\n",
    "                avg_loss_c = sum(epoch_loss_cars) / len(epoch_loss_cars)\n",
    "                avg_acc_c  = sum(epoch_acc_cars)  / len(epoch_acc_cars)\n",
    "                print(f\"Cars Features  - Avg Loss: {avg_loss_c:.4f} | Avg Acc: {avg_acc_c:.4f}\")\n",
    "\n",
    "                avg_loss_p = sum(epoch_loss_pedestrians) / len(epoch_loss_pedestrians)\n",
    "                avg_acc_p  = sum(epoch_acc_pedestrians)  / len(epoch_acc_pedestrians)\n",
    "                print(f\"Pedestrians Features  - Avg Loss: {avg_loss_p:.4f} | Avg Acc: {avg_acc_p:.4f}\")\n",
    "\n",
    "                avg_loss_r = sum(epoch_loss_riders) / len(epoch_loss_riders)\n",
    "                avg_acc_r  = sum(epoch_acc_riders)  / len(epoch_acc_riders)\n",
    "                print(f\"Riders Features  - Avg Loss: {avg_loss_r:.4f} | Avg Acc: {avg_acc_r:.4f}\")\n",
    "\n",
    "                avg_loss_o = sum(epoch_loss_others) / len(epoch_loss_others)\n",
    "                avg_acc_o  = sum(epoch_acc_others)  / len(epoch_acc_others)\n",
    "                print(f\"Others Features  - Avg Loss: {avg_loss_o:.4f} | Avg Acc: {avg_acc_o:.4f}\")\n",
    "\n",
    "                traffic_lights_scheduler.step()\n",
    "                traffic_signs_model_scheduler.step()\n",
    "                car_model_scheduler.step()\n",
    "                pedestrians_model_scheduler.step()\n",
    "                rider_model_scheduler.step()\n",
    "                others_model_scheduler.step()\n",
    "                \n",
    "                execute = False\n",
    "                compute_prototypical_batch = False\n",
    "\n",
    "            # --------------------------------------\n",
    "            # ^ MAIN MODEL TRAINING\n",
    "            # --------------------------------------\n",
    "            else:\n",
    "                if random.random() > UNS_PERCENTAGE:\n",
    "                    continue  # Skip this batch with probability (1 - percentage)\n",
    "\n",
    "                # & Use the expert Prototypical Networks to compute logits for their classes of expertise\n",
    "                traffic_lights_model.eval()\n",
    "                traffic_signs_model.eval()\n",
    "                car_model.eval()\n",
    "                pedestrians_model.eval()\n",
    "                rider_model.eval()\n",
    "                others_model.eval()\n",
    "\n",
    "                support_embeddings_traffic_lights, support_labels_traffic_lights = get_random_classes(\n",
    "                    proto_dataset_traffic_lights.embeddings, proto_dataset_traffic_lights.labels, sz_traffic_lights, 2\n",
    "                )\n",
    "                support_embeddings_traffic_signs, support_labels_traffic_signs = get_random_classes(\n",
    "                    proto_dataset_traffic_signs.embeddings, proto_dataset_traffic_signs.labels, sz_traffic_signs, 2\n",
    "                )\n",
    "                support_embeddings_cars, support_labels_cars = get_random_classes(\n",
    "                    proto_dataset_cars.embeddings, proto_dataset_cars.labels, sz_cars, 2\n",
    "                )\n",
    "                support_embeddings_pedestrians, support_labels_pedestrians = get_random_classes(\n",
    "                    proto_dataset_pedestrians.embeddings, proto_dataset_pedestrians.labels, sz_pedestrians, 2\n",
    "                )\n",
    "                support_embeddings_riders, support_labels_riders = get_random_classes(\n",
    "                    proto_dataset_riders.embeddings, proto_dataset_riders.labels, sz_riders, 2\n",
    "                )\n",
    "                support_embeddings_others, support_labels_others = get_random_classes(\n",
    "                    proto_dataset_others.embeddings, proto_dataset_others.labels, sz_others, 2\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits_tfs = compute_class_logits_per_batch(concat_inputs_traffic_lights, \n",
    "                                support_embeddings_traffic_lights, \n",
    "                                support_labels_traffic_lights, \n",
    "                                traffic_lights_model,\n",
    "                            )\n",
    "                    logits_ts = compute_class_logits_per_batch(concat_input_traffic_signs,\n",
    "                                support_embeddings_traffic_signs, \n",
    "                                support_labels_traffic_signs, \n",
    "                                traffic_signs_model,\n",
    "                            )\n",
    "                    logits_cars = compute_class_logits_per_batch(concat_input_cars,\n",
    "                                support_embeddings_cars, \n",
    "                                support_labels_cars, \n",
    "                                car_model,\n",
    "                            )\n",
    "                    logits_peds = compute_class_logits_per_batch(concat_input_pedestrians,\n",
    "                                support_embeddings_pedestrians, \n",
    "                                support_labels_pedestrians, \n",
    "                                pedestrians_model,\n",
    "                            )\n",
    "                    logits_rid = compute_class_logits_per_batch(concat_input_riders,\n",
    "                                support_embeddings_riders, \n",
    "                                support_labels_riders, \n",
    "                                rider_model,\n",
    "                            )\n",
    "                    logits_oth = compute_class_logits_per_batch(concat_input_others,\n",
    "                                support_embeddings_others, \n",
    "                                support_labels_others, \n",
    "                                others_model,\n",
    "                            )\n",
    "\n",
    "                    assert logits_tfs.shape == (len(concat_inputs_traffic_lights),), f\"Unexpected logits shape: {logits_tfs.shape}\"\n",
    "                    assert logits_ts.shape == (len(concat_input_traffic_signs),), f\"Unexpected logits shape: {logits_ts.shape}\"\n",
    "                    assert logits_cars.shape == (len(concat_input_cars),), f\"Unexpected logits shape: {logits_cars.shape}\"\n",
    "                    assert logits_peds.shape == (len(concat_input_pedestrians),), f\"Unexpected logits shape: {logits_peds.shape}\"\n",
    "                    assert logits_rid.shape == (len(concat_input_riders),), f\"Unexpected logits shape: {logits_rid.shape}\"\n",
    "                    assert logits_oth.shape == (len(concat_input_others),), f\"Unexpected logits shape: {logits_oth.shape}\"\n",
    "\n",
    "                # & Main Standard Training\n",
    "                traffic_lights_model.train()\n",
    "                traffic_signs_model.train()\n",
    "                car_model.train()\n",
    "                pedestrians_model.train()\n",
    "                rider_model.train()\n",
    "                others_model.train()\n",
    "\n",
    "                out_dict = model(images_embeddings, \n",
    "                            logits_tfs=logits_tfs, \n",
    "                            logits_ts=logits_ts,\n",
    "                            logits_cars=logits_cars,\n",
    "                            logits_peds=logits_peds,\n",
    "                            logits_rid=logits_rid,\n",
    "                            logits_oth=logits_oth, \n",
    "                        )  \n",
    "                \n",
    "                out_dict.update({\"LABELS\": class_labels, \"CONCEPTS\": attr_labels})\n",
    "                \n",
    "                model.opt.zero_grad()\n",
    "                loss, losses = _loss(out_dict, args)\n",
    "\n",
    "                loss.backward()\n",
    "                model.opt.step()\n",
    "\n",
    "                if ys is None:\n",
    "                    ys = out_dict[\"YS\"]\n",
    "                    y_true = out_dict[\"LABELS\"]\n",
    "                    cs = out_dict[\"pCS\"]\n",
    "                    cs_true = out_dict[\"CONCEPTS\"]\n",
    "                else:\n",
    "                    ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                    y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                    cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                    cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "                \n",
    "        # --------------------------------------\n",
    "        # ^ Evaluation phase\n",
    "        # --------------------------------------    \n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        acc, f1 = accuracy_binary(ys, y_true)\n",
    "        print(\"\\n Train Label acc: \", acc, \"Train Label f1\", f1,)\n",
    "        \n",
    "        # & Compute the support set for evaluating the Prototypical Networks + Main Model\n",
    "        model.eval()\n",
    "        traffic_lights_model.eval()\n",
    "        traffic_signs_model.eval()\n",
    "        car_model.eval()\n",
    "        pedestrians_model.eval()\n",
    "        rider_model.eval()\n",
    "        others_model.eval()\n",
    "\n",
    "        support_embeddings_traffic_lights, support_labels_traffic_lights = get_random_classes(\n",
    "            proto_dataset_traffic_lights.embeddings, proto_dataset_traffic_lights.labels, sz_traffic_lights, 2\n",
    "        )\n",
    "        support_embeddings_traffic_signs, support_labels_traffic_signs = get_random_classes(\n",
    "            proto_dataset_traffic_signs.embeddings, proto_dataset_traffic_signs.labels, sz_traffic_signs, 2\n",
    "        )\n",
    "        support_embeddings_cars, support_labels_cars = get_random_classes(\n",
    "            proto_dataset_cars.embeddings, proto_dataset_cars.labels, sz_cars, 2\n",
    "        )\n",
    "        support_embeddings_pedestrians, support_labels_pedestrians = get_random_classes(\n",
    "            proto_dataset_pedestrians.embeddings, proto_dataset_pedestrians.labels, sz_pedestrians, 2\n",
    "        )\n",
    "        support_embeddings_riders, support_labels_riders = get_random_classes(\n",
    "            proto_dataset_riders.embeddings, proto_dataset_riders.labels, sz_riders, 2\n",
    "        )\n",
    "        support_embeddings_others, support_labels_others = get_random_classes(\n",
    "            proto_dataset_others.embeddings, proto_dataset_others.labels, sz_others, 2\n",
    "        )\n",
    "        my_metrics = evaluate_metrics(\n",
    "                            model=model, \n",
    "                            loader=val_loader,\n",
    "                            args=args,\n",
    "                            support_embeddings_traffic_lights=support_embeddings_traffic_lights,\n",
    "                            support_labels_traffic_lights=support_labels_traffic_lights,\n",
    "                            support_embeddings_traffic_signs=support_embeddings_traffic_signs,\n",
    "                            support_labels_traffic_signs=support_labels_traffic_signs,\n",
    "                            support_embeddings_cars=support_embeddings_cars,\n",
    "                            support_labels_cars=support_labels_cars,\n",
    "                            support_embeddings_pedestrians=support_embeddings_pedestrians,\n",
    "                            support_labels_pedestrians=support_labels_pedestrians,\n",
    "                            support_embeddings_riders=support_embeddings_riders,\n",
    "                            support_labels_riders=support_labels_riders,\n",
    "                            support_embeddings_others=support_embeddings_others,\n",
    "                            support_labels_others=support_labels_others,\n",
    "                            traffic_lights_model=traffic_lights_model,\n",
    "                            traffic_signs_model=traffic_signs_model,\n",
    "                            car_model=car_model,\n",
    "                            pedestrians_model=pedestrians_model,\n",
    "                            rider_model=rider_model,\n",
    "                            others_model=others_model,\n",
    "                            eval_concepts=['traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others'],\n",
    "                        )\n",
    "\n",
    "        loss = my_metrics[0]\n",
    "        cacc = my_metrics[1]\n",
    "        yacc = my_metrics[2]\n",
    "        f1_stop_concepts = my_metrics[83]\n",
    "\n",
    "        # update at end of the epoch\n",
    "        if epoch < args.warmup_steps:   w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1, \"F1 C\", f1_stop_concepts)\n",
    "        \n",
    "        if not args.tuning and f1_stop_concepts > best_f1_stop_concepts:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_f1_stop_concepts = f1_stop_concepts\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with STOP F1(C) score: {best_f1_stop_concepts}\")\n",
    "            if epoch == 0:\n",
    "                torch.save(model.state_dict(), save_path.replace(\".pth\", \"_backup.pth\"))\n",
    "                print(\"Saved backup copy of the model.\")\n",
    "\n",
    "        elif f1_stop_concepts <= best_f1_stop_concepts:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "\n",
    "    # return all the prototypical datasets used to train the prototypical networks\n",
    "    all_prototypical_datasets = {\n",
    "        \"traffic_lights\": proto_dataset_traffic_lights,\n",
    "        \"traffic_signs\": proto_dataset_traffic_signs,\n",
    "        \"cars\": proto_dataset_cars,\n",
    "        \"pedestrians\": proto_dataset_pedestrians,\n",
    "        \"riders\": proto_dataset_riders,\n",
    "        \"others\": proto_dataset_others\n",
    "    }\n",
    "\n",
    "    # return all the sizes of the prototypical datasets\n",
    "    all_sz = {\n",
    "        \"traffic_lights\": sz_traffic_lights,\n",
    "        \"traffic_signs\": sz_traffic_signs,\n",
    "        \"cars\": sz_cars,\n",
    "        \"pedestrians\": sz_pedestrians,\n",
    "        \"riders\": sz_riders,\n",
    "        \"others\": sz_others\n",
    "    }\n",
    "    \n",
    "    return best_f1_stop_concepts, all_prototypical_datasets, all_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e6feb",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "# ! make train return all the prototypical datasets built and use them for evaluationd\n",
    "best_f1_c, all_prototypical_datasets, all_sz = train(\n",
    "        model=model,\n",
    "        # ^ Prototypical Networks (start)\n",
    "        traffic_lights_model=traffic_lights_model, # 3\n",
    "        traffic_signs_model=traffic_signs_model, # 4\n",
    "        car_model=car_model, # 5\n",
    "        pedestrians_model=pedestrians_model, # 6\n",
    "        rider_model=rider_model, # 7\n",
    "        others_model=others_model, # 8\n",
    "        # ^ Prototypical Networks (end)\n",
    "        train_loader=unsup_train_loader,\n",
    "        val_loader=unsup_val_loader,\n",
    "        save_path=save_folder,\n",
    "        _loss=loss,\n",
    "        args=args,\n",
    "        seed=SEED,\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "print(f\"*** Finished training model with seed {args.seed} and best F1 score {best_f1_c}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb08277",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a2e9e",
   "metadata": {},
   "source": [
    "## Evaluation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc647f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the datasets and the sizes for the prototypical networks\n",
    "proto_dataset_traffic_lights = all_prototypical_datasets[\"traffic_lights\"]\n",
    "proto_dataset_traffic_signs = all_prototypical_datasets[\"traffic_signs\"]\n",
    "proto_dataset_cars = all_prototypical_datasets[\"cars\"]\n",
    "proto_dataset_pedestrians = all_prototypical_datasets[\"pedestrians\"]\n",
    "proto_dataset_riders = all_prototypical_datasets[\"riders\"]\n",
    "proto_dataset_others = all_prototypical_datasets[\"others\"]\n",
    "\n",
    "sz_traffic_lights = all_sz[\"traffic_lights\"]\n",
    "sz_traffic_signs = all_sz[\"traffic_signs\"]\n",
    "sz_cars = all_sz[\"cars\"]\n",
    "sz_pedestrians = all_sz[\"pedestrians\"]\n",
    "sz_riders = all_sz[\"riders\"]\n",
    "sz_others = all_sz[\"others\"]\n",
    "\n",
    "print(\"Prototypical datasets built and saved.\")\n",
    "print(\"Prototypical datasets sizes: \")\n",
    "print(\"Traffic Lights: \", sz_traffic_lights)\n",
    "print(\"Traffic Signs: \", sz_traffic_signs)\n",
    "print(\"Cars: \", sz_cars)\n",
    "print(\"Pedestrians: \", sz_pedestrians)\n",
    "print(\"Riders: \", sz_riders)\n",
    "print(\"Others: \", sz_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_my_model(model: MnistDPL, \n",
    "        save_path: str, \n",
    "        test_loader: DataLoader,\n",
    "    ):\n",
    "    \n",
    "    model.eval()\n",
    "    traffic_lights_model.eval()\n",
    "    traffic_signs_model.eval()\n",
    "    car_model.eval()\n",
    "    pedestrians_model.eval()\n",
    "    rider_model.eval()\n",
    "    others_model.eval()\n",
    "\n",
    "    support_embeddings_traffic_lights, support_labels_traffic_lights = get_random_classes(\n",
    "        proto_dataset_traffic_lights.embeddings, proto_dataset_traffic_lights.labels, sz_traffic_lights, 2\n",
    "    )\n",
    "    support_embeddings_traffic_signs, support_labels_traffic_signs = get_random_classes(\n",
    "        proto_dataset_traffic_signs.embeddings, proto_dataset_traffic_signs.labels, sz_traffic_signs, 2\n",
    "    )\n",
    "    support_embeddings_cars, support_labels_cars = get_random_classes(\n",
    "        proto_dataset_cars.embeddings, proto_dataset_cars.labels, sz_cars, 2\n",
    "    )\n",
    "    support_embeddings_pedestrians, support_labels_pedestrians = get_random_classes(\n",
    "        proto_dataset_pedestrians.embeddings, proto_dataset_pedestrians.labels, sz_pedestrians, 2\n",
    "    )\n",
    "    support_embeddings_riders, support_labels_riders = get_random_classes(\n",
    "        proto_dataset_riders.embeddings, proto_dataset_riders.labels, sz_riders, 2\n",
    "    )\n",
    "    support_embeddings_others, support_labels_others = get_random_classes(\n",
    "        proto_dataset_others.embeddings, proto_dataset_others.labels, sz_others, 2\n",
    "    )\n",
    "    my_metrics = evaluate_metrics(\n",
    "                        model=model, \n",
    "                        loader=test_loader,\n",
    "                        args=args,\n",
    "                        support_embeddings_traffic_lights=support_embeddings_traffic_lights,\n",
    "                        support_labels_traffic_lights=support_labels_traffic_lights,\n",
    "                        support_embeddings_traffic_signs=support_embeddings_traffic_signs,\n",
    "                        support_labels_traffic_signs=support_labels_traffic_signs,\n",
    "                        support_embeddings_cars=support_embeddings_cars,\n",
    "                        support_labels_cars=support_labels_cars,\n",
    "                        support_embeddings_pedestrians=support_embeddings_pedestrians,\n",
    "                        support_labels_pedestrians=support_labels_pedestrians,\n",
    "                        support_embeddings_riders=support_embeddings_riders,\n",
    "                        support_labels_riders=support_labels_riders,\n",
    "                        support_embeddings_others=support_embeddings_others,\n",
    "                        support_labels_others=support_labels_others,\n",
    "                        traffic_lights_model=traffic_lights_model,\n",
    "                        traffic_signs_model=traffic_signs_model,\n",
    "                        car_model=car_model,\n",
    "                        pedestrians_model=pedestrians_model,\n",
    "                        rider_model=rider_model,\n",
    "                        others_model=others_model,\n",
    "                        eval_concepts=['traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others'],\n",
    "                    )\n",
    "    \n",
    "    loss = my_metrics[0]\n",
    "    cacc = my_metrics[1]\n",
    "    yacc = my_metrics[2]\n",
    "    f1 = my_metrics[3]\n",
    "    # Save all metrics to the log file\n",
    "    metrics_log_path = save_path.replace(\".pth\", \"_metrics.log\")\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ACC C: {cacc}, ACC Y: {yacc}, F1 Y: {f1}\\n\\n\")\n",
    "\n",
    "        def write_metrics(class_name, offset):\n",
    "            log_file.write(f\"{class_name.upper()}\\n\")\n",
    "            log_file.write(f\"  F1 - Binary:   {my_metrics[offset]:.4f}\\n\")\n",
    "            log_file.write(f\"  F1 - Macro:    {my_metrics[offset+1]:.4f}\\n\")\n",
    "            log_file.write(f\"  F1 - Micro:    {my_metrics[offset+2]:.4f}\\n\")\n",
    "            log_file.write(f\"  F1 - Weighted: {my_metrics[offset+3]:.4f}\\n\")\n",
    "            log_file.write(f\"  Precision - Binary:   {my_metrics[offset+4]:.4f}\\n\")\n",
    "            log_file.write(f\"  Precision - Macro:    {my_metrics[offset+5]:.4f}\\n\")\n",
    "            log_file.write(f\"  Precision - Micro:    {my_metrics[offset+6]:.4f}\\n\")\n",
    "            log_file.write(f\"  Precision - Weighted: {my_metrics[offset+7]:.4f}\\n\")\n",
    "            log_file.write(f\"  Recall - Binary:   {my_metrics[offset+8]:.4f}\\n\")\n",
    "            log_file.write(f\"  Recall - Macro:    {my_metrics[offset+9]:.4f}\\n\")\n",
    "            log_file.write(f\"  Recall - Micro:    {my_metrics[offset+10]:.4f}\\n\")\n",
    "            log_file.write(f\"  Recall - Weighted: {my_metrics[offset+11]:.4f}\\n\")\n",
    "            log_file.write(f\"  Balanced Accuracy: {my_metrics[offset+12]:.4f}\\n\\n\")\n",
    "\n",
    "        write_metrics(\"Traffic Light\", 4)\n",
    "        write_metrics(\"Traffic Sign\", 17)\n",
    "        write_metrics(\"Car\", 30)\n",
    "        write_metrics(\"Pedestrian\", 43)\n",
    "        write_metrics(\"Rider\", 56)\n",
    "        write_metrics(\"Other\", 69)\n",
    "\n",
    "        log_file.write(\"EVAL CONCEPTS (Aggregated)\\n\")\n",
    "        log_file.write(f\"  F1 - Binary:   {my_metrics[82]:.4f}\\n\")\n",
    "        log_file.write(f\"  F1 - Macro:    {my_metrics[83]:.4f}\\n\")\n",
    "        log_file.write(f\"  F1 - Micro:    {my_metrics[84]:.4f}\\n\")\n",
    "        log_file.write(f\"  F1 - Weighted: {my_metrics[85]:.4f}\\n\")\n",
    "        log_file.write(f\"  Precision - Binary:   {my_metrics[86]:.4f}\\n\")\n",
    "        log_file.write(f\"  Precision - Macro:    {my_metrics[87]:.4f}\\n\")\n",
    "        log_file.write(f\"  Precision - Micro:    {my_metrics[88]:.4f}\\n\")\n",
    "        log_file.write(f\"  Precision - Weighted: {my_metrics[89]:.4f}\\n\")\n",
    "        log_file.write(f\"  Recall - Binary:   {my_metrics[90]:.4f}\\n\")\n",
    "        log_file.write(f\"  Recall - Macro:    {my_metrics[91]:.4f}\\n\")\n",
    "        log_file.write(f\"  Recall - Micro:    {my_metrics[92]:.4f}\\n\")\n",
    "        log_file.write(f\"  Recall - Weighted: {my_metrics[93]:.4f}\\n\")\n",
    "        log_file.write(f\"  Balanced Accuracy: {my_metrics[94]:.4f}\\n\\n\")\n",
    "\n",
    "        \n",
    "    y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "        evaluate_metrics(\n",
    "            model=model, \n",
    "            loader=test_loader,\n",
    "            args=args,\n",
    "            support_embeddings_traffic_lights=support_embeddings_traffic_lights,\n",
    "            support_labels_traffic_lights=support_labels_traffic_lights,\n",
    "            support_embeddings_traffic_signs=support_embeddings_traffic_signs,\n",
    "            support_labels_traffic_signs=support_labels_traffic_signs,\n",
    "            support_embeddings_cars=support_embeddings_cars,\n",
    "            support_labels_cars=support_labels_cars,\n",
    "            support_embeddings_pedestrians=support_embeddings_pedestrians,\n",
    "            support_labels_pedestrians=support_labels_pedestrians,\n",
    "            support_embeddings_riders=support_embeddings_riders,\n",
    "            support_labels_riders=support_labels_riders,\n",
    "            support_embeddings_others=support_embeddings_others,\n",
    "            support_labels_others=support_labels_others,\n",
    "            traffic_lights_model=traffic_lights_model,\n",
    "            traffic_signs_model=traffic_signs_model,\n",
    "            car_model=car_model,\n",
    "            pedestrians_model=pedestrians_model,\n",
    "            rider_model=rider_model,\n",
    "            others_model=others_model,\n",
    "            eval_concepts=['traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others'],\n",
    "            last=True,\n",
    "        )\n",
    "    )\n",
    "    y_labels = [\"stop\", \"forward\", \"left\", \"right\"]\n",
    "    concept_labels = [\n",
    "        \"green_light\",      \n",
    "        \"follow\",           \n",
    "        \"road_clear\",       \n",
    "        \"red_light\",        # ! 3\n",
    "        \"traffic_sign\",     # ! 4\n",
    "        \"car\",              # ! 5\n",
    "        \"person\",           # ! 6\n",
    "        \"rider\",            # ! 7\n",
    "        \"other_obstacle\",   # ! 8\n",
    "        \"left_lane\",\n",
    "        \"left_green_light\",\n",
    "        \"left_follow\",\n",
    "        \"no_left_lane\",\n",
    "        \"left_obstacle\",\n",
    "        \"letf_solid_line\",\n",
    "        \"right_lane\",\n",
    "        \"right_green_light\",\n",
    "        \"right_follow\",\n",
    "        \"no_right_lane\",\n",
    "        \"right_obstacle\",\n",
    "        \"right_solid_line\",\n",
    "    ]\n",
    "\n",
    "    plot_multilabel_confusion_matrix(y_true, y_pred, y_labels, \"Labels\", save_path=save_path)\n",
    "    cfs = plot_actions_confusion_matrix(c_true, c_pred, \"Concepts\", save_path=save_path)\n",
    "    cf = plot_multilabel_confusion_matrix(c_true, c_pred, concept_labels, \"Concepts\", save_path=save_path)\n",
    "    print(\"Concept collapse\", 1 - compute_coverage(cf))\n",
    "\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        for key, value in cfs.items():\n",
    "            log_file.write(f\"Concept collapse: {key}, {1 - compute_coverage(value):.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "    print(\"Evaluation metrics saved to:\", metrics_log_path)\n",
    "    fprint(\"\\n--- End of Evaluation ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87947313",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model object\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "\n",
    "# Load the model state dictionary into the model object\n",
    "# save_folder = '../NEW-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-article-STOP/dpl_200_backup.pth'\n",
    "model_state_dict = torch.load(save_folder)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_my_model(model, save_folder, unsup_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
