{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db624e1e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee0a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a97cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import setproctitle, socket, uuid\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from datasets import get_dataset\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "from utils import fprint\n",
    "from utils.status import progress_bar\n",
    "from utils.metrics import evaluate_metrics\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.checkpoint import save_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from baseline_modules.arguments import args_dpl \n",
    "from backbones.bddoia_protonet import ProtoNetConv1D, PrototypicalLoss\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_helpers import assert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b846",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce461ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "UNS_PERCENTAGE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0680d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n",
      "Save paths: ['../NEW-outputs/bddoia/baseline/dpl/baseline-disj-PRE']\n"
     ]
    }
   ],
   "source": [
    "args = args_dpl\n",
    "args.seed = SEED\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"bddoia\" \n",
    "save_model_name = 'dpl'\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"NEW-outputs\", \n",
    "    save_folder, \n",
    "    \"baseline\", \n",
    "    save_model_name,\n",
    "    f\"baseline-disj-PRE\"\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "print(f\"Save paths: {str(save_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511d01",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c1be3",
   "metadata": {},
   "source": [
    "## Test Set Evaluation (alternative to full fledged notebook eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c24d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * helper function for 'plot_multilabel_confusion_matrix'\n",
    "def convert_to_categories(elements):\n",
    "    # Convert vector of 0s and 1s to a single binary representation along the first dimension\n",
    "    binary_rep = np.apply_along_axis(\n",
    "        lambda x: \"\".join(map(str, x)), axis=1, arr=elements\n",
    "    )\n",
    "    return np.array([int(x, 2) for x in binary_rep])\n",
    "\n",
    "\n",
    "# * BBDOIA custom confusion matrix for concepts\n",
    "def plot_multilabel_confusion_matrix(\n",
    "    y_true, y_pred, class_names, title, save_path=None\n",
    "):\n",
    "    y_true_categories = convert_to_categories(y_true.astype(int))\n",
    "    y_pred_categories = convert_to_categories(y_pred.astype(int))\n",
    "\n",
    "    to_rtn_cm = confusion_matrix(y_true_categories, y_pred_categories)\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    num_classes = len(class_names)\n",
    "    num_rows = (num_classes + 4) // 5  # Calculate the number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_rows))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.subplot(num_rows, 5, i + 1)  # Set the subplot position\n",
    "        plt.imshow(cm[i], interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Class: {class_names[i]}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "        plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "        fmt = \".0f\"\n",
    "        thresh = cm[i].max() / 2.0\n",
    "        for j in range(cm[i].shape[0]):\n",
    "            for k in range(cm[i].shape[1]):\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    format(cm[i][j, k], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i][j, k] > thresh else \"black\",\n",
    "                )\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_total.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return to_rtn_cm\n",
    "\n",
    "\n",
    "# * Concept collapse (Soft)\n",
    "def compute_coverage(confusion_matrix):\n",
    "    \"\"\"Compute the coverage of a confusion matrix.\n",
    "\n",
    "    Essentially this metric is\n",
    "    \"\"\"\n",
    "\n",
    "    max_values = np.max(confusion_matrix, axis=0)\n",
    "    clipped_values = np.clip(max_values, 0, 1)\n",
    "\n",
    "    # Redefinition of soft coverage\n",
    "    coverage = np.sum(clipped_values) / len(clipped_values)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# * BDDOIA custom confusion matrix for actions\n",
    "def plot_actions_confusion_matrix(c_true, c_pred, title, save_path=None):\n",
    "\n",
    "    # Define scenarios and corresponding labels\n",
    "    my_scenarios = {\n",
    "        \"forward\": [slice(0, 3), slice(0, 3)],  \n",
    "        \"stop\": [slice(3, 9), slice(3, 9)],\n",
    "        \"left\": [slice(9, 11), slice(18,20)],\n",
    "        \"right\": [slice(12, 17), slice(12,17)],\n",
    "    }\n",
    "\n",
    "    to_rtn = {}\n",
    "\n",
    "    # Plot confusion matrix for each scenario\n",
    "    for scenario, indices in my_scenarios.items():\n",
    "\n",
    "        g_true = convert_to_categories(c_true[:, indices[0]].astype(int))\n",
    "        c_pred_scenario = convert_to_categories(c_pred[:, indices[1]].astype(int))\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(g_true, c_pred_scenario)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{title} - {scenario}\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        n_classes = c_true[:, indices[0]].shape[1]\n",
    "\n",
    "        tick_marks = np.arange(2**n_classes)\n",
    "        plt.xticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "        plt.yticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or show plot\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{scenario}.png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        to_rtn.update({scenario: cm})\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return to_rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3245d",
   "metadata": {},
   "source": [
    "# Other Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bce4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * method used to check if all encoder parameters are registered in the model's optimizer\n",
    "def check_optimizer_params(model):\n",
    "    \"\"\"Check that all encoder parameters are registered in the optimizer.\"\"\"\n",
    "    # Get all encoder parameters\n",
    "    encoder_params = []\n",
    "    for i in range(21):\n",
    "        encoder = model.encoder[i]\n",
    "        for name, param in encoder.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue  # skip frozen params\n",
    "            encoder_params.append((f\"encoder_{i}.{name}\", param))\n",
    "\n",
    "    # Get all parameters in the optimizer\n",
    "    opt_param_ids = set(id(p) for group in model.opt.param_groups for p in group['params'])\n",
    "\n",
    "    # Check each encoder param is in the optimizer\n",
    "    missing = [(name, p.shape) for name, p in encoder_params if id(p) not in opt_param_ids]\n",
    "\n",
    "    if missing:\n",
    "        print(\"⚠️ The following parameters are missing from the optimizer:\")\n",
    "        for name, shape in missing:\n",
    "            print(f\"  - {name}: {shape}\")\n",
    "        raise RuntimeError(\"Some encoder parameters are not registered in the optimizer.\")\n",
    "    else:\n",
    "        print(\"✅ All encoder parameters are correctly registered in the optimizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583857e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bddoiadpldisj'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e79c9",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c811ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bddoiadpldisj'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39a91b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'bddoiadpldisj', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "✅ All encoder parameters are correctly registered in the optimizer.\n",
      "<datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f0f7888d760>\n",
      "Using Dataset:  <datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f0f7888d760>\n",
      "Using backbone:  (BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "), BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "))\n",
      "Using Model:  BddoiaDPLDisj()\n",
      "Using Loss:  SDDOIA_DPL()\n",
      "Dataset sizes - train: 16080, val: 2264, test: 4568\n",
      "Loaded datasets in 0.15s\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "assert isinstance(encoder, tuple) and len(encoder) == 21, \"encoder must be a tuple of 21 elements\"\n",
    "\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "check_optimizer_params(model)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f8268",
   "metadata": {},
   "source": [
    "# PROTOTYPES CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20e70",
   "metadata": {},
   "source": [
    "## DATASET & BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fee336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert embeddings.shape[0] == labels.shape[0]\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01ef6c",
   "metadata": {},
   "source": [
    "## Build positive annotation set for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cd8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id: <class 'tuple'>\n",
      "images_embeddings_raw: torch.Size([2048])\n",
      "attr_labels: torch.Size([21])\n",
      "is_positive: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "pos_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "target_per_class = 6\n",
    "debug = True\n",
    "\n",
    "# Loop over dataset until we collect target_per_class for each class\n",
    "for batch_idx, batch in enumerate(unsup_train_loader):\n",
    "    raw_embs = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "    attrs = torch.stack(batch['attr_labels']).to(model.device)  # shape [B,21]\n",
    "    batch_size = attrs.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        attr_vector = attrs[b].clone().cpu()  # clone to avoid in-place issues\n",
    "        for cls in torch.nonzero(attr_vector).flatten().tolist():\n",
    "            if len(pos_examples[cls]) >= target_per_class:\n",
    "                continue\n",
    "            example = {\n",
    "                'source_id': (batch_idx, b),\n",
    "                'images_embeddings_raw': raw_embs[b].detach().cpu().clone(),\n",
    "                'attr_labels': attr_vector,\n",
    "                'is_positive': True\n",
    "            }\n",
    "            if debug:\n",
    "                for key, value in example.items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        print(f\"{key}: {value.shape}\")\n",
    "                    elif isinstance(value, list) and len(value) and torch.is_tensor(value[0]):\n",
    "                        print(f\"{key}: list of {len(value)} tensors, first shape: {value[0].shape}\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {type(value)}\")\n",
    "                debug = False\n",
    "            pos_examples[cls].append(example)\n",
    "\n",
    "    # Check if all classes reached target\n",
    "    if all(len(pos_examples[c]) >= target_per_class for c in range(21)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddabe2f",
   "metadata": {},
   "source": [
    "## Augment positive sets while building negative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f972a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 1 and ex['source_id'] not in seen_ids:\n",
    "                new_ex = ex.copy()\n",
    "                new_ex['is_positive'] = True\n",
    "                pos_examples[cls].append(new_ex)\n",
    "                seen_ids.add(ex['source_id'])\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids_pos = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 0 and ex['source_id'] not in seen_ids_pos:\n",
    "                neg_ex = ex.copy()\n",
    "                neg_ex['is_positive'] = False\n",
    "                neg_examples[cls].append(neg_ex)\n",
    "\n",
    "for cls in range(21):\n",
    "    assert not set(ex['source_id'] for ex in neg_examples[cls]) & set(ex['source_id'] for ex in pos_examples[cls]), \\\n",
    "        f\"Overlap in pos/neg for class {cls}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21dcb6",
   "metadata": {},
   "source": [
    "## Construct embeddings and labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34199a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: embeddings shape = torch.Size([184, 2048]), labels shape = torch.Size([184, 21])\n",
      "Class 1: embeddings shape = torch.Size([218, 2048]), labels shape = torch.Size([218, 21])\n",
      "Class 2: embeddings shape = torch.Size([188, 2048]), labels shape = torch.Size([188, 21])\n",
      "Class 3: embeddings shape = torch.Size([204, 2048]), labels shape = torch.Size([204, 21])\n",
      "Class 4: embeddings shape = torch.Size([220, 2048]), labels shape = torch.Size([220, 21])\n",
      "Class 5: embeddings shape = torch.Size([226, 2048]), labels shape = torch.Size([226, 21])\n",
      "Class 6: embeddings shape = torch.Size([234, 2048]), labels shape = torch.Size([234, 21])\n",
      "Class 7: embeddings shape = torch.Size([209, 2048]), labels shape = torch.Size([209, 21])\n",
      "Class 8: embeddings shape = torch.Size([226, 2048]), labels shape = torch.Size([226, 21])\n",
      "Class 9: embeddings shape = torch.Size([219, 2048]), labels shape = torch.Size([219, 21])\n",
      "Class 10: embeddings shape = torch.Size([207, 2048]), labels shape = torch.Size([207, 21])\n",
      "Class 11: embeddings shape = torch.Size([225, 2048]), labels shape = torch.Size([225, 21])\n",
      "Class 12: embeddings shape = torch.Size([224, 2048]), labels shape = torch.Size([224, 21])\n",
      "Class 13: embeddings shape = torch.Size([207, 2048]), labels shape = torch.Size([207, 21])\n",
      "Class 14: embeddings shape = torch.Size([224, 2048]), labels shape = torch.Size([224, 21])\n",
      "Class 15: embeddings shape = torch.Size([200, 2048]), labels shape = torch.Size([200, 21])\n",
      "Class 16: embeddings shape = torch.Size([194, 2048]), labels shape = torch.Size([194, 21])\n",
      "Class 17: embeddings shape = torch.Size([211, 2048]), labels shape = torch.Size([211, 21])\n",
      "Class 18: embeddings shape = torch.Size([197, 2048]), labels shape = torch.Size([197, 21])\n",
      "Class 19: embeddings shape = torch.Size([191, 2048]), labels shape = torch.Size([191, 21])\n",
      "Class 20: embeddings shape = torch.Size([218, 2048]), labels shape = torch.Size([218, 21])\n"
     ]
    }
   ],
   "source": [
    "dataset_per_class = {}\n",
    "for cls in range(21):\n",
    "    examples = pos_examples[cls] + neg_examples[cls]\n",
    "    emb_list, label_list = [], []\n",
    "    for ex in examples:\n",
    "        emb_list.append(ex['images_embeddings_raw'].unsqueeze(0))\n",
    "        label_list.append(ex['attr_labels'])\n",
    "    embeddings_tensor = torch.stack(emb_list).to(model.device)  # [N,1,2048]\n",
    "    labels_tensor = torch.stack(label_list)\n",
    "    dataset_per_class[cls] = {'embeddings': embeddings_tensor.squeeze(1), 'labels': labels_tensor}\n",
    "        \n",
    "for cls in range(21):\n",
    "    print(f\"Class {cls}: embeddings shape = {dataset_per_class[cls]['embeddings'].shape}, labels shape = {dataset_per_class[cls]['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fd11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_dataloaders = {}\n",
    "for cls in range(21):\n",
    "    supervised_data = dataset_per_class[cls]['embeddings']\n",
    "    supervised_labels = dataset_per_class[cls]['labels']\n",
    "    supervised_dataset = ProtoDataset(supervised_data, supervised_labels)\n",
    "    supervised_dataloaders[cls] = DataLoader(\n",
    "        supervised_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,       \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b138d",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b4d4",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a8206e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(\n",
    "        model, \n",
    "        dataloaders, \n",
    "        args, \n",
    "        eval_concepts: list = None,\n",
    "        seed: int = 0\n",
    "    ):\n",
    "\n",
    "    # ^ for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # ^ create optimizers and schedulers for each encoder\n",
    "    enc_opts, enc_schs = {}, {}\n",
    "    for c, name in enumerate(eval_concepts):\n",
    "        opt = torch.optim.Adam(model.encoder[c].parameters())\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "        enc_opts[c], enc_schs[c] = opt, sch\n",
    "\n",
    "    # ^ move each encoder to device\n",
    "    for b in range(len(model.encoder)):\n",
    "        model.encoder[b].train()\n",
    "        model.encoder[b].to(model.device)\n",
    "    \n",
    "    fprint(\"\\n--- Start of PreTraining ---\\n\")\n",
    "    for epoch in range(args.proto_epochs):\n",
    "        \n",
    "        # ^ for each concept extractor, train its encoder\n",
    "        for k, name in enumerate(eval_concepts):\n",
    "            dl = dataloaders[k]\n",
    "            opt, sch = enc_opts[k], enc_schs[k]\n",
    "            fprint(f\"\\n--- Pretraining of {name} ---\\n\")\n",
    "            \n",
    "            # training\n",
    "            for i, batch in enumerate(dl):\n",
    "                batch_embeds, batch_labels = batch\n",
    "                batch_embeds = batch_embeds.to(model.device)\n",
    "                batch_labels = batch_labels.to(model.device)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                preds = model.encoder[k](batch_embeds)\n",
    "                assert preds.shape == (batch_embeds.shape[0], 1),\\\n",
    "                    f\"Expected shape ({batch_embeds.shape[0]}, 1), got {preds.shape}\"\n",
    "                \n",
    "                loss = F.binary_cross_entropy(\n",
    "                        preds.squeeze(1), \n",
    "                        batch_labels[:, k].float()\n",
    "                    )\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                progress_bar(i, len(dl), epoch, loss.item())\n",
    "            \n",
    "            sch.step()\n",
    "\n",
    "            # ^ evaluation\n",
    "            model.encoder[k].eval()\n",
    "            eval_loss, correct, total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for batch in dl:  # if you have a validation loader, replace with that\n",
    "                    batch_embeds, batch_labels = batch\n",
    "                    batch_embeds = batch_embeds.to(model.device)\n",
    "                    batch_labels = batch_labels.to(model.device)\n",
    "\n",
    "                    preds = model.encoder[k](batch_embeds).squeeze(1)\n",
    "                    eval_loss += F.binary_cross_entropy(\n",
    "                        preds, batch_labels[:, k].float(), reduction=\"sum\"\n",
    "                    ).item()\n",
    "\n",
    "                    pred_labels = (preds > 0.5).long()\n",
    "                    correct += (pred_labels == batch_labels[:, k]).sum().item()\n",
    "                    total += batch_embeds.size(0)\n",
    "            \n",
    "            avg_loss = eval_loss / total\n",
    "            accuracy = correct / total\n",
    "            fprint(f\"[Epoch {epoch+1}] {name} Eval Loss: {avg_loss:.4f}, \"\n",
    "                   f\"Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            model.encoder[k].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4574b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start of PreTraining ---\n",
      "\n",
      "\n",
      "--- Pretraining of green_lights ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.22626159"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] green_lights Eval Loss: 1.3723, Accuracy: 0.8967\n",
      "\n",
      "--- Pretraining of follow_traffic ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.56628591"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] follow_traffic Eval Loss: 0.4797, Accuracy: 0.9495\n",
      "\n",
      "--- Pretraining of road_clear ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 1.20516419"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] road_clear Eval Loss: 0.9896, Accuracy: 0.9043\n",
      "\n",
      "--- Pretraining of traffic_lights ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.35032409"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] traffic_lights Eval Loss: 0.7351, Accuracy: 0.9314\n",
      "\n",
      "--- Pretraining of traffic_signs ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.51919723"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] traffic_signs Eval Loss: 0.6463, Accuracy: 0.9500\n",
      "\n",
      "--- Pretraining of cars ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.38778484"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] cars Eval Loss: 0.4130, Accuracy: 0.9735\n",
      "\n",
      "--- Pretraining of pedestrians ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.11471567"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] pedestrians Eval Loss: 0.2503, Accuracy: 0.9744\n",
      "\n",
      "--- Pretraining of riders ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.66693449"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] riders Eval Loss: 0.9948, Accuracy: 0.9234\n",
      "\n",
      "--- Pretraining of others ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.26868469"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] others Eval Loss: 0.2457, Accuracy: 0.9735\n",
      "\n",
      "--- Pretraining of no_lane_left ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.00167267"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] no_lane_left Eval Loss: 0.4604, Accuracy: 0.9726\n",
      "\n",
      "--- Pretraining of obstacle_left_lane ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.25661656"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] obstacle_left_lane Eval Loss: 0.6085, Accuracy: 0.9469\n",
      "\n",
      "--- Pretraining of solid_left_line ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.59500794"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] solid_left_line Eval Loss: 0.2787, Accuracy: 0.9733\n",
      "\n",
      "--- Pretraining of on_right_turn_lane ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.00160225"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] on_right_turn_lane Eval Loss: 0.3944, Accuracy: 0.9732\n",
      "\n",
      "--- Pretraining of traffic_light_right ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.30523866"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] traffic_light_right Eval Loss: 0.5475, Accuracy: 0.9469\n",
      "\n",
      "--- Pretraining of front_car_right ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.20850615"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] front_car_right Eval Loss: 0.2594, Accuracy: 0.9732\n",
      "\n",
      "--- Pretraining of no_lane_right ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.63265216"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] no_lane_right Eval Loss: 1.0157, Accuracy: 0.9300\n",
      "\n",
      "--- Pretraining of obstacle_right_lane ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 1.30599546"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] obstacle_right_lane Eval Loss: 0.9761, Accuracy: 0.9072\n",
      "\n",
      "--- Pretraining of solid_right_line ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.16879353"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] solid_right_line Eval Loss: 0.6286, Accuracy: 0.9526\n",
      "\n",
      "--- Pretraining of on_left_turn_lane ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 1.40134144"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] on_left_turn_lane Eval Loss: 0.9723, Accuracy: 0.9137\n",
      "\n",
      "--- Pretraining of traffic_light_left ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.49830231"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] traffic_light_left Eval Loss: 0.8396, Accuracy: 0.9058\n",
      "\n",
      "--- Pretraining of front_car_left ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:32 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.25576407"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] front_car_left Eval Loss: 0.3976, Accuracy: 0.9633\n"
     ]
    }
   ],
   "source": [
    "eval_concepts = ['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "                'on_right_turn_lane', 'traffic_light_right', 'front_car_right', \n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "                'on_left_turn_lane', 'traffic_light_left', 'front_car_left']\n",
    "\n",
    "pre_train(model, supervised_dataloaders, args, \n",
    "    eval_concepts=eval_concepts, seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e14769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: MnistDPL, \n",
    "        _loss: ADDMNIST_DPL,\n",
    "        save_path: str,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        args: Namespace,\n",
    "        eval_concepts: list = None,\n",
    "        seed: int = 0,\n",
    "    ) -> float:\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # early stopping\n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # scheduler & warmup (not used) for main model\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:\n",
    "        w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.to(model.device)\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # & Training start\n",
    "    for epoch in range(args.n_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.n_epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # * Unsupervised Training\n",
    "        print(\"Unsupervised training phase\")\n",
    "        ys, y_true, cs, cs_true, batch = None, None, None, None, 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            \n",
    "            # ------------------ original embneddings\n",
    "            images_embeddings = torch.stack(batch['embeddings']).to(model.device)\n",
    "            attr_labels = torch.stack(batch['attr_labels']).to(model.device)\n",
    "            class_labels = torch.stack(batch['class_labels'])[:,:-1].to(model.device)\n",
    "            # ------------------ my extracted features\n",
    "            images_embeddings_raw = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "            detected_rois = batch['rois']\n",
    "            detected_rois_feats = batch['roi_feats']\n",
    "            detection_labels = batch['detection_labels']\n",
    "            detection_scores = batch['detection_scores']\n",
    "            assert_inputs(images_embeddings, attr_labels, class_labels,\n",
    "                   detected_rois_feats, detected_rois, detection_labels,\n",
    "                   detection_scores, images_embeddings_raw)\n",
    "\n",
    "            out_dict = model(images_embeddings_raw)\n",
    "            out_dict.update({\"LABELS\": class_labels, \"CONCEPTS\": attr_labels})\n",
    "            \n",
    "            model.opt.zero_grad()\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "            \n",
    "        # ^ Evaluation phase\n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        my_metrics = evaluate_metrics(\n",
    "                model=model, \n",
    "                loader=val_loader, \n",
    "                args=args,\n",
    "                eval_concepts=eval_concepts)\n",
    "        loss = my_metrics[0]\n",
    "        cacc = my_metrics[1]\n",
    "        yacc = my_metrics[2]\n",
    "        f1_y = my_metrics[3]\n",
    "       \n",
    "        # update at end of the epoch\n",
    "        if epoch < args.warmup_steps:   w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1_y)\n",
    "        \n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "\n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e6feb",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6353f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training model with seed 0\n",
      "Chosen device: cuda\n",
      "Saving model in folder:  ../NEW-outputs/bddoia/baseline/dpl/baseline-disj-PRE/dpl_0.pth\n",
      "\n",
      "--- Start of Training ---\n",
      "\n",
      "Epoch 1/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:33 ] epoch 0: |██████████████████████████████████████████████████| loss: 4.75548792"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.26358203093211   ACC Y 70.22569444444444 F1 Y 56.272930960117975\n",
      "Saving...\n",
      "Saved best model with CACC score: 84.26358203093211\n",
      "Epoch 2/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:33 ] epoch 1: |██████████████████████████████████████████████████| loss: 4.77326536"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.58450006114111   ACC Y 63.07705965909091 F1 Y 60.46534366922825\n",
      "Saving...\n",
      "Saved best model with CACC score: 84.58450006114111\n",
      "Epoch 3/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:34 ] epoch 2: |██████████████████████████████████████████████████| loss: 4.73955631"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.31412511401706   ACC Y 61.945628156565654 F1 Y 59.56880897456301\n",
      "Epoch 4/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:35 ] epoch 3: |██████████████████████████████████████████████████| loss: 4.72808266"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.55387353897095   ACC Y 68.2360716540404 F1 Y 60.59219222652515\n",
      "Epoch 5/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:35 ] epoch 4: |██████████████████████████████████████████████████| loss: 4.74091482"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.75322524706523   ACC Y 70.09548611111111 F1 Y 60.198208391633166\n",
      "Saving...\n",
      "Saved best model with CACC score: 84.75322524706523\n",
      "Epoch 6/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:36 ] epoch 5: |██████████████████████████████████████████████████| loss: 4.73691702"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.62301790714264   ACC Y 67.42128314393939 F1 Y 61.309797683939095\n",
      "Epoch 7/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:36 ] epoch 6: |██████████████████████████████████████████████████| loss: 4.75249863"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.74477032820384   ACC Y 70.63012941919192 F1 Y 60.562428300076554\n",
      "Epoch 8/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:37 ] epoch 7: |██████████████████████████████████████████████████| loss: 4.71096706"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.52700542079077   ACC Y 68.01215277777777 F1 Y 60.71332717960325\n",
      "Epoch 9/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:38 ] epoch 8: |██████████████████████████████████████████████████| loss: 4.73797703"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.69742238521576   ACC Y 74.47620738636364 F1 Y 52.39877291409552\n",
      "Epoch 10/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:38 ] epoch 9: |██████████████████████████████████████████████████| loss: 4.72500801"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.67487461037106   ACC Y 73.57855902777777 F1 Y 57.120519359803566\n",
      "Epoch 11/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:39 ] epoch 10: |██████████████████████████████████████████████████| loss: 4.72138739"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.54748491446178   ACC Y 74.89938446969697 F1 Y 45.741515916244566\n",
      "Epoch 12/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:39 ] epoch 11: |██████████████████████████████████████████████████| loss: 4.72024727"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.03087581528558   ACC Y 72.60396938131312 F1 Y 50.504837374827076\n",
      "Epoch 13/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:40 ] epoch 12: |██████████████████████████████████████████████████| loss: 4.71095896"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 83.11744762791528   ACC Y 72.54872948232324 F1 Y 50.52846288802723\n",
      "Epoch 14/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:41 ] epoch 13: |██████████████████████████████████████████████████| loss: 4.67393446"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.67458875974019   ACC Y 70.48413825757576 F1 Y 51.06568470190591\n",
      "Epoch 15/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:41 ] epoch 14: |██████████████████████████████████████████████████| loss: 4.66131021"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.36795167128246   ACC Y 71.12334280303031 F1 Y 50.1032015445642\n",
      "Epoch 16/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:42 ] epoch 15: |██████████████████████████████████████████████████| loss: 4.72431707"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.96079267395868   ACC Y 70.53740530303031 F1 Y 50.70693699543647\n",
      "Epoch 17/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:42 ] epoch 16: |██████████████████████████████████████████████████| loss: 4.75232416"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.46870599852667   ACC Y 72.17388731060606 F1 Y 49.9049892069551\n",
      "Epoch 18/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:43 ] epoch 17: |██████████████████████████████████████████████████| loss: 4.72701745"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.10133446587457   ACC Y 71.70434816919192 F1 Y 51.194174209344744\n",
      "Epoch 19/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:44 ] epoch 18: |██████████████████████████████████████████████████| loss: 4.65083456"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.26874536938138   ACC Y 70.55220170454545 F1 Y 50.44866711026231\n",
      "Epoch 20/40\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:44 ] epoch 19: |██████████████████████████████████████████████████| loss: 4.71089554"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.72118634647794   ACC Y 72.69077493686868 F1 Y 50.335723320339916\n",
      "Early stopping triggered after 20 epochs.\n",
      "\n",
      "--- End of Training ---\n",
      "\n",
      "*** Finished training model with seed 0 and best CACC score 84.75322524706523\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "eval_concepts = ['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "                'on_right_turn_lane', 'traffic_light_right', 'front_car_right', \n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "                'on_left_turn_lane', 'traffic_light_left', 'front_car_left']\n",
    "\n",
    "best_cacc = train(\n",
    "        model=model,\n",
    "        train_loader=unsup_train_loader,\n",
    "        val_loader=unsup_val_loader,\n",
    "        save_path=save_folder,\n",
    "        _loss=loss,\n",
    "        args=args,\n",
    "        eval_concepts=eval_concepts,\n",
    "        seed=args.seed,\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "print(f\"*** Finished training model with seed {args.seed} and best CACC score {best_cacc}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb08277",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a2e9e",
   "metadata": {},
   "source": [
    "## Evaluation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "892851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_my_model(model: MnistDPL, \n",
    "        save_path: str, \n",
    "        test_loader: DataLoader,\n",
    "        eval_concepts,\n",
    "    ):\n",
    "    \n",
    "    my_metrics = evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,)\n",
    "    \n",
    "    loss = my_metrics[0]\n",
    "    cacc = my_metrics[1]\n",
    "    yacc = my_metrics[2]\n",
    "    f1_y = my_metrics[3]\n",
    "    f1_micro = my_metrics[4]\n",
    "    f1_weight = my_metrics[5]\n",
    "    f1_bin = my_metrics[6]\n",
    "\n",
    "    metrics_log_path = save_path.replace(\".pth\", \"_metrics.log\")\n",
    "    \n",
    "    all_concepts = [ 'Green Traffic Light', 'Follow Traffic', 'Road Is Clear',\n",
    "        'Red Traffic Light', 'Traffic Sign', 'Obstacle Car', 'Obstacle Pedestrian', 'Obstacle Rider', 'Obstacle Others',\n",
    "        'No Lane On The Left',  'Obstacle On The Left Lane',  'Solid Left Line',\n",
    "                'On The Right Turn Lane', 'Traffic Light Allows Right', 'Front Car Turning Right', \n",
    "        'No Lane On The Right', 'Obstacle On The Right Lane', 'Solid Right Line',\n",
    "                'On The Left Turn Lane',  'Traffic Light Allows Left',  'Front Car Turning Left' \n",
    "    ]\n",
    "    aggregated_metrics = [\n",
    "            'F1 - Binary', 'F1 - Macro', 'F1 - Micro', 'F1 - Weighted',\n",
    "            'Precision - Binary', 'Precision - Macro', 'Precision - Micro', 'Precision - Weighted',\n",
    "            'Recall - Binary', 'Recall - Macro', 'Recall - Micro', 'Recall - Weighted',\n",
    "            'Balanced Accuracy'\n",
    "    ]\n",
    "\n",
    "    sums = [0.0] * len(aggregated_metrics)\n",
    "    num_concepts = len(all_concepts)\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ACC C: {cacc}, ACC Y: {yacc}\\n\\n\")\n",
    "        log_file.write(f\"F1 Y - Macro: {f1_y}, F1 Y - Micro: {f1_micro}, F1 Y - Weighted: {f1_weight}, F1 Y - Binary: {f1_bin} \\n\\n\")\n",
    "\n",
    "        def write_metrics(class_name, offset):\n",
    "            print(f\"Metrics for {class_name}:\")\n",
    "            log_file.write(f\"{class_name.upper()}\\n\")\n",
    "            for idx, metric_name in enumerate(aggregated_metrics):\n",
    "                value = my_metrics[offset + idx]\n",
    "                sums[idx] += value\n",
    "                log_file.write(f\"  {metric_name:<18} {value:.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "        i = 7\n",
    "        for concept in all_concepts:\n",
    "            write_metrics(concept, i)\n",
    "            i += len(aggregated_metrics)\n",
    "\n",
    "        log_file.write(\"MEAN ACROSS ALL CONCEPTS\\n\")\n",
    "        for idx, metric_name in enumerate(aggregated_metrics):\n",
    "            mean_value = sums[idx] / num_concepts\n",
    "            log_file.write(f\"  {metric_name:<18} {mean_value:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "    assert len(my_metrics) == 7 + len(all_concepts) * len(aggregated_metrics), \\\n",
    "        f\"Expected {7 + len(all_concepts) * len(aggregated_metrics)} metrics, but got {len(my_metrics)}\"\n",
    "    y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "        evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,\n",
    "                    last=True\n",
    "            )\n",
    "    )\n",
    "    y_labels = [\"stop\", \"forward\", \"left\", \"right\"]\n",
    "    concept_labels = [\n",
    "        \"green_light\",      \n",
    "        \"follow\",           \n",
    "        \"road_clear\",       \n",
    "        \"red_light\",        \n",
    "        \"traffic_sign\",     \n",
    "        \"car\",              \n",
    "        \"person\",           \n",
    "        \"rider\",            \n",
    "        \"other_obstacle\",   \n",
    "        \"left_lane\",\n",
    "        \"left_green_light\",\n",
    "        \"left_follow\",\n",
    "        \"no_left_lane\",\n",
    "        \"left_obstacle\",\n",
    "        \"letf_solid_line\",\n",
    "        \"right_lane\",\n",
    "        \"right_green_light\",\n",
    "        \"right_follow\",\n",
    "        \"no_right_lane\",\n",
    "        \"right_obstacle\",\n",
    "        \"right_solid_line\",\n",
    "    ]\n",
    "\n",
    "    plot_multilabel_confusion_matrix(y_true, y_pred, y_labels, \"Labels\", save_path=save_path)\n",
    "    cfs = plot_actions_confusion_matrix(c_true, c_pred, \"Concepts\", save_path=save_path)\n",
    "    cf = plot_multilabel_confusion_matrix(c_true, c_pred, concept_labels, \"Concepts\", save_path=save_path)\n",
    "    print(\"Concept collapse\", 1 - compute_coverage(cf))\n",
    "\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        for key, value in cfs.items():\n",
    "            log_file.write(f\"Concept collapse: {key}, {1 - compute_coverage(value):.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "    fprint(\"\\n--- End of Evaluation ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87947313",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2806065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'bddoiadpldisj', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "Metrics for Green Traffic Light:\n",
      "Metrics for Follow Traffic:\n",
      "Metrics for Road Is Clear:\n",
      "Metrics for Red Traffic Light:\n",
      "Metrics for Traffic Sign:\n",
      "Metrics for Obstacle Car:\n",
      "Metrics for Obstacle Pedestrian:\n",
      "Metrics for Obstacle Rider:\n",
      "Metrics for Obstacle Others:\n",
      "Metrics for No Lane On The Left:\n",
      "Metrics for Obstacle On The Left Lane:\n",
      "Metrics for Solid Left Line:\n",
      "Metrics for On The Right Turn Lane:\n",
      "Metrics for Traffic Light Allows Right:\n",
      "Metrics for Front Car Turning Right:\n",
      "Metrics for No Lane On The Right:\n",
      "Metrics for Obstacle On The Right Lane:\n",
      "Metrics for Solid Right Line:\n",
      "Metrics for On The Left Turn Lane:\n",
      "Metrics for Traffic Light Allows Left:\n",
      "Metrics for Front Car Turning Left:\n",
      "Concept collapse 0.993103448275862\n",
      "\n",
      "--- End of Evaluation ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model object\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "\n",
    "# Load the model state dictionary into the model object\n",
    "model_state_dict = torch.load(save_folder)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "evaluate_my_model(model, save_folder, unsup_test_loader, eval_concepts=eval_concepts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
