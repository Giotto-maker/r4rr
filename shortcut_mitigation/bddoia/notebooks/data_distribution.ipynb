{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db624e1e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee0a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a97cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models import get_model\n",
    "from datasets import get_dataset\n",
    "from collections import defaultdict\n",
    "from baseline_modules.arguments import args_dpl \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e79c9",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39a91b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "Available models: ['promnistltn', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'xordpl', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'mnistsl', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "<datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f2f614e9f70>\n",
      "Using Dataset:  <datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f2f614e9f70>\n",
      "Using backbone:  BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=21, bias=True)\n",
      ")\n",
      "Using Model:  BoiaDPL(\n",
      "  (encoder): BOIAConceptizer(\n",
      "    (enc1): Linear(in_features=2048, out_features=21, bias=True)\n",
      "  )\n",
      ")\n",
      "Using Loss:  SDDOIA_DPL()\n",
      "Dataset sizes - train: 16080, val: 2264, test: 4568\n",
      "Loaded datasets in 0.13s\n"
     ]
    }
   ],
   "source": [
    "args = args_dpl\n",
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b4d4",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e14769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples processed: 16080\n",
      "Attribute Label Statistics (number of 1s and percentage over total):\n",
      "Concept 0: 5478 (34.07%)\n",
      "Concept 1: 2448 (15.22%)\n",
      "Concept 2: 3422 (21.28%)\n",
      "Concept 3: 3766 (23.42%)\n",
      "Concept 4: 1095 (6.81%)\n",
      "Concept 5: 171 (1.06%)\n",
      "Concept 6: 116 (0.72%)\n",
      "Concept 7: 3703 (23.03%)\n",
      "Concept 8: 329 (2.05%)\n",
      "Concept 9: 100 (0.62%)\n",
      "Concept 10: 464 (2.89%)\n",
      "Concept 11: 220 (1.37%)\n",
      "Concept 12: 115 (0.72%)\n",
      "Concept 13: 629 (3.91%)\n",
      "Concept 14: 270 (1.68%)\n",
      "Concept 15: 3182 (19.79%)\n",
      "Concept 16: 3177 (19.76%)\n",
      "Concept 17: 2587 (16.09%)\n",
      "Concept 18: 4303 (26.76%)\n",
      "Concept 19: 2796 (17.39%)\n",
      "Concept 20: 1563 (9.72%)\n"
     ]
    }
   ],
   "source": [
    "attribute_counts = torch.zeros(21, dtype=torch.long).to('cuda:0')  # Assuming CUDA\n",
    "total_samples = 0\n",
    "\n",
    "for i, batch in enumerate(unsup_train_loader):\n",
    "    attr_labels = torch.stack(batch['attr_labels']).to(attribute_counts.device)\n",
    "    # Accumulate 1s per column\n",
    "    attribute_counts += attr_labels.sum(dim=0).long()\n",
    "    # Accumulate total samples\n",
    "    total_samples += attr_labels.size(0)\n",
    "\n",
    "attribute_stats = {\n",
    "    f'Concept {i}': {\n",
    "        'count_1s': int(count.item()),\n",
    "        'percentage': round(100.0 * count.item() / total_samples, 2)\n",
    "    }\n",
    "    for i, count in enumerate(attribute_counts)\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal samples processed: {total_samples}\")\n",
    "print(\"Attribute Label Statistics (number of 1s and percentage over total):\")\n",
    "for attr, stats in attribute_stats.items():\n",
    "    print(f\"{attr}: {stats['count_1s']} ({stats['percentage']}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
