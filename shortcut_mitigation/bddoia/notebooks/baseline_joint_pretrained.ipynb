{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db624e1e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import setproctitle, socket, uuid\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from datasets import get_dataset\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "from utils import fprint\n",
    "from utils.status import progress_bar\n",
    "from utils.metrics import evaluate_metrics\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.checkpoint import save_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from baseline_modules.arguments import args_dpl \n",
    "from backbones.bddoia_protonet import ProtoNetConv1D, PrototypicalLoss\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_helpers import assert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b846",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0680d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args_dpl\n",
    "args.seed = 2048\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"bddoia\" \n",
    "save_model_name = 'dpl'\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"NEW-outputs\", \n",
    "    save_folder, \n",
    "    \"baseline\", \n",
    "    save_model_name,\n",
    "    f\"PRE-baseline\"\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "print(f\"Save paths: {str(save_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511d01",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c1be3",
   "metadata": {},
   "source": [
    "## Test Set Evaluation (alternative to full fledged notebook eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * helper function for 'plot_multilabel_confusion_matrix'\n",
    "def convert_to_categories(elements):\n",
    "    # Convert vector of 0s and 1s to a single binary representation along the first dimension\n",
    "    binary_rep = np.apply_along_axis(\n",
    "        lambda x: \"\".join(map(str, x)), axis=1, arr=elements\n",
    "    )\n",
    "    return np.array([int(x, 2) for x in binary_rep])\n",
    "\n",
    "\n",
    "# * BBDOIA custom confusion matrix for concepts\n",
    "def plot_multilabel_confusion_matrix(\n",
    "    y_true, y_pred, class_names, title, save_path=None\n",
    "):\n",
    "    y_true_categories = convert_to_categories(y_true.astype(int))\n",
    "    y_pred_categories = convert_to_categories(y_pred.astype(int))\n",
    "\n",
    "    to_rtn_cm = confusion_matrix(y_true_categories, y_pred_categories)\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    num_classes = len(class_names)\n",
    "    num_rows = (num_classes + 4) // 5  # Calculate the number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_rows))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.subplot(num_rows, 5, i + 1)  # Set the subplot position\n",
    "        plt.imshow(cm[i], interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Class: {class_names[i]}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "        plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "        fmt = \".0f\"\n",
    "        thresh = cm[i].max() / 2.0\n",
    "        for j in range(cm[i].shape[0]):\n",
    "            for k in range(cm[i].shape[1]):\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    format(cm[i][j, k], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i][j, k] > thresh else \"black\",\n",
    "                )\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_total.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return to_rtn_cm\n",
    "\n",
    "\n",
    "# * Concept collapse (Soft)\n",
    "def compute_coverage(confusion_matrix):\n",
    "    \"\"\"Compute the coverage of a confusion matrix.\n",
    "\n",
    "    Essentially this metric is\n",
    "    \"\"\"\n",
    "\n",
    "    max_values = np.max(confusion_matrix, axis=0)\n",
    "    clipped_values = np.clip(max_values, 0, 1)\n",
    "\n",
    "    # Redefinition of soft coverage\n",
    "    coverage = np.sum(clipped_values) / len(clipped_values)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# * BDDOIA custom confusion matrix for actions\n",
    "def plot_actions_confusion_matrix(c_true, c_pred, title, save_path=None):\n",
    "\n",
    "    # Define scenarios and corresponding labels\n",
    "    my_scenarios = {\n",
    "        \"forward\": [slice(0, 3), slice(0, 3)],  \n",
    "        \"stop\": [slice(3, 9), slice(3, 9)],\n",
    "        \"left\": [slice(9, 11), slice(18,20)],\n",
    "        \"right\": [slice(12, 17), slice(12,17)],\n",
    "    }\n",
    "\n",
    "    to_rtn = {}\n",
    "\n",
    "    # Plot confusion matrix for each scenario\n",
    "    for scenario, indices in my_scenarios.items():\n",
    "\n",
    "        g_true = convert_to_categories(c_true[:, indices[0]].astype(int))\n",
    "        c_pred_scenario = convert_to_categories(c_pred[:, indices[1]].astype(int))\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(g_true, c_pred_scenario)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{title} - {scenario}\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        n_classes = c_true[:, indices[0]].shape[1]\n",
    "\n",
    "        tick_marks = np.arange(2**n_classes)\n",
    "        plt.xticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "        plt.yticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or show plot\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{scenario}.png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        to_rtn.update({scenario: cm})\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return to_rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e79c9",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f8268",
   "metadata": {},
   "source": [
    "# PROTOTYPES CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20e70",
   "metadata": {},
   "source": [
    "## DATASET & BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fee336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert embeddings.shape[0] == labels.shape[0]\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01ef6c",
   "metadata": {},
   "source": [
    "## Build positive annotation set for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "target_per_class = 6\n",
    "debug = True\n",
    "\n",
    "# Loop over dataset until we collect target_per_class for each class\n",
    "for batch_idx, batch in enumerate(unsup_train_loader):\n",
    "    raw_embs = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "    attrs = torch.stack(batch['attr_labels']).to(model.device)  # shape [B,21]\n",
    "    batch_size = attrs.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        attr_vector = attrs[b].clone().cpu()  # clone to avoid in-place issues\n",
    "        for cls in torch.nonzero(attr_vector).flatten().tolist():\n",
    "            if len(pos_examples[cls]) >= target_per_class:\n",
    "                continue\n",
    "            example = {\n",
    "                'source_id': (batch_idx, b),\n",
    "                'images_embeddings_raw': raw_embs[b].detach().cpu().clone(),\n",
    "                'attr_labels': attr_vector,\n",
    "                'is_positive': True\n",
    "            }\n",
    "            if debug:\n",
    "                for key, value in example.items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        print(f\"{key}: {value.shape}\")\n",
    "                    elif isinstance(value, list) and len(value) and torch.is_tensor(value[0]):\n",
    "                        print(f\"{key}: list of {len(value)} tensors, first shape: {value[0].shape}\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {type(value)}\")\n",
    "                debug = False\n",
    "            pos_examples[cls].append(example)\n",
    "\n",
    "    # Check if all classes reached target\n",
    "    if all(len(pos_examples[c]) >= target_per_class for c in range(21)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddabe2f",
   "metadata": {},
   "source": [
    "## Augment positive sets while building negative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 1 and ex['source_id'] not in seen_ids:\n",
    "                new_ex = ex.copy()\n",
    "                new_ex['is_positive'] = True\n",
    "                pos_examples[cls].append(new_ex)\n",
    "                seen_ids.add(ex['source_id'])\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids_pos = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 0 and ex['source_id'] not in seen_ids_pos:\n",
    "                neg_ex = ex.copy()\n",
    "                neg_ex['is_positive'] = False\n",
    "                neg_examples[cls].append(neg_ex)\n",
    "\n",
    "for cls in range(21):\n",
    "    assert not set(ex['source_id'] for ex in neg_examples[cls]) & set(ex['source_id'] for ex in pos_examples[cls]), \\\n",
    "        f\"Overlap in pos/neg for class {cls}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21dcb6",
   "metadata": {},
   "source": [
    "## Construct embeddings and labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34199a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_per_class = {}\n",
    "for cls in range(21):\n",
    "    examples = pos_examples[cls] + neg_examples[cls]\n",
    "    emb_list, label_list = [], []\n",
    "    for ex in examples:\n",
    "        emb_list.append(ex['images_embeddings_raw'].unsqueeze(0))\n",
    "        label_list.append(ex['attr_labels'])\n",
    "    embeddings_tensor = torch.stack(emb_list).to(model.device)  # [N,1,2048]\n",
    "    labels_tensor = torch.stack(label_list)\n",
    "    dataset_per_class[cls] = {'embeddings': embeddings_tensor.squeeze(1), 'labels': labels_tensor}\n",
    "        \n",
    "for cls in range(21):\n",
    "    print(f\"Class {cls}: embeddings shape = {dataset_per_class[cls]['embeddings'].shape}, labels shape = {dataset_per_class[cls]['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat([per_class['embeddings'] for per_class in dataset_per_class.values()], dim=0)\n",
    "all_labels = torch.cat([per_class['labels'] for per_class in dataset_per_class.values()], dim=0)\n",
    "all_embeddings = all_embeddings.cpu()\n",
    "all_labels     = all_labels.cpu()\n",
    "dataset = ProtoDataset(all_embeddings, all_labels)\n",
    "nn_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,        \n",
    "    pin_memory=True       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b138d",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b4d4",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8206e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(model, train_loader, args, seed: int = 0):\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # start optimizer\n",
    "    enc_opt = torch.optim.Adam(model.encoder.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.encoder.to(model.device)\n",
    "    \n",
    "    for epoch in range(args.n_epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch_embeds, batch_labels = batch\n",
    "            batch_embeds = batch_embeds.to(model.device)\n",
    "            batch_labels = batch_labels.to(model.device)\n",
    "\n",
    "            enc_opt.zero_grad()\n",
    "            preds = model.encoder(batch_embeds)\n",
    "            assert preds.shape == (batch_embeds.shape[0], 21), f\"Expected shape ({batch_embeds.shape[0]}, 21), got {preds.shape}\"\n",
    "            loss = F.binary_cross_entropy(preds, batch_labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "            enc_opt.step()\n",
    "\n",
    "            progress_bar(i, len(train_loader), epoch, loss.item())\n",
    "\n",
    "\n",
    "pre_train(model, nn_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e14769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: MnistDPL, \n",
    "        _loss: ADDMNIST_DPL,\n",
    "        save_path: str,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        args: Namespace,\n",
    "        eval_concepts: list = None,\n",
    "        seed: int = 0,\n",
    "    ) -> float:\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # early stopping\n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # scheduler & warmup (not used) for main model\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:\n",
    "        w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.to(model.device)\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # & Training start\n",
    "    for epoch in range(args.n_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.n_epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # * Unsupervised Training\n",
    "        print(\"Unsupervised training phase\")\n",
    "        ys, y_true, cs, cs_true, batch = None, None, None, None, 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            \n",
    "            # ------------------ original embneddings\n",
    "            images_embeddings = torch.stack(batch['embeddings']).to(model.device)\n",
    "            attr_labels = torch.stack(batch['attr_labels']).to(model.device)\n",
    "            class_labels = torch.stack(batch['class_labels'])[:,:-1].to(model.device)\n",
    "            # ------------------ my extracted features\n",
    "            images_embeddings_raw = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "            detected_rois = batch['rois']\n",
    "            detected_rois_feats = batch['roi_feats']\n",
    "            detection_labels = batch['detection_labels']\n",
    "            detection_scores = batch['detection_scores']\n",
    "            assert_inputs(images_embeddings, attr_labels, class_labels,\n",
    "                   detected_rois_feats, detected_rois, detection_labels,\n",
    "                   detection_scores, images_embeddings_raw)\n",
    "\n",
    "            out_dict = model(images_embeddings_raw)\n",
    "            out_dict.update({\"LABELS\": class_labels, \"CONCEPTS\": attr_labels})\n",
    "            \n",
    "            model.opt.zero_grad()\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "            \n",
    "        # ^ Evaluation phase\n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        my_metrics = evaluate_metrics(\n",
    "                model=model, \n",
    "                loader=val_loader, \n",
    "                args=args,\n",
    "                eval_concepts=eval_concepts)\n",
    "        loss = my_metrics[0]\n",
    "        cacc = my_metrics[1]\n",
    "        yacc = my_metrics[2]\n",
    "        f1_y = my_metrics[3]\n",
    "       \n",
    "        # update at end of the epoch\n",
    "        if epoch < args.warmup_steps:   w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1_y)\n",
    "        \n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "\n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e6feb",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "eval_concepts = ['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "                'on_right_turn_lane', 'traffic_light_right', 'front_car_right', \n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "                'on_left_turn_lane', 'traffic_light_left', 'front_car_left']\n",
    "\n",
    "best_cacc = train(\n",
    "        model=model,\n",
    "        train_loader=unsup_train_loader,\n",
    "        val_loader=unsup_val_loader,\n",
    "        save_path=save_folder,\n",
    "        _loss=loss,\n",
    "        args=args,\n",
    "        eval_concepts=eval_concepts,\n",
    "        seed=args.seed,\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "print(f\"*** Finished training model with seed {args.seed} and best CACC score {best_cacc}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb08277",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a2e9e",
   "metadata": {},
   "source": [
    "## Evaluation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_my_model(model: MnistDPL, \n",
    "        save_path: str, \n",
    "        test_loader: DataLoader,\n",
    "        eval_concepts,\n",
    "    ):\n",
    "    \n",
    "    my_metrics = evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,)\n",
    "    \n",
    "    loss = my_metrics[0]\n",
    "    cacc = my_metrics[1]\n",
    "    yacc = my_metrics[2]\n",
    "    f1_y = my_metrics[3]\n",
    "    f1_micro = my_metrics[4]\n",
    "    f1_weight = my_metrics[5]\n",
    "    f1_bin = my_metrics[6]\n",
    "\n",
    "    metrics_log_path = save_path.replace(\".pth\", \"_metrics.log\")\n",
    "    \n",
    "    all_concepts = [ 'Green Traffic Light', 'Follow Traffic', 'Road Is Clear',\n",
    "        'Red Traffic Light', 'Traffic Sign', 'Obstacle Car', 'Obstacle Pedestrian', 'Obstacle Rider', 'Obstacle Others',\n",
    "        'No Lane On The Left',  'Obstacle On The Left Lane',  'Solid Left Line',\n",
    "                'On The Right Turn Lane', 'Traffic Light Allows Right', 'Front Car Turning Right', \n",
    "        'No Lane On The Right', 'Obstacle On The Right Lane', 'Solid Right Line',\n",
    "                'On The Left Turn Lane',  'Traffic Light Allows Left',  'Front Car Turning Left' \n",
    "    ]\n",
    "    aggregated_metrics = [\n",
    "            'F1 - Binary', 'F1 - Macro', 'F1 - Micro', 'F1 - Weighted',\n",
    "            'Precision - Binary', 'Precision - Macro', 'Precision - Micro', 'Precision - Weighted',\n",
    "            'Recall - Binary', 'Recall - Macro', 'Recall - Micro', 'Recall - Weighted',\n",
    "            'Balanced Accuracy'\n",
    "    ]\n",
    "\n",
    "    sums = [0.0] * len(aggregated_metrics)\n",
    "    num_concepts = len(all_concepts)\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ACC C: {cacc}, ACC Y: {yacc}\\n\\n\")\n",
    "        log_file.write(f\"F1 Y - Macro: {f1_y}, F1 Y - Micro: {f1_micro}, F1 Y - Weighted: {f1_weight}, F1 Y - Binary: {f1_bin} \\n\\n\")\n",
    "\n",
    "        def write_metrics(class_name, offset):\n",
    "            print(f\"Metrics for {class_name}:\")\n",
    "            log_file.write(f\"{class_name.upper()}\\n\")\n",
    "            for idx, metric_name in enumerate(aggregated_metrics):\n",
    "                value = my_metrics[offset + idx]\n",
    "                sums[idx] += value\n",
    "                log_file.write(f\"  {metric_name:<18} {value:.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "        i = 7\n",
    "        for concept in all_concepts:\n",
    "            write_metrics(concept, i)\n",
    "            i += len(aggregated_metrics)\n",
    "\n",
    "        log_file.write(\"MEAN ACROSS ALL CONCEPTS\\n\")\n",
    "        for idx, metric_name in enumerate(aggregated_metrics):\n",
    "            mean_value = sums[idx] / num_concepts\n",
    "            log_file.write(f\"  {metric_name:<18} {mean_value:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "    assert len(my_metrics) == 7 + len(all_concepts) * len(aggregated_metrics), \\\n",
    "        f\"Expected {7 + len(all_concepts) * len(aggregated_metrics)} metrics, but got {len(my_metrics)}\"\n",
    "    y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "        evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,\n",
    "                    last=True\n",
    "            )\n",
    "    )\n",
    "    y_labels = [\"stop\", \"forward\", \"left\", \"right\"]\n",
    "    concept_labels = [\n",
    "        \"green_light\",      \n",
    "        \"follow\",           \n",
    "        \"road_clear\",       \n",
    "        \"red_light\",        \n",
    "        \"traffic_sign\",     \n",
    "        \"car\",              \n",
    "        \"person\",           \n",
    "        \"rider\",            \n",
    "        \"other_obstacle\",   \n",
    "        \"left_lane\",\n",
    "        \"left_green_light\",\n",
    "        \"left_follow\",\n",
    "        \"no_left_lane\",\n",
    "        \"left_obstacle\",\n",
    "        \"letf_solid_line\",\n",
    "        \"right_lane\",\n",
    "        \"right_green_light\",\n",
    "        \"right_follow\",\n",
    "        \"no_right_lane\",\n",
    "        \"right_obstacle\",\n",
    "        \"right_solid_line\",\n",
    "    ]\n",
    "\n",
    "    plot_multilabel_confusion_matrix(y_true, y_pred, y_labels, \"Labels\", save_path=save_path)\n",
    "    cfs = plot_actions_confusion_matrix(c_true, c_pred, \"Concepts\", save_path=save_path)\n",
    "    cf = plot_multilabel_confusion_matrix(c_true, c_pred, concept_labels, \"Concepts\", save_path=save_path)\n",
    "    print(\"Concept collapse\", 1 - compute_coverage(cf))\n",
    "\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        for key, value in cfs.items():\n",
    "            log_file.write(f\"Concept collapse: {key}, {1 - compute_coverage(value):.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "    fprint(\"\\n--- End of Evaluation ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87947313",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model object\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "\n",
    "# Load the model state dictionary into the model object\n",
    "model_state_dict = torch.load(save_folder)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "evaluate_my_model(model, save_folder, unsup_test_loader, eval_concepts=eval_concepts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
