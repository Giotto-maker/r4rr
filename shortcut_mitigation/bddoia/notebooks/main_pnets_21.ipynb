{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668000b2",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a64ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f212d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import setproctitle, socket, uuid\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from datasets import get_dataset\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "from utils import fprint\n",
    "from utils.status import progress_bar\n",
    "from utils.metrics import evaluate_metrics\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.checkpoint import save_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from backbones.bddoia_protonet import PrototypicalLoss\n",
    "from protonet_bddoia_modules.data_modules.proto_data import build_prototypical_dataloaders  # TODO: use to refactor\n",
    "from protonet_bddoia_modules.arguments import args_dpl \n",
    "from baseline_modules.supervision_modules.build_sup_set_joint import get_augmented_train_loader  # TODO: use to refactor\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_helpers import (\n",
    "    assert_inputs,\n",
    "    get_random_classes,\n",
    ")\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_functions import (\n",
    "    train_my_prototypical_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b55fe4",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fbfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "UNS_PERCENTAGE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabeedcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1\n",
      "Save paths: ['../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA']\n"
     ]
    }
   ],
   "source": [
    "args = args_dpl\n",
    "args.seed = SEED\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"bddoia\" \n",
    "save_model_name = 'dpl'\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"notebook-outputs\", \n",
    "    save_folder, \n",
    "    \"my_models\", \n",
    "    save_model_name,\n",
    "    f\"episodic-proto-net-pipeline-{UNS_PERCENTAGE}-PROVA\"\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "print(f\"Save paths: {str(save_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb86a4a",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ae54f",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * helper function for 'plot_multilabel_confusion_matrix'\n",
    "def convert_to_categories(elements):\n",
    "    # Convert vector of 0s and 1s to a single binary representation along the first dimension\n",
    "    binary_rep = np.apply_along_axis(\n",
    "        lambda x: \"\".join(map(str, x)), axis=1, arr=elements\n",
    "    )\n",
    "    return np.array([int(x, 2) for x in binary_rep])\n",
    "\n",
    "\n",
    "# * BBDOIA custom confusion matrix for concepts\n",
    "def plot_multilabel_confusion_matrix(\n",
    "    y_true, y_pred, class_names, title, save_path=None\n",
    "):\n",
    "    y_true_categories = convert_to_categories(y_true.astype(int))\n",
    "    y_pred_categories = convert_to_categories(y_pred.astype(int))\n",
    "\n",
    "    to_rtn_cm = confusion_matrix(y_true_categories, y_pred_categories)\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    num_classes = len(class_names)\n",
    "    num_rows = (num_classes + 4) // 5  # Calculate the number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_rows))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.subplot(num_rows, 5, i + 1)  # Set the subplot position\n",
    "        plt.imshow(cm[i], interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Class: {class_names[i]}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "        plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "        fmt = \".0f\"\n",
    "        thresh = cm[i].max() / 2.0\n",
    "        for j in range(cm[i].shape[0]):\n",
    "            for k in range(cm[i].shape[1]):\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    format(cm[i][j, k], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i][j, k] > thresh else \"black\",\n",
    "                )\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_total.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return to_rtn_cm\n",
    "\n",
    "\n",
    "# * Concept collapse (Soft)\n",
    "def compute_coverage(confusion_matrix):\n",
    "    \"\"\"Compute the coverage of a confusion matrix.\n",
    "\n",
    "    Essentially this metric is\n",
    "    \"\"\"\n",
    "\n",
    "    max_values = np.max(confusion_matrix, axis=0)\n",
    "    clipped_values = np.clip(max_values, 0, 1)\n",
    "\n",
    "    # Redefinition of soft coverage\n",
    "    coverage = np.sum(clipped_values) / len(clipped_values)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# * BDDOIA custom confusion matrix for actions\n",
    "def plot_actions_confusion_matrix(c_true, c_pred, title, save_path=None):\n",
    "\n",
    "    my_scenarios = {\n",
    "        \"forward\": [slice(0, 3), slice(0, 3)],  \n",
    "        \"stop\": [slice(3, 9), slice(3, 9)],\n",
    "        \"left\": [slice(9, 11), slice(18,20)],\n",
    "        \"right\": [slice(12, 17), slice(12,17)],\n",
    "    }\n",
    "\n",
    "    to_rtn = {}\n",
    "\n",
    "    # Plot confusion matrix for each scenario\n",
    "    for scenario, indices in my_scenarios.items():\n",
    "\n",
    "        g_true = convert_to_categories(c_true[:, indices[0]].astype(int))\n",
    "        c_pred_scenario = convert_to_categories(c_pred[:, indices[1]].astype(int))\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(g_true, c_pred_scenario)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{title} - {scenario}\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        n_classes = c_true[:, indices[0]].shape[1]\n",
    "\n",
    "        tick_marks = np.arange(2**n_classes)\n",
    "        plt.xticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "        plt.yticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or show plot\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{scenario}.png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        to_rtn.update({scenario: cm})\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return to_rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e7660",
   "metadata": {},
   "source": [
    "## Other Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf554b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * method used to check if all encoder parameters are registered in the model's optimizer\n",
    "def check_optimizer_params(model):\n",
    "    \"\"\"Check that all encoder parameters are registered in the optimizer.\"\"\"\n",
    "    # Get all encoder parameters\n",
    "    encoder_params = []\n",
    "    for i in range(21):\n",
    "        encoder = model.encoder[i]\n",
    "        for name, param in encoder.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue  # skip frozen params\n",
    "            encoder_params.append((f\"encoder_{i}.{name}\", param))\n",
    "\n",
    "    # Get all parameters in the optimizer\n",
    "    opt_param_ids = set(id(p) for group in model.opt.param_groups for p in group['params'])\n",
    "\n",
    "    # Check each encoder param is in the optimizer\n",
    "    missing = [(name, p.shape) for name, p in encoder_params if id(p) not in opt_param_ids]\n",
    "\n",
    "    if missing:\n",
    "        print(\"⚠️ The following parameters are missing from the optimizer:\")\n",
    "        for name, shape in missing:\n",
    "            print(f\"  - {name}: {shape}\")\n",
    "        raise RuntimeError(\"Some encoder parameters are not registered in the optimizer.\")\n",
    "    else:\n",
    "        print(\"✅ All encoder parameters are correctly registered in the optimizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c474d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * semi-deterministic variant of get_random_classes where the positive examples are always the same for a given class index\n",
    "def get_per_class_support_set(proto_datasets:Dataset, pos_examples:dict, class_idx:int, device:str, debug=False):\n",
    "    pos_list = pos_examples[class_idx]  \n",
    "    support_embeddings_pos = torch.stack([ ex['images_embeddings_raw'].unsqueeze(0) for ex in pos_list ], dim=0).to(device)\n",
    "    support_labels_pos = torch.ones(len(pos_list), dtype=torch.long, device=device)\n",
    "    num_pos_labels = support_labels_pos.sum().item()\n",
    "    if debug:\n",
    "        print(f\"Class {class_idx}: {support_labels_pos.shape} embeddings, {support_labels_pos.shape} labels (all 1)\")\n",
    "\n",
    "    proto_labels = proto_datasets[class_idx].labels\n",
    "    proto_data = proto_datasets[class_idx].embeddings\n",
    "    \n",
    "    mask = proto_labels == 0\n",
    "    proto_data_neg = proto_data[mask]\n",
    "    proto_labels_neg = proto_labels[mask]\n",
    "    support_embeddings_neg, support_labels_neg = get_random_classes(\n",
    "        proto_data_neg, proto_labels_neg, n_support=num_pos_labels, n_classes=1\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Support embeddings shape: \", support_embeddings_neg.shape)\n",
    "        print(\"Support labels shape: \", support_labels_neg.shape)\n",
    "    \n",
    "    assert torch.all(support_labels_neg == 0), \"support_labels contains non-zero entries\"\n",
    "\n",
    "    support_embeddings_combined = torch.cat([support_embeddings_pos, support_embeddings_neg], dim=0)\n",
    "    support_labels_combined = torch.cat([support_labels_pos, support_labels_neg], dim=0)\n",
    "    if debug:\n",
    "        print(\"Combined support embeddings shape:\", support_embeddings_combined.shape)\n",
    "        print(\"Combined support labels shape:\", support_labels_combined.shape)\n",
    "\n",
    "    return support_embeddings_combined, support_labels_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd4461",
   "metadata": {},
   "source": [
    "# ANNOTATIONS DATASET & BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4129c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert embeddings.shape[0] == labels.shape[0]\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class PrototypicalBatchSampler(object):\n",
    "    \"\"\"\n",
    "    Yields a batch of indices for episodic training.\n",
    "    At each iteration, it randomly selects 'classes_per_it' classes and then picks\n",
    "    'num_samples' samples for each selected class.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (array-like): 1D array or list of labels for the target task.\n",
    "                                 This should be either the shape labels or the colour labels.\n",
    "            classes_per_it (int): Number of random classes for each iteration.\n",
    "            num_samples (int): Number of samples per class (support + query) in each episode.\n",
    "            iterations (int): Number of iterations (episodes) per epoch.\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # Create an index matrix of shape (num_classes, max_samples_in_class)\n",
    "        max_count = max(self.counts)\n",
    "        self.indexes = np.empty((len(self.classes), max_count), dtype=int)\n",
    "        self.indexes.fill(-1)\n",
    "        self.indexes = torch.LongTensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros(len(self.classes), dtype=torch.long)\n",
    "\n",
    "        # Fill in the matrix with indices for each class.\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            # Find the row corresponding to this label\n",
    "            class_idx = (self.classes == label).nonzero(as_tuple=False).item()\n",
    "            # Find the next available column (where the value is -1)\n",
    "            pos = (self.indexes[class_idx] == -1).nonzero(as_tuple=False)[0].item()\n",
    "            self.indexes[class_idx, pos] = idx\n",
    "            self.numel_per_class[class_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield a batch of indices for each episode.\n",
    "        \"\"\"\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            batch = torch.LongTensor(cpi * spc)\n",
    "            # Randomly choose 'classes_per_it' classes\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "            for i, class_idx in enumerate(c_idxs):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "\n",
    "                n_avail = self.numel_per_class[class_idx]\n",
    "                if spc <= n_avail:\n",
    "                    # enough examples → sample without replacement\n",
    "                    perm = torch.randperm(n_avail)\n",
    "                    sample_idxs = perm[:spc]\n",
    "                else:\n",
    "                    # too few → sample with replacement\n",
    "                    sample_idxs = torch.randint(0, n_avail, (spc,), dtype=torch.long)\n",
    "\n",
    "                batch[s] = self.indexes[class_idx, sample_idxs]\n",
    "\n",
    "            # Shuffle the batch indices\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd69c85",
   "metadata": {},
   "source": [
    "# UNSUPERVISED DATA AND MODEL LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf791f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "[PROTO-INFO] Using Prototypical Networks as backbone\n",
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'bddoiadpldisj', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'shieldedmnist', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'proshieldedmnist', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "✅ All encoder parameters are correctly registered in the optimizer.\n",
      "<datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f66d43bc790>\n",
      "Using Dataset:  <datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f66d43bc790>\n",
      "Using Model:  ProBddoiaDPL()\n",
      "Using Loss:  SDDOIA_DPL()\n",
      "Dataset sizes - train: 16080, val: 2264, test: 4568\n",
      "Loaded datasets in 0.12s\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "assert isinstance(encoder, tuple) and len(encoder) == 21, \"encoder must be a tuple of 21 elements\"\n",
    "\n",
    "# & Main model\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "check_optimizer_params(model)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8363d",
   "metadata": {},
   "source": [
    "## Check Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f5fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = sum(p.numel() for p in model.opt.param_groups[0]['params'])\n",
    "pnets_params = sum(p.numel() for enc in model.encoder for p in enc.parameters())\n",
    "mlp_params = sum(p.numel() for p in model.mlp.parameters()) if args.expressive_model else 0\n",
    "\n",
    "expected_total = pnets_params + mlp_params\n",
    "\n",
    "assert all_params == expected_total, (\n",
    "    f\"Mismatch in optimizer parameters!\\n\"\n",
    "    f\"- Backbone params: {pnets_params:,}\\n\"\n",
    "    f\"- MLP params: {mlp_params:,} (expressive_model={args.expressive_model})\\n\"\n",
    "    f\"- Total in optimizer: {all_params:,}\\n\"\n",
    "    f\"→ Expected total: {expected_total:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75b846",
   "metadata": {},
   "source": [
    "# FETCHING DATA ANNOTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf28ee6",
   "metadata": {},
   "source": [
    "## Build positive annotation set for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id: <class 'tuple'>\n",
      "images_embeddings_raw: torch.Size([2048])\n",
      "attr_labels: torch.Size([21])\n",
      "is_positive: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * 1 POSITIVE EXAMPLES COLLECTION =====\n",
    "pos_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "target_per_class = 6    # Desired number of positives per class\n",
    "debug = True\n",
    "\n",
    "# Loop over dataset until we collect target_per_class for each class\n",
    "for batch_idx, batch in enumerate(unsup_train_loader):\n",
    "    raw_embs = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "    attrs = torch.stack(batch['attr_labels']).to(model.device)  # shape [B,21]\n",
    "    batch_size = attrs.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        attr_vector = attrs[b].clone().cpu()\n",
    "        for cls in torch.nonzero(attr_vector).flatten().tolist():\n",
    "            if len(pos_examples[cls]) >= target_per_class:\n",
    "                continue\n",
    "            example = {\n",
    "                'source_id': (batch_idx, b),\n",
    "                'images_embeddings_raw': raw_embs[b].detach().cpu().clone(),\n",
    "                'attr_labels': attr_vector,\n",
    "                'is_positive': True\n",
    "            }\n",
    "            if debug:\n",
    "                for key, value in example.items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        print(f\"{key}: {value.shape}\")\n",
    "                    elif isinstance(value, list) and len(value) and torch.is_tensor(value[0]):\n",
    "                        print(f\"{key}: list of {len(value)} tensors, first shape: {value[0].shape}\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {type(value)}\")\n",
    "                debug = False\n",
    "            pos_examples[cls].append(example)\n",
    "\n",
    "    if all(len(pos_examples[c]) >= target_per_class for c in range(21)):    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748db809",
   "metadata": {},
   "source": [
    "## Augment positive sets while building negative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d6fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * 2: NEGATIVE EXAMPLES & MULTI-LABEL AUGMENTATION =====\n",
    "neg_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "\n",
    "# Allow multi-label augmentation: add any example with attr_labels[i]==1 to pos_examples[i]\n",
    "for cls in range(21):\n",
    "    seen_ids = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 1 and ex['source_id'] not in seen_ids:\n",
    "                new_ex = ex.copy()\n",
    "                new_ex['is_positive'] = True\n",
    "                pos_examples[cls].append(new_ex)\n",
    "                seen_ids.add(ex['source_id'])\n",
    "\n",
    "# Build negatives: any example that has attr_labels[i]==0 but appears in any pos_examples of other classes\n",
    "for cls in range(21):\n",
    "    seen_ids_pos = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 0 and ex['source_id'] not in seen_ids_pos:\n",
    "                neg_ex = ex.copy()\n",
    "                neg_ex['is_positive'] = False\n",
    "                neg_examples[cls].append(neg_ex)\n",
    "\n",
    "# Ensure no overlap between pos and neg\n",
    "for cls in range(21):\n",
    "    assert not set(ex['source_id'] for ex in neg_examples[cls]) & set(ex['source_id'] for ex in pos_examples[cls]), \\\n",
    "        f\"Overlap in pos/neg for class {cls}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6f384",
   "metadata": {},
   "source": [
    "## Turn annotations into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a240f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: embeddings shape = torch.Size([184, 1, 2048]), labels shape = torch.Size([184]), extended labels shape = torch.Size([184, 21])\n",
      "Class 1: embeddings shape = torch.Size([206, 1, 2048]), labels shape = torch.Size([206]), extended labels shape = torch.Size([206, 21])\n",
      "Class 2: embeddings shape = torch.Size([188, 1, 2048]), labels shape = torch.Size([188]), extended labels shape = torch.Size([188, 21])\n",
      "Class 3: embeddings shape = torch.Size([204, 1, 2048]), labels shape = torch.Size([204]), extended labels shape = torch.Size([204, 21])\n",
      "Class 4: embeddings shape = torch.Size([219, 1, 2048]), labels shape = torch.Size([219]), extended labels shape = torch.Size([219, 21])\n",
      "Class 5: embeddings shape = torch.Size([228, 1, 2048]), labels shape = torch.Size([228]), extended labels shape = torch.Size([228, 21])\n",
      "Class 6: embeddings shape = torch.Size([230, 1, 2048]), labels shape = torch.Size([230]), extended labels shape = torch.Size([230, 21])\n",
      "Class 7: embeddings shape = torch.Size([208, 1, 2048]), labels shape = torch.Size([208]), extended labels shape = torch.Size([208, 21])\n",
      "Class 8: embeddings shape = torch.Size([235, 1, 2048]), labels shape = torch.Size([235]), extended labels shape = torch.Size([235, 21])\n",
      "Class 9: embeddings shape = torch.Size([221, 1, 2048]), labels shape = torch.Size([221]), extended labels shape = torch.Size([221, 21])\n",
      "Class 10: embeddings shape = torch.Size([206, 1, 2048]), labels shape = torch.Size([206]), extended labels shape = torch.Size([206, 21])\n",
      "Class 11: embeddings shape = torch.Size([220, 1, 2048]), labels shape = torch.Size([220]), extended labels shape = torch.Size([220, 21])\n",
      "Class 12: embeddings shape = torch.Size([220, 1, 2048]), labels shape = torch.Size([220]), extended labels shape = torch.Size([220, 21])\n",
      "Class 13: embeddings shape = torch.Size([214, 1, 2048]), labels shape = torch.Size([214]), extended labels shape = torch.Size([214, 21])\n",
      "Class 14: embeddings shape = torch.Size([215, 1, 2048]), labels shape = torch.Size([215]), extended labels shape = torch.Size([215, 21])\n",
      "Class 15: embeddings shape = torch.Size([214, 1, 2048]), labels shape = torch.Size([214]), extended labels shape = torch.Size([214, 21])\n",
      "Class 16: embeddings shape = torch.Size([193, 1, 2048]), labels shape = torch.Size([193]), extended labels shape = torch.Size([193, 21])\n",
      "Class 17: embeddings shape = torch.Size([208, 1, 2048]), labels shape = torch.Size([208]), extended labels shape = torch.Size([208, 21])\n",
      "Class 18: embeddings shape = torch.Size([194, 1, 2048]), labels shape = torch.Size([194]), extended labels shape = torch.Size([194, 21])\n",
      "Class 19: embeddings shape = torch.Size([209, 1, 2048]), labels shape = torch.Size([209]), extended labels shape = torch.Size([209, 21])\n",
      "Class 20: embeddings shape = torch.Size([219, 1, 2048]), labels shape = torch.Size([219]), extended labels shape = torch.Size([219, 21])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * 3: BUILD EMBEDDING TENSORS AND LABELS =====\n",
    "dataset_per_class = {}\n",
    "for cls in range(21):\n",
    "    examples = pos_examples[cls] + neg_examples[cls]\n",
    "    emb_list, label_list, extended_label_list = [], [], []\n",
    "    for ex in examples:\n",
    "        emb_list.append(ex['images_embeddings_raw'].unsqueeze(0))\n",
    "        label_list.append(1 if ex['is_positive'] else 0)\n",
    "        extended_label_list.append(ex['attr_labels'])\n",
    "\n",
    "    embeddings_tensor = torch.stack(emb_list).to(model.device)                  # [N,1,2048]\n",
    "    labels_tensor = torch.tensor(label_list, device=model.device)               # [N]\n",
    "    extended_labels_tensor = torch.stack(extended_label_list).to(model.device)  # [N,21]\n",
    "    dataset_per_class[cls] = {'embeddings': embeddings_tensor, 'labels': labels_tensor, 'extended_labels': extended_labels_tensor}\n",
    "\n",
    "        \n",
    "for cls in range(21):\n",
    "    print(\n",
    "        f\"Class {cls}: embeddings shape = {dataset_per_class[cls]['embeddings'].shape}, \"\n",
    "        f\"labels shape = {dataset_per_class[cls]['labels'].shape}, \"\n",
    "        f\"extended labels shape = {dataset_per_class[cls]['extended_labels'].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c4d46",
   "metadata": {},
   "source": [
    "## *X-Model*: dataloader istantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b08e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * 4: BUILD THE DATALOADER FOR THE EXPRESSIVE MODEL =====\n",
    "if args.expressive_model:\n",
    "    all_embeddings = torch.cat([per_class['embeddings'] for per_class in dataset_per_class.values()], dim=0)\n",
    "    all_labels = torch.cat([per_class['extended_labels'] for per_class in dataset_per_class.values()], dim=0)\n",
    "    all_embeddings = all_embeddings.cpu()\n",
    "    all_labels     = all_labels.cpu()\n",
    "    dataset = ProtoDataset(all_embeddings, all_labels)\n",
    "    x_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    x_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849fd49",
   "metadata": {},
   "source": [
    "## *PNets*: Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3c18e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset per class built with explicit flags and no overlaps.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * 5: CONSISTENCY CHECKS FOR PNETs DATA =====\n",
    "for cls in range(21):\n",
    "    emb = dataset_per_class[cls]['embeddings']\n",
    "    lab = dataset_per_class[cls]['labels']\n",
    "    # Exact count assertion\n",
    "    expected_total = len(pos_examples[cls]) + len(neg_examples[cls])\n",
    "    assert emb.size(0) == expected_total, f\"Class {cls} count mismatch: {emb.size(0)} vs {expected_total}\"\n",
    "    # Label values correct\n",
    "    assert set(lab.tolist()) <= {0,1}, f\"Invalid labels for class {cls}\"\n",
    "    # Check that each positive and negative example has correct label\n",
    "    pos_count = len(pos_examples[cls])\n",
    "    neg_count = len(neg_examples[cls])\n",
    "    # positives should be labeled 1 in the first pos_count entries\n",
    "    for idx in range(pos_count):\n",
    "        assert lab[idx].item() == 1, f\"Positive at wrong pos for class {cls}, idx {idx}\"\n",
    "    # negatives should be labeled 0 in the next neg_count entries\n",
    "    for idx in range(neg_count):\n",
    "        assert lab[pos_count + idx].item() == 0, f\"Negative at wrong pos for class {cls}, idx {pos_count + idx}\"\n",
    "\n",
    "print(\"Dataset per class built with explicit flags and no overlaps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c973f88",
   "metadata": {},
   "source": [
    "## *PNets*: Istantiate Batch Samplers for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e17e495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Dataset Label 0 count: 162, Label 1 count: 22\n",
      "Class 0 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 1 - Dataset Label 0 count: 193, Label 1 count: 13\n",
      "Class 1 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 2 - Dataset Label 0 count: 169, Label 1 count: 19\n",
      "Class 2 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 3 - Dataset Label 0 count: 188, Label 1 count: 16\n",
      "Class 3 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 4 - Dataset Label 0 count: 209, Label 1 count: 10\n",
      "Class 4 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 5 - Dataset Label 0 count: 222, Label 1 count: 6\n",
      "Class 5 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 6 - Dataset Label 0 count: 224, Label 1 count: 6\n",
      "Class 6 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 7 - Dataset Label 0 count: 192, Label 1 count: 16\n",
      "Class 7 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 8 - Dataset Label 0 count: 229, Label 1 count: 6\n",
      "Class 8 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 9 - Dataset Label 0 count: 215, Label 1 count: 6\n",
      "Class 9 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 10 - Dataset Label 0 count: 194, Label 1 count: 12\n",
      "Class 10 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 11 - Dataset Label 0 count: 214, Label 1 count: 6\n",
      "Class 11 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 12 - Dataset Label 0 count: 214, Label 1 count: 6\n",
      "Class 12 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 13 - Dataset Label 0 count: 204, Label 1 count: 10\n",
      "Class 13 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 14 - Dataset Label 0 count: 206, Label 1 count: 9\n",
      "Class 14 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 15 - Dataset Label 0 count: 205, Label 1 count: 9\n",
      "Class 15 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 16 - Dataset Label 0 count: 178, Label 1 count: 15\n",
      "Class 16 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 17 - Dataset Label 0 count: 196, Label 1 count: 12\n",
      "Class 17 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 18 - Dataset Label 0 count: 177, Label 1 count: 17\n",
      "Class 18 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 19 - Dataset Label 0 count: 195, Label 1 count: 14\n",
      "Class 19 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 20 - Dataset Label 0 count: 212, Label 1 count: 7\n",
      "Class 20 - Dataloader Label 0 count: 1000, Label 1 count: 1000\n",
      "Class 0: Dataset size = 184, Dataloader batches = 100\n",
      "Class 1: Dataset size = 206, Dataloader batches = 100\n",
      "Class 2: Dataset size = 188, Dataloader batches = 100\n",
      "Class 3: Dataset size = 204, Dataloader batches = 100\n",
      "Class 4: Dataset size = 219, Dataloader batches = 100\n",
      "Class 5: Dataset size = 228, Dataloader batches = 100\n",
      "Class 6: Dataset size = 230, Dataloader batches = 100\n",
      "Class 7: Dataset size = 208, Dataloader batches = 100\n",
      "Class 8: Dataset size = 235, Dataloader batches = 100\n",
      "Class 9: Dataset size = 221, Dataloader batches = 100\n",
      "Class 10: Dataset size = 206, Dataloader batches = 100\n",
      "Class 11: Dataset size = 220, Dataloader batches = 100\n",
      "Class 12: Dataset size = 220, Dataloader batches = 100\n",
      "Class 13: Dataset size = 214, Dataloader batches = 100\n",
      "Class 14: Dataset size = 215, Dataloader batches = 100\n",
      "Class 15: Dataset size = 214, Dataloader batches = 100\n",
      "Class 16: Dataset size = 193, Dataloader batches = 100\n",
      "Class 17: Dataset size = 208, Dataloader batches = 100\n",
      "Class 18: Dataset size = 194, Dataloader batches = 100\n",
      "Class 19: Dataset size = 209, Dataloader batches = 100\n",
      "Class 20: Dataset size = 219, Dataloader batches = 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * 6 : CREATE EPISODIC DATALOADERS FOR PNETs =====\n",
    "proto_datasets = {}\n",
    "proto_dataloaders = {}\n",
    "\n",
    "for cls in range(21):\n",
    "    proto_data = dataset_per_class[cls]['embeddings']\n",
    "    proto_labels = dataset_per_class[cls]['labels']\n",
    "    proto_datasets[cls] = ProtoDataset(proto_data, proto_labels)\n",
    "    proto_sampler = PrototypicalBatchSampler(\n",
    "                    labels = proto_labels.cpu().numpy(),\n",
    "                    classes_per_it = args.classes_per_it,\n",
    "                    num_samples = args.num_samples,\n",
    "                    iterations = args.iterations,\n",
    "                )\n",
    "    proto_dataloaders[cls] = DataLoader(proto_datasets[cls], batch_sampler=proto_sampler)\n",
    "\n",
    "\n",
    "# Labels count check for proto_datasets[cls] and proto_dataloaders[cls]\n",
    "for cls in range(21):\n",
    "    # Dataset label count\n",
    "    label_counter_dataset = Counter(proto_datasets[cls].labels.cpu().tolist())\n",
    "    print(f\"Class {cls} - Dataset Label 0 count: {label_counter_dataset[0]}, Label 1 count: {label_counter_dataset[1]}\")\n",
    "\n",
    "    # Dataloader label count\n",
    "    label_counter_loader = Counter()\n",
    "    for batch in proto_dataloaders[cls]:\n",
    "        _, labels = batch\n",
    "        label_counter_loader.update(labels.tolist())\n",
    "    print(f\"Class {cls} - Dataloader Label 0 count: {label_counter_loader[0]}, Label 1 count: {label_counter_loader[1]}\")\n",
    "\n",
    "\n",
    "# Final Sanity Check\n",
    "for cls in range(21):\n",
    "    print(f\"Class {cls}: Dataset size = {len(proto_datasets[cls])}, Dataloader batches = {len(proto_dataloaders[cls])}\")\n",
    "    assert len(proto_dataloaders[cls]) == args.iterations, \\\n",
    "        f\"Class {cls}: Expected {args.iterations} batches, got {len(proto_dataloaders[cls])}\"\n",
    "    for batch in proto_dataloaders[cls]:\n",
    "        embeddings, labels = batch\n",
    "        #print(\"Batch Embeddings Shape:\", embeddings.shape, \"Labels Shape:\", labes.shape)\n",
    "        assert embeddings.shape == ((args.num_support + args.num_query) * args.classes_per_it, 1, 2048), \\\n",
    "            f\"Embeddings shape mismatch: {embeddings.shape}\"\n",
    "        assert labels.shape == ((args.num_support + args.num_query) * args.classes_per_it,), \\\n",
    "            f\"Labels shape mismatch: {labels.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0bbaf",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0213922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: MnistDPL, \n",
    "        _loss: ADDMNIST_DPL,\n",
    "        save_path: str, \n",
    "        proto_datasets: dict,\n",
    "        proto_dataloaders: dict,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        x_loader: DataLoader,\n",
    "        args: Namespace,\n",
    "        seed: int = 0,\n",
    "        eval_concepts: List[str] = ['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "        'on_right_turn_lane', 'traffic_light_right', 'front_car_right', \n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "        'on_left_turn_lane', 'traffic_light_left', 'front_car_left']\n",
    "    ) -> float:\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # early stopping\n",
    "    best_f1_cacc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # scheduler & warmup (if used) for main model\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:\n",
    "        w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "    \n",
    "    # --------------------------------------\n",
    "    # ^ 1. PROTOTYPICAL NETWORKS SCHEDULERS & OPTIMIZERS\n",
    "    # --------------------------------------\n",
    "    proto_opts, proto_schs = {}, {}\n",
    "    for c, name in enumerate(eval_concepts):\n",
    "        opt = torch.optim.Adam(model.encoder[c].parameters())\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "        proto_opts[c], proto_schs[c] = opt, sch\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    for b in range(len(model.encoder)):\n",
    "        model.encoder[b].train()\n",
    "        model.encoder[b].to(model.device)\n",
    "            \n",
    "    pNet_loss = PrototypicalLoss(n_support=args.num_support)\n",
    "    for epoch in range(args.n_epochs):\n",
    "\n",
    "        for e in range(args.proto_epochs):\n",
    "        # --------------------------------------\n",
    "        # ^ 2. PROTOTYPICAL NETWORKS TRAINING\n",
    "        # --------------------------------------\n",
    "            print('----------------------------------')\n",
    "            print('--- Prototypical Networks Training ---')                        \n",
    "            print(f\"Prototypical Networks Training Epoch {e + 1}/{args.proto_epochs}\")\n",
    "            losses, accs = {}, {}\n",
    "            for k, name in enumerate(eval_concepts):\n",
    "                dl = proto_dataloaders[k]\n",
    "                opt, sch = proto_opts[k], proto_schs[k]\n",
    "                l, a = train_my_prototypical_network(\n",
    "                    dl, args.iterations,\n",
    "                    model.encoder[k], opt, pNet_loss\n",
    "                )\n",
    "                losses[name], accs[name] = l, a\n",
    "                sch.step()\n",
    "\n",
    "            for name in eval_concepts:\n",
    "                avg_l = sum(losses[name]) / len(losses[name])\n",
    "                avg_a = sum(accs[name])   / len(accs[name])\n",
    "                print(f\"  {name:>25s} - Loss {avg_l:.4f} | Acc {avg_a:.4f}\")\n",
    "\n",
    "        # --------------------------------------\n",
    "        # ^ 3. X-MODEL TRAINING\n",
    "        # --------------------------------------\n",
    "        if args.expressive_model:\n",
    "            print('------------------')\n",
    "            print('--- X-Model Training ---')\n",
    "            model.to(model.device)\n",
    "            model.train()\n",
    "            for i, batch in enumerate(x_loader):\n",
    "                batch_embeds, batch_labels = batch\n",
    "                batch_embeds = batch_embeds.to(model.device)\n",
    "                batch_labels = batch_labels.to(model.device)\n",
    "                assert batch_labels.shape[1] == model.n_facts, (\n",
    "                    f\"batch_labels shape is {batch_labels.shape}, expected (batch_size, {model.n_facts})\"\n",
    "                )\n",
    "\n",
    "                support_emb_dict = {}\n",
    "                for j in range(model.n_facts):\n",
    "                    emb_s, lab_s = get_per_class_support_set(\n",
    "                        proto_datasets, pos_examples, class_idx=j, device=model.device\n",
    "                    )\n",
    "                    support_emb_dict[j] = (emb_s, lab_s)\n",
    "\n",
    "                model.opt.zero_grad()\n",
    "                out_dict = model(batch_embeds, support_emb_dict)\n",
    "                concept_predictions = out_dict[\"CS\"]\n",
    "                loss = F.binary_cross_entropy(concept_predictions, batch_labels.float())\n",
    "\n",
    "                loss.backward()\n",
    "                model.opt.step()\n",
    "\n",
    "                progress_bar(i, len(x_loader), epoch, loss.item())\n",
    "\n",
    "        # --------------------------------------\n",
    "        # ^ 4. MAIN MODEL TRAINING\n",
    "        # --------------------------------------\n",
    "        print('------------------')\n",
    "        print('--- Main Model Training ---')    \n",
    "        print(f\"Main Model Training Epoch {epoch + 1}/{args.n_epochs}\")  \n",
    "        ys, y_true, cs, cs_true, batch = None, None, None, None, 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # ------------------ original embeddings\n",
    "            images_embeddings = torch.stack(batch['embeddings']).to(model.device)\n",
    "            attr_labels = torch.stack(batch['attr_labels']).to(model.device)\n",
    "            class_labels = torch.stack(batch['class_labels'])[:,:-1].to(model.device) # exclude the last column\n",
    "            # ------------------ my extracted features\n",
    "            images_embeddings_raw = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "            detected_rois = batch['rois']\n",
    "            detected_rois_feats = batch['roi_feats']\n",
    "            detection_labels = batch['detection_labels']\n",
    "            detection_scores = batch['detection_scores']\n",
    "            assert_inputs(images_embeddings, attr_labels, class_labels,\n",
    "                    detected_rois_feats, detected_rois, detection_labels,\n",
    "                    detection_scores, images_embeddings_raw)\n",
    "            \n",
    "            support_emb_dict = {}\n",
    "            for j in range(model.n_facts):\n",
    "                emb_s, lab_s = get_per_class_support_set(\n",
    "                    proto_datasets, pos_examples, class_idx=j, device=model.device\n",
    "                )\n",
    "                support_emb_dict[j] = (emb_s, lab_s)\n",
    "\n",
    "            if random.random() > UNS_PERCENTAGE:\n",
    "                continue  # Skip this batch with probability (1 - percentage)\n",
    "            \n",
    "            out_dict = model(images_embeddings_raw, support_emb_dict)\n",
    "            out_dict.update({\"LABELS\": class_labels, \"CONCEPTS\": attr_labels})\n",
    "                \n",
    "            model.opt.zero_grad()\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                    ys = out_dict[\"YS\"]\n",
    "                    y_true = out_dict[\"LABELS\"]\n",
    "                    cs = out_dict[\"pCS\"]\n",
    "                    cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            progress_bar(i, len(train_loader), epoch, loss.item())\n",
    "            \n",
    "        # --------------------------------------\n",
    "        # ^ 5. Evaluation phase\n",
    "        # --------------------------------------\n",
    "        model.eval()\n",
    "        for b in range(len(model.encoder)): \n",
    "            model.encoder[b].eval()\n",
    "\n",
    "        if debug:\n",
    "            y_pred = torch.argmax(ys, dim=-1)\n",
    "            print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        my_metrics = evaluate_metrics(model, val_loader, args, \n",
    "                    support_emb_dict=support_emb_dict,\n",
    "                    eval_concepts=eval_concepts,)\n",
    "\n",
    "        loss = my_metrics[0]\n",
    "        cacc = my_metrics[1]\n",
    "        yacc = my_metrics[2]\n",
    "        f1_y = my_metrics[3]\n",
    "       \n",
    "        # update at end of the epoch\n",
    "        if epoch < args.warmup_steps:   w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1_y)\n",
    "        \n",
    "        if not args.tuning and cacc > best_f1_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_f1_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with cacc score: {best_f1_cacc}\")\n",
    "            \n",
    "        elif cacc <= best_f1_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "\n",
    "    return best_f1_cacc, support_emb_dict\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16280d26",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174d8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training model with seed 1\n",
      "Chosen device: cuda\n",
      "Saving model in folder:  ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1.pth\n",
      "\n",
      "--- Start of Training ---\n",
      "\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 118.64it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.87it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.32it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 87.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.05it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 123.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.54it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 118.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 10.6145 | Acc 0.5560\n",
      "             follow_traffic - Loss 15.5993 | Acc 0.5320\n",
      "                 road_clear - Loss 13.5651 | Acc 0.5310\n",
      "             traffic_lights - Loss 11.1642 | Acc 0.5620\n",
      "              traffic_signs - Loss 5.8869 | Acc 0.8950\n",
      "                       cars - Loss 12.2961 | Acc 0.6700\n",
      "                pedestrians - Loss 12.6790 | Acc 0.5660\n",
      "                     riders - Loss 13.9757 | Acc 0.5560\n",
      "                     others - Loss 9.6395 | Acc 0.7510\n",
      "               no_lane_left - Loss 11.6033 | Acc 0.5580\n",
      "         obstacle_left_lane - Loss 12.4582 | Acc 0.5410\n",
      "            solid_left_line - Loss 15.5246 | Acc 0.5820\n",
      "         on_right_turn_lane - Loss 10.4804 | Acc 0.6950\n",
      "        traffic_light_right - Loss 14.0604 | Acc 0.5470\n",
      "            front_car_right - Loss 15.2369 | Acc 0.5350\n",
      "              no_lane_right - Loss 12.9419 | Acc 0.5310\n",
      "        obstacle_right_lane - Loss 14.3094 | Acc 0.5260\n",
      "           solid_right_line - Loss 12.6107 | Acc 0.4830\n",
      "          on_left_turn_lane - Loss 14.0987 | Acc 0.5110\n",
      "         traffic_light_left - Loss 11.5592 | Acc 0.5140\n",
      "             front_car_left - Loss 11.5643 | Acc 0.5850\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:37 ] epoch 0: |██████████████████████████████████████████████████| loss: 4.35422659"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 63.46657458278868   ACC Y 49.18981481481481 F1 Y 46.39600776538742\n",
      "Saving...\n",
      "Saved best model with cacc score: 63.46657458278868\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 112.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 120.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.63it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 118.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.24it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.56it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 1.2327 | Acc 0.5080\n",
      "             follow_traffic - Loss 0.9592 | Acc 0.5510\n",
      "                 road_clear - Loss 0.7108 | Acc 0.5330\n",
      "             traffic_lights - Loss 1.6761 | Acc 0.6060\n",
      "              traffic_signs - Loss 3.5405 | Acc 0.8850\n",
      "                       cars - Loss 1.9818 | Acc 0.6240\n",
      "                pedestrians - Loss 0.7690 | Acc 0.5380\n",
      "                     riders - Loss 1.1029 | Acc 0.5720\n",
      "                     others - Loss 1.2769 | Acc 0.7420\n",
      "               no_lane_left - Loss 0.7531 | Acc 0.5280\n",
      "         obstacle_left_lane - Loss 1.0211 | Acc 0.5320\n",
      "            solid_left_line - Loss 1.0349 | Acc 0.5560\n",
      "         on_right_turn_lane - Loss 1.3093 | Acc 0.6100\n",
      "        traffic_light_right - Loss 1.5525 | Acc 0.5760\n",
      "            front_car_right - Loss 1.7040 | Acc 0.5160\n",
      "              no_lane_right - Loss 1.0729 | Acc 0.5140\n",
      "        obstacle_right_lane - Loss 1.5471 | Acc 0.5490\n",
      "           solid_right_line - Loss 1.6266 | Acc 0.4470\n",
      "          on_left_turn_lane - Loss 0.7373 | Acc 0.4880\n",
      "         traffic_light_left - Loss 0.9589 | Acc 0.5500\n",
      "             front_car_left - Loss 1.3582 | Acc 0.5230\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:40 ] epoch 1: |██████████████████████████████████████████████████| loss: 2.92285562"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 56.58206707901425   ACC Y 47.413917824074076 F1 Y 27.59282355646202\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 97.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.54it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.99it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.8888 | Acc 0.5950\n",
      "             follow_traffic - Loss 1.3564 | Acc 0.5600\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.8224 | Acc 0.5410\n",
      "              traffic_signs - Loss 1.1097 | Acc 0.8820\n",
      "                       cars - Loss 0.5831 | Acc 0.7430\n",
      "                pedestrians - Loss 0.6933 | Acc 0.5240\n",
      "                     riders - Loss 0.8184 | Acc 0.5790\n",
      "                     others - Loss 0.4893 | Acc 0.7590\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5010\n",
      "         obstacle_left_lane - Loss 0.6349 | Acc 0.7090\n",
      "            solid_left_line - Loss 0.6146 | Acc 0.6970\n",
      "         on_right_turn_lane - Loss 1.0646 | Acc 0.5580\n",
      "        traffic_light_right - Loss 0.6794 | Acc 0.5830\n",
      "            front_car_right - Loss 0.7056 | Acc 0.5160\n",
      "              no_lane_right - Loss 0.7624 | Acc 0.5460\n",
      "        obstacle_right_lane - Loss 0.8405 | Acc 0.5400\n",
      "           solid_right_line - Loss 0.7626 | Acc 0.4470\n",
      "          on_left_turn_lane - Loss 0.7048 | Acc 0.4900\n",
      "         traffic_light_left - Loss 0.6920 | Acc 0.5250\n",
      "             front_car_left - Loss 0.6238 | Acc 0.7120\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:43 ] epoch 2: |██████████████████████████████████████████████████| loss: 2.78687882"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 58.204504185252716   ACC Y 49.576822916666664 F1 Y 34.36952659886376\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 121.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.46it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 93.06it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 132.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.84it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.05it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.96it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7335 | Acc 0.5720\n",
      "             follow_traffic - Loss 0.8327 | Acc 0.5810\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 1.4037 | Acc 0.5340\n",
      "              traffic_signs - Loss 0.7298 | Acc 0.8470\n",
      "                       cars - Loss 0.3835 | Acc 0.9020\n",
      "                pedestrians - Loss 0.6911 | Acc 0.5060\n",
      "                     riders - Loss 0.7887 | Acc 0.5370\n",
      "                     others - Loss 0.4235 | Acc 0.7910\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5010\n",
      "         obstacle_left_lane - Loss 0.8234 | Acc 0.6790\n",
      "            solid_left_line - Loss 0.4460 | Acc 0.8410\n",
      "         on_right_turn_lane - Loss 0.8089 | Acc 0.5940\n",
      "        traffic_light_right - Loss 0.6915 | Acc 0.5330\n",
      "            front_car_right - Loss 1.5813 | Acc 0.5150\n",
      "              no_lane_right - Loss 0.6947 | Acc 0.4790\n",
      "        obstacle_right_lane - Loss 0.8020 | Acc 0.5460\n",
      "           solid_right_line - Loss 0.6981 | Acc 0.4740\n",
      "          on_left_turn_lane - Loss 0.7000 | Acc 0.5590\n",
      "         traffic_light_left - Loss 0.6922 | Acc 0.5230\n",
      "             front_car_left - Loss 0.2173 | Acc 0.9370\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:46 ] epoch 3: |██████████████████████████████████████████████████| loss: 2.83612776"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 57.800100247065224   ACC Y 50.0 F1 Y 35.45214426371678\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 150.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.64it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 150.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 150.75it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.58it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 95.16it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.73it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 138.35it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 150.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7822 | Acc 0.6000\n",
      "             follow_traffic - Loss 0.9221 | Acc 0.5540\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.8922 | Acc 0.5230\n",
      "              traffic_signs - Loss 0.4779 | Acc 0.8730\n",
      "                       cars - Loss 0.1782 | Acc 0.9430\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.7243 | Acc 0.6450\n",
      "                     others - Loss 0.3907 | Acc 0.8210\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5040\n",
      "         obstacle_left_lane - Loss 0.9511 | Acc 0.5470\n",
      "            solid_left_line - Loss 0.1783 | Acc 0.9600\n",
      "         on_right_turn_lane - Loss 0.7991 | Acc 0.5830\n",
      "        traffic_light_right - Loss 0.6682 | Acc 0.5870\n",
      "            front_car_right - Loss 0.7167 | Acc 0.5160\n",
      "              no_lane_right - Loss 0.6935 | Acc 0.4680\n",
      "        obstacle_right_lane - Loss 1.2980 | Acc 0.4860\n",
      "           solid_right_line - Loss 0.6941 | Acc 0.4610\n",
      "          on_left_turn_lane - Loss 0.6924 | Acc 0.5770\n",
      "         traffic_light_left - Loss 0.6924 | Acc 0.5380\n",
      "             front_car_left - Loss 0.1324 | Acc 0.9570\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:49 ] epoch 4: |██████████████████████████████████████████████████| loss: 2.78131747"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 56.960290835963356   ACC Y 54.38006365740741 F1 Y 39.206821733887665\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 113.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 123.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 123.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 126.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 125.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 125.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 126.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 125.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 126.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.57it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.70it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.35it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 90.60it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.87it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 90.94it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 90.13it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7483 | Acc 0.5390\n",
      "             follow_traffic - Loss 0.7541 | Acc 0.5950\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.8107 | Acc 0.5370\n",
      "              traffic_signs - Loss 0.4060 | Acc 0.8900\n",
      "                       cars - Loss 0.0282 | Acc 0.9930\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.5745 | Acc 0.7460\n",
      "                     others - Loss 0.1438 | Acc 0.9510\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5000\n",
      "         obstacle_left_lane - Loss 0.6694 | Acc 0.6470\n",
      "            solid_left_line - Loss 0.1136 | Acc 0.9610\n",
      "         on_right_turn_lane - Loss 0.8741 | Acc 0.6360\n",
      "        traffic_light_right - Loss 0.9718 | Acc 0.6030\n",
      "            front_car_right - Loss 0.6931 | Acc 0.5210\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.4980\n",
      "        obstacle_right_lane - Loss 0.7214 | Acc 0.4760\n",
      "           solid_right_line - Loss 0.6932 | Acc 0.4500\n",
      "          on_left_turn_lane - Loss 0.6826 | Acc 0.5600\n",
      "         traffic_light_left - Loss 0.6930 | Acc 0.4980\n",
      "             front_car_left - Loss 0.0920 | Acc 0.9840\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:52 ] epoch 5: |██████████████████████████████████████████████████| loss: 2.77803755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 61.200673547055985   ACC Y 55.045572916666664 F1 Y 37.8323209540063\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 117.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.35it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.69it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.32it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.14it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.45it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 86.47it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 85.30it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 85.99it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.77it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 132.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 151.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7253 | Acc 0.5830\n",
      "             follow_traffic - Loss 0.6917 | Acc 0.5810\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.7284 | Acc 0.5560\n",
      "              traffic_signs - Loss 0.2473 | Acc 0.9320\n",
      "                       cars - Loss 0.0300 | Acc 0.9890\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.4228 | Acc 0.8160\n",
      "                     others - Loss 0.0401 | Acc 0.9890\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5040\n",
      "         obstacle_left_lane - Loss 0.4477 | Acc 0.8770\n",
      "            solid_left_line - Loss 0.0406 | Acc 0.9840\n",
      "         on_right_turn_lane - Loss 0.8156 | Acc 0.6400\n",
      "        traffic_light_right - Loss 0.6933 | Acc 0.4740\n",
      "            front_car_right - Loss 0.6847 | Acc 0.5270\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.4730\n",
      "        obstacle_right_lane - Loss 0.7304 | Acc 0.4770\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.4470\n",
      "          on_left_turn_lane - Loss 0.6709 | Acc 0.6130\n",
      "         traffic_light_left - Loss 0.6912 | Acc 0.5540\n",
      "             front_car_left - Loss 0.0530 | Acc 0.9890\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:55 ] epoch 6: |██████████████████████████████████████████████████| loss: 2.67471957"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 61.90131836467319   ACC Y 55.812355324074076 F1 Y 39.29279148315151\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 84.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.67it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.91it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.26it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.18it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.40it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.36it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 82.87it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.80it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.66it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7058 | Acc 0.6110\n",
      "             follow_traffic - Loss 0.6905 | Acc 0.5710\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.7830 | Acc 0.5430\n",
      "              traffic_signs - Loss 0.1283 | Acc 0.9630\n",
      "                       cars - Loss 0.0665 | Acc 0.9890\n",
      "                pedestrians - Loss 0.6924 | Acc 0.5310\n",
      "                     riders - Loss 0.2765 | Acc 0.9070\n",
      "                     others - Loss 0.0500 | Acc 0.9790\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5020\n",
      "         obstacle_left_lane - Loss 0.0567 | Acc 0.9810\n",
      "            solid_left_line - Loss 0.0026 | Acc 1.0000\n",
      "         on_right_turn_lane - Loss 0.7370 | Acc 0.5570\n",
      "        traffic_light_right - Loss 0.6932 | Acc 0.4740\n",
      "            front_car_right - Loss 0.6923 | Acc 0.4970\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.4900\n",
      "        obstacle_right_lane - Loss 0.6953 | Acc 0.4670\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.4430\n",
      "          on_left_turn_lane - Loss 0.6652 | Acc 0.5870\n",
      "         traffic_light_left - Loss 0.6914 | Acc 0.5420\n",
      "             front_car_left - Loss 0.0642 | Acc 0.9820\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 15:58 ] epoch 7: |██████████████████████████████████████████████████| loss: 3.00546217"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 57.209684782558014   ACC Y 55.67491319444444 F1 Y 39.8304512379514\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 99.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.37it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 90.06it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 85.81it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.99it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.54it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.69it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.31it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.57it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 86.34it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.23it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6380 | Acc 0.6430\n",
      "             follow_traffic - Loss 0.6909 | Acc 0.5700\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5000\n",
      "             traffic_lights - Loss 0.7377 | Acc 0.5200\n",
      "              traffic_signs - Loss 0.1563 | Acc 0.9630\n",
      "                       cars - Loss 0.0574 | Acc 0.9830\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.1416 | Acc 0.9460\n",
      "                     others - Loss 0.0617 | Acc 0.9880\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5050\n",
      "         obstacle_left_lane - Loss 0.0518 | Acc 0.9840\n",
      "            solid_left_line - Loss 0.1310 | Acc 0.9640\n",
      "         on_right_turn_lane - Loss 0.7680 | Acc 0.5560\n",
      "        traffic_light_right - Loss 0.6932 | Acc 0.4670\n",
      "            front_car_right - Loss 0.6889 | Acc 0.5180\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.4810\n",
      "        obstacle_right_lane - Loss 0.6932 | Acc 0.4680\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.4960\n",
      "          on_left_turn_lane - Loss 0.6822 | Acc 0.5750\n",
      "         traffic_light_left - Loss 0.6905 | Acc 0.5530\n",
      "             front_car_left - Loss 0.0245 | Acc 0.9990\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:01 ] epoch 8: |██████████████████████████████████████████████████| loss: 2.77759433"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 56.17972993188434   ACC Y 56.95891203703704 F1 Y 40.99537510223115\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 88.67it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 113.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.84it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.50it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 122.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 113.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.06it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.54it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 95.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6399 | Acc 0.6330\n",
      "             follow_traffic - Loss 0.6928 | Acc 0.5590\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5010\n",
      "             traffic_lights - Loss 0.6927 | Acc 0.6220\n",
      "              traffic_signs - Loss 0.0345 | Acc 0.9930\n",
      "                       cars - Loss 0.1798 | Acc 0.9490\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0575 | Acc 0.9890\n",
      "                     others - Loss 0.0384 | Acc 0.9940\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5070\n",
      "         obstacle_left_lane - Loss 0.0191 | Acc 0.9940\n",
      "            solid_left_line - Loss 0.0719 | Acc 0.9810\n",
      "         on_right_turn_lane - Loss 0.7221 | Acc 0.5930\n",
      "        traffic_light_right - Loss 0.6932 | Acc 0.4570\n",
      "            front_car_right - Loss 0.7237 | Acc 0.5180\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5040\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.4920\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.6606 | Acc 0.5980\n",
      "         traffic_light_left - Loss 0.6926 | Acc 0.5250\n",
      "             front_car_left - Loss 0.0213 | Acc 0.9970\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:04 ] epoch 9: |██████████████████████████████████████████████████| loss: 2.79806471"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 57.172482212384544   ACC Y 55.660445601851855 F1 Y 37.99096508778635\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 105.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 114.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.33it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.19it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.30it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.23it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 111.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.24it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7110 | Acc 0.5990\n",
      "             follow_traffic - Loss 0.6930 | Acc 0.5550\n",
      "                 road_clear - Loss 0.6931 | Acc 0.4960\n",
      "             traffic_lights - Loss 0.6161 | Acc 0.6840\n",
      "              traffic_signs - Loss 0.0350 | Acc 0.9900\n",
      "                       cars - Loss 0.0008 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0388 | Acc 0.9870\n",
      "                     others - Loss 0.0002 | Acc 1.0000\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5070\n",
      "         obstacle_left_lane - Loss 0.0065 | Acc 0.9990\n",
      "            solid_left_line - Loss 0.0049 | Acc 1.0000\n",
      "         on_right_turn_lane - Loss 0.7980 | Acc 0.6430\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.4630\n",
      "            front_car_right - Loss 1.3939 | Acc 0.5240\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5010\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.6620 | Acc 0.6290\n",
      "         traffic_light_left - Loss 0.6930 | Acc 0.5220\n",
      "             front_car_left - Loss 0.0300 | Acc 0.9940\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:07 ] epoch 10: |██████████████████████████████████████████████████| loss: 2.71448493"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 57.1153011586931   ACC Y 58.06206597222222 F1 Y 41.55334004354282\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 94.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 92.03it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 95.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 107.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 110.17it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 109.06it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 108.29it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6815 | Acc 0.5840\n",
      "             follow_traffic - Loss 0.6931 | Acc 0.5520\n",
      "                 road_clear - Loss 0.6931 | Acc 0.4990\n",
      "             traffic_lights - Loss 0.3670 | Acc 0.8630\n",
      "              traffic_signs - Loss 0.0382 | Acc 0.9920\n",
      "                       cars - Loss 0.0004 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0203 | Acc 0.9960\n",
      "                     others - Loss 0.0048 | Acc 0.9980\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5020\n",
      "         obstacle_left_lane - Loss 0.0036 | Acc 1.0000\n",
      "            solid_left_line - Loss 0.0006 | Acc 1.0000\n",
      "         on_right_turn_lane - Loss 0.9110 | Acc 0.6250\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.4950\n",
      "            front_car_right - Loss 1.2246 | Acc 0.5460\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5030\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.7033 | Acc 0.5520\n",
      "         traffic_light_left - Loss 0.6914 | Acc 0.5630\n",
      "             front_car_left - Loss 0.0351 | Acc 0.9950\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:10 ] epoch 11: |██████████████████████████████████████████████████| loss: 2.72844195"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 54.960318737559845   ACC Y 57.51229745370371 F1 Y 41.27989070145826\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 101.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.31it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.05it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.69it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.43it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.23it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.42it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.32it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.77it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6716 | Acc 0.6210\n",
      "             follow_traffic - Loss 0.6931 | Acc 0.5370\n",
      "                 road_clear - Loss 0.6931 | Acc 0.4980\n",
      "             traffic_lights - Loss 0.3239 | Acc 0.8960\n",
      "              traffic_signs - Loss 0.0378 | Acc 0.9880\n",
      "                       cars - Loss 0.0003 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0065 | Acc 0.9980\n",
      "                     others - Loss 0.0003 | Acc 1.0000\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5080\n",
      "         obstacle_left_lane - Loss 0.0015 | Acc 1.0000\n",
      "            solid_left_line - Loss 0.0169 | Acc 0.9980\n",
      "         on_right_turn_lane - Loss 0.8031 | Acc 0.6390\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.4810\n",
      "            front_car_right - Loss 1.0649 | Acc 0.5110\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5000\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.6194 | Acc 0.6580\n",
      "         traffic_light_left - Loss 0.6893 | Acc 0.5660\n",
      "             front_car_left - Loss 0.0662 | Acc 0.9810\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:13 ] epoch 12: |██████████████████████████████████████████████████| loss: 2.78028512"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 55.95307101806005   ACC Y 58.22120949074073 F1 Y 42.25888128820598\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 119.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 116.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 119.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.06it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.31it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.04it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.68it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 86.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.35it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.29it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.7869 | Acc 0.6430\n",
      "             follow_traffic - Loss 0.6925 | Acc 0.6000\n",
      "                 road_clear - Loss 0.6931 | Acc 0.5030\n",
      "             traffic_lights - Loss 0.1009 | Acc 0.9650\n",
      "              traffic_signs - Loss 0.0296 | Acc 0.9910\n",
      "                       cars - Loss 0.0001 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0145 | Acc 0.9960\n",
      "                     others - Loss 0.0002 | Acc 1.0000\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5320\n",
      "         obstacle_left_lane - Loss 0.0014 | Acc 1.0000\n",
      "            solid_left_line - Loss 0.0015 | Acc 0.9990\n",
      "         on_right_turn_lane - Loss 0.7690 | Acc 0.6450\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.4910\n",
      "            front_car_right - Loss 0.9659 | Acc 0.5250\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5000\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.5866 | Acc 0.7170\n",
      "         traffic_light_left - Loss 0.6916 | Acc 0.5400\n",
      "             front_car_left - Loss 0.0158 | Acc 1.0000\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:16 ] epoch 13: |██████████████████████████████████████████████████| loss: 2.98752889"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 55.375745064682434   ACC Y 58.83608217592593 F1 Y 42.63313744682659\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 113.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.05it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 100.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 101.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.42it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.75it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.03it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6604 | Acc 0.6660\n",
      "             follow_traffic - Loss 0.6926 | Acc 0.5660\n",
      "                 road_clear - Loss 0.6931 | Acc 0.4950\n",
      "             traffic_lights - Loss 0.0584 | Acc 0.9790\n",
      "              traffic_signs - Loss 0.0238 | Acc 0.9930\n",
      "                       cars - Loss 0.0008 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0026 | Acc 1.0000\n",
      "                     others - Loss 0.0004 | Acc 1.0000\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5570\n",
      "         obstacle_left_lane - Loss 0.0354 | Acc 0.9860\n",
      "            solid_left_line - Loss 0.0010 | Acc 1.0000\n",
      "         on_right_turn_lane - Loss 0.8264 | Acc 0.6200\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.5270\n",
      "            front_car_right - Loss 1.4503 | Acc 0.5270\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5000\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.4780 | Acc 0.7970\n",
      "         traffic_light_left - Loss 0.6865 | Acc 0.5790\n",
      "             front_car_left - Loss 0.0521 | Acc 0.9880\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:18 ] epoch 14: |██████████████████████████████████████████████████| loss: 2.69927359"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 56.007497012615204   ACC Y 59.75477430555556 F1 Y 44.576167305103795\n",
      "----------------------------------\n",
      "--- Prototypical Networks Training ---\n",
      "Prototypical Networks Training Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 112.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.29it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.28it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.03it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 99.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.08it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.36it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.11it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 85.12it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.73it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.29it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 104.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 105.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               green_lights - Loss 0.6838 | Acc 0.6400\n",
      "             follow_traffic - Loss 0.6927 | Acc 0.6150\n",
      "                 road_clear - Loss 0.6931 | Acc 0.4910\n",
      "             traffic_lights - Loss 0.0462 | Acc 0.9890\n",
      "              traffic_signs - Loss 0.0151 | Acc 0.9960\n",
      "                       cars - Loss 0.0005 | Acc 1.0000\n",
      "                pedestrians - Loss 0.6931 | Acc 0.5000\n",
      "                     riders - Loss 0.0031 | Acc 1.0000\n",
      "                     others - Loss 0.0005 | Acc 1.0000\n",
      "               no_lane_left - Loss 0.6931 | Acc 0.5610\n",
      "         obstacle_left_lane - Loss 0.0006 | Acc 1.0000\n",
      "            solid_left_line - Loss 0.0010 | Acc 1.0000\n",
      "         on_right_turn_lane - Loss 0.8034 | Acc 0.6480\n",
      "        traffic_light_right - Loss 0.6931 | Acc 0.5530\n",
      "            front_car_right - Loss 0.7163 | Acc 0.4850\n",
      "              no_lane_right - Loss 0.6931 | Acc 0.5000\n",
      "        obstacle_right_lane - Loss 0.6931 | Acc 0.5000\n",
      "           solid_right_line - Loss 0.6931 | Acc 0.5000\n",
      "          on_left_turn_lane - Loss 0.4089 | Acc 0.8480\n",
      "         traffic_light_left - Loss 0.6890 | Acc 0.5510\n",
      "             front_car_left - Loss 0.0189 | Acc 1.0000\n",
      "------------------\n",
      "--- Main Model Training ---\n",
      "Main Model Training Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 09-04 | 16:21 ] epoch 15: |██████████████████████████████████████████████████| loss: 2.75822139"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 62.414573629697166   ACC Y 59.450954861111114 F1 Y 44.30103823512157\n",
      "Early stopping triggered after 16 epochs.\n",
      "\n",
      "--- End of Training ---\n",
      "\n",
      "*** Finished training model with seed 1 and best CACC score 63.46657458278868\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "best_f1_c, support_emb_dict = train(\n",
    "        model=model,\n",
    "        proto_datasets=proto_datasets,\n",
    "        proto_dataloaders=proto_dataloaders,\n",
    "        train_loader=unsup_train_loader,\n",
    "        val_loader=unsup_val_loader,\n",
    "        x_loader=x_loader,\n",
    "        save_path=save_folder,\n",
    "        _loss=loss,\n",
    "        args=args,\n",
    "        seed=SEED,\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "print(f\"*** Finished training model with seed {args.seed} and best CACC score {best_f1_c}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c24c9",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4033596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Evaluate the model and save metrics\n",
    "def evaluate_my_model(model: MnistDPL, \n",
    "        save_path: str, \n",
    "        test_loader: DataLoader,\n",
    "        eval_concepts: List[str],\n",
    "        args: Namespace,\n",
    "        support_embeddings=None,\n",
    "    ):\n",
    "    \n",
    "    if args.model == 'probddoiadpl':\n",
    "        assert support_embeddings is not None, \"Support embeddings must be provided for probddoiadpl model evaluation.\"\n",
    "        my_metrics = evaluate_metrics(model, test_loader, args,\n",
    "                        support_emb_dict=support_embeddings, \n",
    "                        eval_concepts=eval_concepts\n",
    "        )\n",
    "    else:\n",
    "        my_metrics = evaluate_metrics(model, test_loader, args, \n",
    "                        eval_concepts=eval_concepts\n",
    "        )\n",
    "\n",
    "    loss = my_metrics[0]\n",
    "    cacc = my_metrics[1]\n",
    "    yacc = my_metrics[2]\n",
    "    f1_y = my_metrics[3]\n",
    "    f1_micro = my_metrics[4]\n",
    "    f1_weight = my_metrics[5]\n",
    "    f1_bin = my_metrics[6]\n",
    "\n",
    "    metrics_log_path = save_path.replace(\".pth\", \"_metrics.log\")\n",
    "    \n",
    "    all_concepts = [ 'Green Traffic Light', 'Follow Traffic', 'Road Is Clear',\n",
    "        'Red Traffic Light', 'Traffic Sign', 'Obstacle Car', 'Obstacle Pedestrian', 'Obstacle Rider', 'Obstacle Others',\n",
    "        'No Lane On The Left',  'Obstacle On The Left Lane',  'Solid Left Line',\n",
    "                'On The Right Turn Lane', 'Traffic Light Allows Right', 'Front Car Turning Right', \n",
    "        'No Lane On The Right', 'Obstacle On The Right Lane', 'Solid Right Line',\n",
    "                'On The Left Turn Lane',  'Traffic Light Allows Left',  'Front Car Turning Left' \n",
    "    ]\n",
    "    aggregated_metrics = [\n",
    "            'F1 - Binary', 'F1 - Macro', 'F1 - Micro', 'F1 - Weighted',\n",
    "            'Precision - Binary', 'Precision - Macro', 'Precision - Micro', 'Precision - Weighted',\n",
    "            'Recall - Binary', 'Recall - Macro', 'Recall - Micro', 'Recall - Weighted',\n",
    "            'Balanced Accuracy'\n",
    "    ]\n",
    "\n",
    "    sums = [0.0] * len(aggregated_metrics)\n",
    "    num_concepts = len(all_concepts)\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ACC C: {cacc}, ACC Y: {yacc}\\n\\n\")\n",
    "        log_file.write(f\"F1 Y - Macro: {f1_y}, F1 Y - Micro: {f1_micro}, F1 Y - Weighted: {f1_weight}, F1 Y - Binary: {f1_bin} \\n\\n\")\n",
    "\n",
    "        def write_metrics(class_name, offset):\n",
    "            print(f\"Reporting Metrics for {class_name} in {metrics_log_path}\")\n",
    "            log_file.write(f\"{class_name.upper()}\\n\")\n",
    "            for idx, metric_name in enumerate(aggregated_metrics):\n",
    "                value = my_metrics[offset + idx]\n",
    "                sums[idx] += value\n",
    "                log_file.write(f\"  {metric_name:<18} {value:.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "        i = 7\n",
    "        for concept in all_concepts:\n",
    "            write_metrics(concept, i)\n",
    "            i += len(aggregated_metrics)\n",
    "\n",
    "        log_file.write(\"**MEAN ACROSS ALL CONCEPTS**\\n\")\n",
    "        for idx, metric_name in enumerate(aggregated_metrics):\n",
    "            mean_value = sums[idx] / num_concepts\n",
    "            log_file.write(f\"  {metric_name:<18} {mean_value:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "    assert len(my_metrics) == 7 + len(all_concepts) * len(aggregated_metrics), \\\n",
    "        f\"Expected {7 + len(all_concepts) * len(aggregated_metrics)} metrics, but got {len(my_metrics)}\"\n",
    "    \n",
    "    if args.model == 'probddoiadpl':\n",
    "        y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "            evaluate_metrics(model, test_loader, args,\n",
    "                        support_emb_dict=support_embeddings, \n",
    "                        eval_concepts=eval_concepts,\n",
    "                        last=True\n",
    "                )\n",
    "        )\n",
    "    else:\n",
    "        y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "            evaluate_metrics(model, test_loader, args,\n",
    "                        eval_concepts=eval_concepts,\n",
    "                        last=True\n",
    "                )\n",
    "        )\n",
    "    \n",
    "    y_labels = [\"stop\", \"forward\", \"left\", \"right\"]\n",
    "    concept_labels = [\n",
    "        \"green_light\",      \n",
    "        \"follow\",           \n",
    "        \"road_clear\",       \n",
    "        \"red_light\",        \n",
    "        \"traffic_sign\",     \n",
    "        \"car\",              \n",
    "        \"person\",           \n",
    "        \"rider\",            \n",
    "        \"other_obstacle\",   \n",
    "        \"left_lane\",\n",
    "        \"left_green_light\",\n",
    "        \"left_follow\",\n",
    "        \"no_left_lane\",\n",
    "        \"left_obstacle\",\n",
    "        \"letf_solid_line\",\n",
    "        \"right_lane\",\n",
    "        \"right_green_light\",\n",
    "        \"right_follow\",\n",
    "        \"no_right_lane\",\n",
    "        \"right_obstacle\",\n",
    "        \"right_solid_line\",\n",
    "    ]\n",
    "\n",
    "    plot_multilabel_confusion_matrix(y_true, y_pred, y_labels, \"Labels\", save_path=save_path)\n",
    "    cfs = plot_actions_confusion_matrix(c_true, c_pred, \"Concepts\", save_path=save_path)\n",
    "    cf = plot_multilabel_confusion_matrix(c_true, c_pred, concept_labels, \"Concepts\", save_path=save_path)\n",
    "    print(\"Concept collapse\", 1 - compute_coverage(cf))\n",
    "\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        for key, value in cfs.items():\n",
    "            log_file.write(f\"Concept collapse: {key}, {1 - compute_coverage(value):.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "    fprint(\"\\n--- End of Evaluation ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353251c6",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c666b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['promnistltn', 'promnmathcbm', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'kandslsingledisj', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'kandltnsinglejoint', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'kanddplsinglejoint', 'xordpl', 'promnmathdpl', 'bddoiadpldisj', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'kandltnsingledisj', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'kanddplsingledisj', 'mnistsl', 'kandslsinglejoint', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "Reporting Metrics for Green Traffic Light in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Follow Traffic in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Road Is Clear in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Red Traffic Light in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Traffic Sign in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle Car in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle Pedestrian in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle Rider in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle Others in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for No Lane On The Left in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle On The Left Lane in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Solid Left Line in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for On The Right Turn Lane in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Traffic Light Allows Right in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Front Car Turning Right in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for No Lane On The Right in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Obstacle On The Right Lane in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Solid Right Line in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for On The Left Turn Lane in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Traffic Light Allows Left in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Reporting Metrics for Front Car Turning Left in ../notebook-outputs/bddoia/my_models/dpl/episodic-proto-net-pipeline-1.0-PROVA/dpl_1_metrics.log\n",
      "Concept collapse 0.7410256410256411\n",
      "\n",
      "--- End of Evaluation ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model object\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "\n",
    "# Load the model state dictionary into the model object\n",
    "model_state_dict = torch.load(save_folder)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_my_model(\n",
    "    model=model, \n",
    "    save_path=save_folder, \n",
    "    test_loader=unsup_test_loader,\n",
    "    eval_concepts=['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "        'on_right_turn_lane', 'traffic_light_right', 'front_car_right',\n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "        'on_left_turn_lane', 'traffic_light_left', 'front_car_left'],\n",
    "    args=args,\n",
    "    support_embeddings=support_emb_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
