{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db624e1e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee0a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia/notebooks', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python38.zip', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/lib-dynload', '', '/users-1/eleonora/.local/lib/python3.8/site-packages', '/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation/bddoia', '/users-1/eleonora/reasoning-shortcuts/IXShort/shortcut_mitigation', '/users-1/eleonora/reasoning-shortcuts/IXShort']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "sys.path.append(os.path.abspath(\"..\"))       # for 'protonet_STOP_bddoia_modules' folder\n",
    "sys.path.append(os.path.abspath(\"../..\"))    # for 'data' folder\n",
    "sys.path.append(os.path.abspath(\"../../..\")) # for 'models' and 'datasets' folders\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a97cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import setproctitle, socket, uuid\n",
    "\n",
    "from models import get_model\n",
    "from models.mnistdpl import MnistDPL\n",
    "from datasets import get_dataset\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "from utils import fprint\n",
    "from utils.status import progress_bar\n",
    "from utils.metrics import evaluate_metrics\n",
    "from utils.dpl_loss import ADDMNIST_DPL\n",
    "from utils.checkpoint import save_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from baseline_modules.arguments import args_dpl \n",
    "from backbones.bddoia_protonet import ProtoNetConv1D, PrototypicalLoss\n",
    "from protonet_STOP_bddoia_modules.proto_modules.proto_helpers import assert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b846",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0680d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 90\n",
      "Save paths: ['../NEW-outputs/bddoia/baseline/dpl/AUG-baseline']\n"
     ]
    }
   ],
   "source": [
    "args = args_dpl\n",
    "args.seed = 90\n",
    "\n",
    "# logging\n",
    "args.conf_jobnum = str(uuid.uuid4())\n",
    "args.conf_timestamp = str(datetime.datetime.now())\n",
    "args.conf_host = socket.gethostname()\n",
    "\n",
    "# set job name\n",
    "setproctitle.setproctitle(\n",
    "    \"{}_{}_{}\".format(\n",
    "        args.model,\n",
    "        args.buffer_size if \"buffer_size\" in args else 0,\n",
    "        args.dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "# saving\n",
    "save_folder = \"bddoia\" \n",
    "save_model_name = 'dpl'\n",
    "save_paths = []\n",
    "save_path = os.path.join(\"..\",\n",
    "    \"NEW-outputs\", \n",
    "    save_folder, \n",
    "    \"baseline\", \n",
    "    save_model_name,\n",
    "    f\"AUG-baseline\"\n",
    ")\n",
    "save_paths.append(save_path)\n",
    "\n",
    "print(\"Seed: \" + str(args.seed))\n",
    "print(f\"Save paths: {str(save_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511d01",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c1be3",
   "metadata": {},
   "source": [
    "## Test Set Evaluation (alternative to full fledged notebook eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c24d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * helper function for 'plot_multilabel_confusion_matrix'\n",
    "def convert_to_categories(elements):\n",
    "    # Convert vector of 0s and 1s to a single binary representation along the first dimension\n",
    "    binary_rep = np.apply_along_axis(\n",
    "        lambda x: \"\".join(map(str, x)), axis=1, arr=elements\n",
    "    )\n",
    "    return np.array([int(x, 2) for x in binary_rep])\n",
    "\n",
    "\n",
    "# * BBDOIA custom confusion matrix for concepts\n",
    "def plot_multilabel_confusion_matrix(\n",
    "    y_true, y_pred, class_names, title, save_path=None\n",
    "):\n",
    "    y_true_categories = convert_to_categories(y_true.astype(int))\n",
    "    y_pred_categories = convert_to_categories(y_pred.astype(int))\n",
    "\n",
    "    to_rtn_cm = confusion_matrix(y_true_categories, y_pred_categories)\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    num_classes = len(class_names)\n",
    "    num_rows = (num_classes + 4) // 5  # Calculate the number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_rows))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.subplot(num_rows, 5, i + 1)  # Set the subplot position\n",
    "        plt.imshow(cm[i], interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Class: {class_names[i]}\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"0\", \"1\"])\n",
    "        plt.yticks(tick_marks, [\"0\", \"1\"])\n",
    "\n",
    "        fmt = \".0f\"\n",
    "        thresh = cm[i].max() / 2.0\n",
    "        for j in range(cm[i].shape[0]):\n",
    "            for k in range(cm[i].shape[1]):\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    format(cm[i][j, k], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i][j, k] > thresh else \"black\",\n",
    "                )\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_total.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return to_rtn_cm\n",
    "\n",
    "\n",
    "# * Concept collapse (Soft)\n",
    "def compute_coverage(confusion_matrix):\n",
    "    \"\"\"Compute the coverage of a confusion matrix.\n",
    "\n",
    "    Essentially this metric is\n",
    "    \"\"\"\n",
    "\n",
    "    max_values = np.max(confusion_matrix, axis=0)\n",
    "    clipped_values = np.clip(max_values, 0, 1)\n",
    "\n",
    "    # Redefinition of soft coverage\n",
    "    coverage = np.sum(clipped_values) / len(clipped_values)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# * BDDOIA custom confusion matrix for actions\n",
    "def plot_actions_confusion_matrix(c_true, c_pred, title, save_path=None):\n",
    "\n",
    "    # Define scenarios and corresponding labels\n",
    "    my_scenarios = {\n",
    "        \"forward\": [slice(0, 3), slice(0, 3)],  \n",
    "        \"stop\": [slice(3, 9), slice(3, 9)],\n",
    "        \"left\": [slice(9, 11), slice(18,20)],\n",
    "        \"right\": [slice(12, 17), slice(12,17)],\n",
    "    }\n",
    "\n",
    "    to_rtn = {}\n",
    "\n",
    "    # Plot confusion matrix for each scenario\n",
    "    for scenario, indices in my_scenarios.items():\n",
    "\n",
    "        g_true = convert_to_categories(c_true[:, indices[0]].astype(int))\n",
    "        c_pred_scenario = convert_to_categories(c_pred[:, indices[1]].astype(int))\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(g_true, c_pred_scenario)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{title} - {scenario}\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        n_classes = c_true[:, indices[0]].shape[1]\n",
    "\n",
    "        tick_marks = np.arange(2**n_classes)\n",
    "        plt.xticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "        plt.yticks(tick_marks, [\"\" for _ in range(len(tick_marks))])\n",
    "\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or show plot\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{scenario}.png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        to_rtn.update({scenario: cm})\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return to_rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e79c9",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39a91b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['mnmath', 'xor', 'clipboia', 'shortmnist', 'restrictedmnist', 'minikandinsky', 'presddoia', 'prekandinsky', 'sddoia', 'clipkandinsky', 'addmnist', 'clipshortmnist', 'boia_original', 'boia_original_embedded', 'clipsddoia', 'boia', 'kandinsky', 'halfmnist']\n",
      "Available models: ['promnistltn', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'xordpl', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'mnistsl', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "<datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f6624637bb0>\n",
      "Using Dataset:  <datasets.boia_original_embedded.FasterBDDOIADataset object at 0x7f6624637bb0>\n",
      "Using backbone:  BOIAConceptizer(\n",
      "  (enc1): Linear(in_features=2048, out_features=21, bias=True)\n",
      ")\n",
      "Using Model:  BoiaDPL(\n",
      "  (encoder): BOIAConceptizer(\n",
      "    (enc1): Linear(in_features=2048, out_features=21, bias=True)\n",
      "  )\n",
      ")\n",
      "Using Loss:  SDDOIA_DPL()\n",
      "Dataset sizes - train: 16080, val: 2264, test: 4568\n",
      "Loaded datasets in 0.12s\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(args)\n",
    "n_images, c_split = dataset.get_split()\n",
    "\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "model.start_optim(args)\n",
    "loss = model.get_loss(args)\n",
    "\n",
    "print(dataset)\n",
    "print(\"Using Dataset: \", dataset)\n",
    "print(\"Using backbone: \", encoder)\n",
    "print(\"Using Model: \", model)\n",
    "print(\"Using Loss: \", loss)\n",
    "\n",
    "unsup_train_loader, unsup_val_loader, unsup_test_loader = dataset.get_data_loaders(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f8268",
   "metadata": {},
   "source": [
    "# PROTOTYPES CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20e70",
   "metadata": {},
   "source": [
    "## DATASET & BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fee336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert embeddings.shape[0] == labels.shape[0]\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01ef6c",
   "metadata": {},
   "source": [
    "## Build positive annotation set for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id: <class 'tuple'>\n",
      "images_embeddings_raw: torch.Size([2048])\n",
      "attr_labels: torch.Size([21])\n",
      "is_positive: <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "pos_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "target_per_class = 6\n",
    "debug = True\n",
    "\n",
    "# Loop over dataset until we collect target_per_class for each class\n",
    "for batch_idx, batch in enumerate(unsup_train_loader):\n",
    "    raw_embs = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "    attrs = torch.stack(batch['attr_labels']).to(model.device)  # shape [B,21]\n",
    "    batch_size = attrs.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        attr_vector = attrs[b].clone().cpu()  # clone to avoid in-place issues\n",
    "        for cls in torch.nonzero(attr_vector).flatten().tolist():\n",
    "            if len(pos_examples[cls]) >= target_per_class:\n",
    "                continue\n",
    "            example = {\n",
    "                'source_id': (batch_idx, b),\n",
    "                'images_embeddings_raw': raw_embs[b].detach().cpu().clone(),\n",
    "                'attr_labels': attr_vector,\n",
    "                'is_positive': True\n",
    "            }\n",
    "            if debug:\n",
    "                for key, value in example.items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        print(f\"{key}: {value.shape}\")\n",
    "                    elif isinstance(value, list) and len(value) and torch.is_tensor(value[0]):\n",
    "                        print(f\"{key}: list of {len(value)} tensors, first shape: {value[0].shape}\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {type(value)}\")\n",
    "                debug = False\n",
    "            pos_examples[cls].append(example)\n",
    "\n",
    "    # Check if all classes reached target\n",
    "    if all(len(pos_examples[c]) >= target_per_class for c in range(21)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddabe2f",
   "metadata": {},
   "source": [
    "## Augment positive sets while building negative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f972a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_examples = {cls_idx: [] for cls_idx in range(21)}\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 1 and ex['source_id'] not in seen_ids:\n",
    "                new_ex = ex.copy()\n",
    "                new_ex['is_positive'] = True\n",
    "                pos_examples[cls].append(new_ex)\n",
    "                seen_ids.add(ex['source_id'])\n",
    "\n",
    "for cls in range(21):\n",
    "    seen_ids_pos = {ex['source_id'] for ex in pos_examples[cls]}\n",
    "    for other_cls in range(21):\n",
    "        if other_cls == cls:\n",
    "            continue\n",
    "        for ex in pos_examples[other_cls]:\n",
    "            if ex['attr_labels'][cls] == 0 and ex['source_id'] not in seen_ids_pos:\n",
    "                neg_ex = ex.copy()\n",
    "                neg_ex['is_positive'] = False\n",
    "                neg_examples[cls].append(neg_ex)\n",
    "\n",
    "for cls in range(21):\n",
    "    assert not set(ex['source_id'] for ex in neg_examples[cls]) & set(ex['source_id'] for ex in pos_examples[cls]), \\\n",
    "        f\"Overlap in pos/neg for class {cls}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21dcb6",
   "metadata": {},
   "source": [
    "## Construct embeddings and labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34199a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: embeddings shape = torch.Size([185, 2048]), labels shape = torch.Size([185, 21])\n",
      "Class 1: embeddings shape = torch.Size([238, 2048]), labels shape = torch.Size([238, 21])\n",
      "Class 2: embeddings shape = torch.Size([199, 2048]), labels shape = torch.Size([199, 21])\n",
      "Class 3: embeddings shape = torch.Size([214, 2048]), labels shape = torch.Size([214, 21])\n",
      "Class 4: embeddings shape = torch.Size([244, 2048]), labels shape = torch.Size([244, 21])\n",
      "Class 5: embeddings shape = torch.Size([247, 2048]), labels shape = torch.Size([247, 21])\n",
      "Class 6: embeddings shape = torch.Size([257, 2048]), labels shape = torch.Size([257, 21])\n",
      "Class 7: embeddings shape = torch.Size([230, 2048]), labels shape = torch.Size([230, 21])\n",
      "Class 8: embeddings shape = torch.Size([254, 2048]), labels shape = torch.Size([254, 21])\n",
      "Class 9: embeddings shape = torch.Size([243, 2048]), labels shape = torch.Size([243, 21])\n",
      "Class 10: embeddings shape = torch.Size([219, 2048]), labels shape = torch.Size([219, 21])\n",
      "Class 11: embeddings shape = torch.Size([239, 2048]), labels shape = torch.Size([239, 21])\n",
      "Class 12: embeddings shape = torch.Size([244, 2048]), labels shape = torch.Size([244, 21])\n",
      "Class 13: embeddings shape = torch.Size([218, 2048]), labels shape = torch.Size([218, 21])\n",
      "Class 14: embeddings shape = torch.Size([234, 2048]), labels shape = torch.Size([234, 21])\n",
      "Class 15: embeddings shape = torch.Size([220, 2048]), labels shape = torch.Size([220, 21])\n",
      "Class 16: embeddings shape = torch.Size([218, 2048]), labels shape = torch.Size([218, 21])\n",
      "Class 17: embeddings shape = torch.Size([231, 2048]), labels shape = torch.Size([231, 21])\n",
      "Class 18: embeddings shape = torch.Size([199, 2048]), labels shape = torch.Size([199, 21])\n",
      "Class 19: embeddings shape = torch.Size([230, 2048]), labels shape = torch.Size([230, 21])\n",
      "Class 20: embeddings shape = torch.Size([244, 2048]), labels shape = torch.Size([244, 21])\n"
     ]
    }
   ],
   "source": [
    "dataset_per_class = {}\n",
    "for cls in range(21):\n",
    "    examples = pos_examples[cls] + neg_examples[cls]\n",
    "    emb_list, label_list = [], []\n",
    "    for ex in examples:\n",
    "        emb_list.append(ex['images_embeddings_raw'].unsqueeze(0))\n",
    "        label_list.append(ex['attr_labels'])\n",
    "    embeddings_tensor = torch.stack(emb_list).to(model.device)  # [N,1,2048]\n",
    "    labels_tensor = torch.stack(label_list)\n",
    "    dataset_per_class[cls] = {'embeddings': embeddings_tensor.squeeze(1), 'labels': labels_tensor}\n",
    "        \n",
    "for cls in range(21):\n",
    "    print(f\"Class {cls}: embeddings shape = {dataset_per_class[cls]['embeddings'].shape}, labels shape = {dataset_per_class[cls]['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fd11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat([per_class['embeddings'] for per_class in dataset_per_class.values()], dim=0)\n",
    "all_labels = torch.cat([per_class['labels'] for per_class in dataset_per_class.values()], dim=0)\n",
    "all_embeddings = all_embeddings.cpu()\n",
    "all_labels     = all_labels.cpu()\n",
    "dataset = ProtoDataset(all_embeddings, all_labels)\n",
    "nn_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,        \n",
    "    pin_memory=True       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b138d",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32b4d4",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e14769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: MnistDPL, \n",
    "        _loss: ADDMNIST_DPL,\n",
    "        save_path: str, \n",
    "        backbone_supervision_loader: DataLoader,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        args: Namespace,\n",
    "        eval_concepts: list = None,\n",
    "        seed: int = 0,\n",
    "    ) -> float:\n",
    "\n",
    "    # for full reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # early stopping\n",
    "    best_cacc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # scheduler & warmup (not used) for main model\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(model.opt, args.exp_decay)\n",
    "    w_scheduler = None\n",
    "    if args.warmup_steps > 0:\n",
    "        w_scheduler = GradualWarmupScheduler(model.opt, 1.0, args.warmup_steps)\n",
    "\n",
    "    fprint(\"\\n--- Start of Training ---\\n\")\n",
    "    model.to(model.device)\n",
    "    model.opt.zero_grad()\n",
    "    model.opt.step()\n",
    "\n",
    "    # & Training start\n",
    "    for epoch in range(args.n_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.n_epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # * Backbone supervision phase\n",
    "        print(\"Backbone supervision phase\")\n",
    "        for i, batch in enumerate(backbone_supervision_loader):\n",
    "            batch_embeds, batch_labels = batch\n",
    "            batch_embeds = batch_embeds.to(model.device)\n",
    "            batch_labels = batch_labels.to(model.device)\n",
    "\n",
    "            model.opt.zero_grad()\n",
    "            out_dict = model(batch_embeds)\n",
    "            concept_predictions = out_dict[\"CS\"]\n",
    "            loss = F.binary_cross_entropy(concept_predictions, batch_labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                progress_bar(i, len(backbone_supervision_loader), epoch, loss.item())\n",
    "\n",
    "        # * Unsupervised Training\n",
    "        print(\"Unsupervised training phase\")\n",
    "        ys, y_true, cs, cs_true, batch = None, None, None, None, 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            \n",
    "            # ------------------ original embneddings\n",
    "            images_embeddings = torch.stack(batch['embeddings']).to(model.device)\n",
    "            attr_labels = torch.stack(batch['attr_labels']).to(model.device)\n",
    "            class_labels = torch.stack(batch['class_labels'])[:,:-1].to(model.device)\n",
    "            # ------------------ my extracted features\n",
    "            images_embeddings_raw = torch.stack(batch['embeddings_raw']).to(model.device)\n",
    "            detected_rois = batch['rois']\n",
    "            detected_rois_feats = batch['roi_feats']\n",
    "            detection_labels = batch['detection_labels']\n",
    "            detection_scores = batch['detection_scores']\n",
    "            assert_inputs(images_embeddings, attr_labels, class_labels,\n",
    "                   detected_rois_feats, detected_rois, detection_labels,\n",
    "                   detection_scores, images_embeddings_raw)\n",
    "\n",
    "            out_dict = model(images_embeddings_raw)\n",
    "            out_dict.update({\"LABELS\": class_labels, \"CONCEPTS\": attr_labels})\n",
    "            \n",
    "            model.opt.zero_grad()\n",
    "            loss, losses = _loss(out_dict, args)\n",
    "\n",
    "            loss.backward()\n",
    "            model.opt.step()\n",
    "\n",
    "            if ys is None:\n",
    "                ys = out_dict[\"YS\"]\n",
    "                y_true = out_dict[\"LABELS\"]\n",
    "                cs = out_dict[\"pCS\"]\n",
    "                cs_true = out_dict[\"CONCEPTS\"]\n",
    "            else:\n",
    "                ys = torch.concatenate((ys, out_dict[\"YS\"]), dim=0)\n",
    "                y_true = torch.concatenate((y_true, out_dict[\"LABELS\"]), dim=0)\n",
    "                cs = torch.concatenate((cs, out_dict[\"pCS\"]), dim=0)\n",
    "                cs_true = torch.concatenate((cs_true, out_dict[\"CONCEPTS\"]), dim=0)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar(i, len(train_loader) - 9, epoch, loss.item())\n",
    "            \n",
    "        # ^ Evaluation phase\n",
    "        y_pred = torch.argmax(ys, dim=-1)\n",
    "        #print(\"Argmax predictions have shape: \", y_pred.shape)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        my_metrics = evaluate_metrics(\n",
    "                model=model, \n",
    "                loader=val_loader, \n",
    "                args=args,\n",
    "                eval_concepts=eval_concepts)\n",
    "        loss = my_metrics[0]\n",
    "        cacc = my_metrics[1]\n",
    "        yacc = my_metrics[2]\n",
    "        f1_y = my_metrics[3]\n",
    "       \n",
    "        # update at end of the epoch\n",
    "        if epoch < args.warmup_steps:   w_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            if hasattr(_loss, \"grade\"):\n",
    "                _loss.update_grade(epoch)\n",
    "\n",
    "        ### LOGGING ###\n",
    "        fprint(\"  ACC C\", cacc, \"  ACC Y\", yacc, \"F1 Y\", f1_y)\n",
    "        \n",
    "        if not args.tuning and cacc > best_cacc:\n",
    "            print(\"Saving...\")\n",
    "            # Update best F1 score\n",
    "            best_cacc = cacc\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with CACC score: {best_cacc}\")\n",
    "\n",
    "        elif cacc <= best_cacc:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    fprint(\"\\n--- End of Training ---\\n\")\n",
    "    return best_cacc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e6feb",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6353f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training model with seed 90\n",
      "Chosen device: cuda\n",
      "Saving model in folder:  ../NEW-outputs/bddoia/baseline/dpl/AUG-baseline/dpl_90.pth\n",
      "\n",
      "--- Start of Training ---\n",
      "\n",
      "Epoch 1/40\n",
      "Backbone supervision phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:36 ] epoch 0: |██████████████████████████████████████████████████| loss: 0.14971389"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:36 ] epoch 0: |██████████████████████████████████████████████████| loss: 4.81785444"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 83.6373421880934   ACC Y 75.01479640151516 F1 Y 42.72238936358323\n",
      "Saving...\n",
      "Saved best model with CACC score: 83.6373421880934\n",
      "Epoch 2/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:37 ] epoch 1: |██████████████████████████████████████████████████| loss: 4.82533073"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 84.74401897854275   ACC Y 64.82402146464646 F1 Y 47.40255317024655\n",
      "Saving...\n",
      "Saved best model with CACC score: 84.74401897854275\n",
      "Epoch 3/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:37 ] epoch 2: |██████████████████████████████████████████████████| loss: 4.74677896"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.03275435500674   ACC Y 74.86781881313132 F1 Y 41.30456367831612\n",
      "Epoch 4/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:38 ] epoch 3: |██████████████████████████████████████████████████| loss: 4.68217993"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.89521862400903   ACC Y 75.94006470959596 F1 Y 46.41540079846947\n",
      "Epoch 5/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:39 ] epoch 4: |██████████████████████████████████████████████████| loss: 4.73424292"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.13797211647034   ACC Y 75.85720486111111 F1 Y 46.04525393961713\n",
      "Epoch 6/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:39 ] epoch 5: |██████████████████████████████████████████████████| loss: 4.66406918"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 83.40886698828803   ACC Y 76.2784090909091 F1 Y 46.95623575959648\n",
      "Epoch 7/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:40 ] epoch 6: |██████████████████████████████████████████████████| loss: 4.63909912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.88995752069685   ACC Y 74.37361900252525 F1 Y 45.04080990821362\n",
      "Epoch 8/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:40 ] epoch 7: |██████████████████████████████████████████████████| loss: 4.70241833"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.64100183380975   ACC Y 76.11959438131312 F1 Y 45.92012233438964\n",
      "Epoch 9/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:41 ] epoch 8: |██████████████████████████████████████████████████| loss: 4.68407583"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.56415455871158   ACC Y 75.77335858585859 F1 Y 44.91526588115761\n",
      "Epoch 10/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:42 ] epoch 9: |██████████████████████████████████████████████████| loss: 4.70724583"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.60079353385501   ACC Y 75.81380208333333 F1 Y 49.295745888112634\n",
      "Epoch 11/40\n",
      "Backbone supervision phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:42 ] epoch 10: |██████████████████████████████████████████████████| loss: 4.47575808"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:42 ] epoch 10: |██████████████████████████████████████████████████| loss: 4.67466593"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 82.00419545173645   ACC Y 75.80492424242424 F1 Y 46.53357144078809\n",
      "Epoch 12/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:43 ] epoch 11: |██████████████████████████████████████████████████| loss: 4.65579891"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.98766112327576   ACC Y 76.04166666666667 F1 Y 50.50785844290034\n",
      "Epoch 13/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:44 ] epoch 12: |██████████████████████████████████████████████████| loss: 4.64660263"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.90179434087541   ACC Y 75.6411773989899 F1 Y 46.88779742555288\n",
      "Epoch 14/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:44 ] epoch 13: |██████████████████████████████████████████████████| loss: 4.68545532"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.5975997183058   ACC Y 76.06040877525253 F1 Y 50.51782801995492\n",
      "Epoch 15/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:45 ] epoch 14: |██████████████████████████████████████████████████| loss: 4.70203209"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.51774538887872   ACC Y 76.16792929292929 F1 Y 48.93949440607612\n",
      "Epoch 16/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:45 ] epoch 15: |██████████████████████████████████████████████████| loss: 4.59797192"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.89691007137299   ACC Y 76.25473484848484 F1 Y 49.52163358317719\n",
      "Epoch 17/40\n",
      "Backbone supervision phase\n",
      "Unsupervised training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 07-10 | 16:46 ] epoch 16: |██████████████████████████████████████████████████| loss: 4.62704086"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ACC C 81.52244289716084   ACC Y 76.20146780303031 F1 Y 50.75763306992858\n",
      "Early stopping triggered after 17 epochs.\n",
      "\n",
      "--- End of Training ---\n",
      "\n",
      "*** Finished training model with seed 90 and best CACC score 84.74401897854275\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Training model with seed {args.seed}\")\n",
    "print(\"Chosen device:\", model.device)\n",
    "if not os.path.exists(save_path): os.makedirs(save_path, exist_ok=True)\n",
    "save_folder = os.path.join(save_path, f\"{save_model_name}_{args.seed}.pth\")\n",
    "print(\"Saving model in folder: \", save_folder)\n",
    "\n",
    "eval_concepts = ['green_lights', 'follow_traffic', 'road_clear',\n",
    "        'traffic_lights', 'traffic_signs', 'cars', 'pedestrians', 'riders', 'others',\n",
    "        'no_lane_left', 'obstacle_left_lane', 'solid_left_line',\n",
    "                'on_right_turn_lane', 'traffic_light_right', 'front_car_right', \n",
    "        'no_lane_right', 'obstacle_right_lane', 'solid_right_line',\n",
    "                'on_left_turn_lane', 'traffic_light_left', 'front_car_left']\n",
    "\n",
    "best_cacc = train(\n",
    "        model=model,\n",
    "        backbone_supervision_loader=nn_loader,\n",
    "        train_loader=unsup_train_loader,\n",
    "        val_loader=unsup_val_loader,\n",
    "        save_path=save_folder,\n",
    "        _loss=loss,\n",
    "        args=args,\n",
    "        eval_concepts=eval_concepts,\n",
    "        seed=args.seed,\n",
    ")\n",
    "save_model(model, args, args.seed)  # save the model parameters\n",
    "print(f\"*** Finished training model with seed {args.seed} and best CACC score {best_cacc}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb08277",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a2e9e",
   "metadata": {},
   "source": [
    "## Evaluation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_my_model(model: MnistDPL, \n",
    "        save_path: str, \n",
    "        test_loader: DataLoader,\n",
    "        eval_concepts,\n",
    "    ):\n",
    "    \n",
    "    my_metrics = evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,)\n",
    "    \n",
    "    loss = my_metrics[0]\n",
    "    cacc = my_metrics[1]\n",
    "    yacc = my_metrics[2]\n",
    "    f1_y = my_metrics[3]\n",
    "    f1_micro = my_metrics[4]\n",
    "    f1_weight = my_metrics[5]\n",
    "    f1_bin = my_metrics[6]\n",
    "\n",
    "    metrics_log_path = save_path.replace(\".pth\", \"_metrics.log\")\n",
    "    \n",
    "    all_concepts = [ 'Green Traffic Light', 'Follow Traffic', 'Road Is Clear',\n",
    "        'Red Traffic Light', 'Traffic Sign', 'Obstacle Car', 'Obstacle Pedestrian', 'Obstacle Rider', 'Obstacle Others',\n",
    "        'No Lane On The Left',  'Obstacle On The Left Lane',  'Solid Left Line',\n",
    "                'On The Right Turn Lane', 'Traffic Light Allows Right', 'Front Car Turning Right', \n",
    "        'No Lane On The Right', 'Obstacle On The Right Lane', 'Solid Right Line',\n",
    "                'On The Left Turn Lane',  'Traffic Light Allows Left',  'Front Car Turning Left' \n",
    "    ]\n",
    "    aggregated_metrics = [\n",
    "            'F1 - Binary', 'F1 - Macro', 'F1 - Micro', 'F1 - Weighted',\n",
    "            'Precision - Binary', 'Precision - Macro', 'Precision - Micro', 'Precision - Weighted',\n",
    "            'Recall - Binary', 'Recall - Macro', 'Recall - Micro', 'Recall - Weighted',\n",
    "            'Balanced Accuracy'\n",
    "    ]\n",
    "\n",
    "    sums = [0.0] * len(aggregated_metrics)\n",
    "    num_concepts = len(all_concepts)\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"ACC C: {cacc}, ACC Y: {yacc}\\n\\n\")\n",
    "        log_file.write(f\"F1 Y - Macro: {f1_y}, F1 Y - Micro: {f1_micro}, F1 Y - Weighted: {f1_weight}, F1 Y - Binary: {f1_bin} \\n\\n\")\n",
    "\n",
    "        def write_metrics(class_name, offset):\n",
    "            print(f\"Metrics for {class_name}:\")\n",
    "            log_file.write(f\"{class_name.upper()}\\n\")\n",
    "            for idx, metric_name in enumerate(aggregated_metrics):\n",
    "                value = my_metrics[offset + idx]\n",
    "                sums[idx] += value\n",
    "                log_file.write(f\"  {metric_name:<18} {value:.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "        i = 7\n",
    "        for concept in all_concepts:\n",
    "            write_metrics(concept, i)\n",
    "            i += len(aggregated_metrics)\n",
    "\n",
    "        log_file.write(\"MEAN ACROSS ALL CONCEPTS\\n\")\n",
    "        for idx, metric_name in enumerate(aggregated_metrics):\n",
    "            mean_value = sums[idx] / num_concepts\n",
    "            log_file.write(f\"  {metric_name:<18} {mean_value:.4f}\\n\")\n",
    "        log_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "    assert len(my_metrics) == 7 + len(all_concepts) * len(aggregated_metrics), \\\n",
    "        f\"Expected {7 + len(all_concepts) * len(aggregated_metrics)} metrics, but got {len(my_metrics)}\"\n",
    "    y_true, c_true, y_pred, c_pred, p_cs, p_ys, p_cs_all, p_ys_all = (\n",
    "        evaluate_metrics(model, test_loader, args, \n",
    "                    eval_concepts=eval_concepts,\n",
    "                    last=True\n",
    "            )\n",
    "    )\n",
    "    y_labels = [\"stop\", \"forward\", \"left\", \"right\"]\n",
    "    concept_labels = [\n",
    "        \"green_light\",      \n",
    "        \"follow\",           \n",
    "        \"road_clear\",       \n",
    "        \"red_light\",        \n",
    "        \"traffic_sign\",     \n",
    "        \"car\",              \n",
    "        \"person\",           \n",
    "        \"rider\",            \n",
    "        \"other_obstacle\",   \n",
    "        \"left_lane\",\n",
    "        \"left_green_light\",\n",
    "        \"left_follow\",\n",
    "        \"no_left_lane\",\n",
    "        \"left_obstacle\",\n",
    "        \"letf_solid_line\",\n",
    "        \"right_lane\",\n",
    "        \"right_green_light\",\n",
    "        \"right_follow\",\n",
    "        \"no_right_lane\",\n",
    "        \"right_obstacle\",\n",
    "        \"right_solid_line\",\n",
    "    ]\n",
    "\n",
    "    plot_multilabel_confusion_matrix(y_true, y_pred, y_labels, \"Labels\", save_path=save_path)\n",
    "    cfs = plot_actions_confusion_matrix(c_true, c_pred, \"Concepts\", save_path=save_path)\n",
    "    cf = plot_multilabel_confusion_matrix(c_true, c_pred, concept_labels, \"Concepts\", save_path=save_path)\n",
    "    print(\"Concept collapse\", 1 - compute_coverage(cf))\n",
    "\n",
    "    with open(metrics_log_path, \"a\") as log_file:\n",
    "        for key, value in cfs.items():\n",
    "            log_file.write(f\"Concept collapse: {key}, {1 - compute_coverage(value):.4f}\\n\")\n",
    "            log_file.write(\"\\n\")\n",
    "\n",
    "    fprint(\"\\n--- End of Evaluation ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87947313",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2806065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['promnistltn', 'sddoiann', 'kandnn', 'sddoiadpl', 'sddoialtn', 'presddoiadpl', 'boiann', 'mnistclip', 'prokanddpl', 'promnistdpl', 'xornn', 'mnistnn', 'mnistslrec', 'kandpreprocess', 'kandsl', 'kandsloneembedding', 'prokandltn', 'kandcbm', 'prokandsl', 'boiacbm', 'kanddpl', 'kandltn', 'xorcbm', 'sddoiaclip', 'xordpl', 'sddoiacbm', 'mnistltnrec', 'mnmathcbm', 'mnmathdpl', 'kandclip', 'minikanddpl', 'mnistdpl', 'mnistltn', 'boiadpl', 'boialtn', 'prokandsloneembedding', 'mnistpcbmdpl', 'mnistcbm', 'probddoiadpl', 'mnistpcbmsl', 'mnistpcbmltn', 'mnistsl', 'mnistdplrec', 'cvae', 'cext', 'mnmathnn', 'promnistsl']\n",
      "Metrics for Green Traffic Light:\n",
      "Metrics for Follow Traffic:\n",
      "Metrics for Road Is Clear:\n",
      "Metrics for Red Traffic Light:\n",
      "Metrics for Traffic Sign:\n",
      "Metrics for Obstacle Car:\n",
      "Metrics for Obstacle Pedestrian:\n",
      "Metrics for Obstacle Rider:\n",
      "Metrics for Obstacle Others:\n",
      "Metrics for No Lane On The Left:\n",
      "Metrics for Obstacle On The Left Lane:\n",
      "Metrics for Solid Left Line:\n",
      "Metrics for On The Right Turn Lane:\n",
      "Metrics for Traffic Light Allows Right:\n",
      "Metrics for Front Car Turning Right:\n",
      "Metrics for No Lane On The Right:\n",
      "Metrics for Obstacle On The Right Lane:\n",
      "Metrics for Solid Right Line:\n",
      "Metrics for On The Left Turn Lane:\n",
      "Metrics for Traffic Light Allows Left:\n",
      "Metrics for Front Car Turning Left:\n",
      "Concept collapse 0.9828178694158075\n",
      "\n",
      "--- End of Evaluation ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model object\n",
    "model = get_model(args, encoder, decoder, n_images, c_split)\n",
    "\n",
    "# Load the model state dictionary into the model object\n",
    "model_state_dict = torch.load(save_folder)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "evaluate_my_model(model, save_folder, unsup_test_loader, eval_concepts=eval_concepts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
