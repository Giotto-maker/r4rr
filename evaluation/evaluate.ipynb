{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_parameter_name = None\n",
    "dataset_parameter_name = None\n",
    "my_task = None\n",
    "uns_parameter_percentage = None\n",
    "NA = False       # do not use augmentations for mnist and kandinsky\n",
    "hide_parameter = None\n",
    "hide_shapes_parameter = None  \n",
    "hide_colors_parameter = None  \n",
    "GPU_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papermill model name is: shieldedmnist\n",
      "Papermill uns_parameter_percentage is: 1.0\n",
      "Papermill dataset name is: shortmnist\n",
      "Papermill hide is: None\n",
      "Papermill hide_shapes is: None\n",
      "Papermill hide_colors is: None\n",
      "Papermill dataset name is: shortmnist\n",
      "Papermill GPU_ID is: 1\n"
     ]
    }
   ],
   "source": [
    "assert model_parameter_name is not None, \"model_parameter_name should not be None\"\n",
    "assert isinstance(model_parameter_name, str), \"model_parameter_name should be a string\"\n",
    "\n",
    "assert uns_parameter_percentage is not None, \"uns_parameter_percentage should not be None\"\n",
    "assert isinstance(uns_parameter_percentage, float), \"uns_parameter_percentage should be a float\"\n",
    "assert 0.0 <= uns_parameter_percentage <= 1.0, \"uns_parameter_percentage should be in the range [0.0, 1.0]\"\n",
    "\n",
    "assert dataset_parameter_name is not None, \"dataset_parameter_name should not be None\"\n",
    "assert isinstance(dataset_parameter_name, str), \"dataset_parameter_name should be a string\"\n",
    "\n",
    "#assert hide_parameter is not None, \"hide_parameter should not be None\"\n",
    "#assert isinstance(hide_parameter, list), \"hide_parameter should be a list\"\n",
    "\n",
    "assert GPU_ID is not None, \"GPU_ID should not be None\"\n",
    "assert isinstance(GPU_ID, str), \"GPU_ID should be a string\"\n",
    "\n",
    "print(\"Papermill model name is: \" + model_parameter_name)\n",
    "print(\"Papermill uns_parameter_percentage is: \" + str(uns_parameter_percentage))\n",
    "print(\"Papermill dataset name is: \" + dataset_parameter_name)\n",
    "print(\"Papermill hide is: \" + str(hide_parameter))\n",
    "print(\"Papermill hide_shapes is: \" + str(hide_shapes_parameter))\n",
    "print(\"Papermill hide_colors is: \" + str(hide_colors_parameter))\n",
    "print(\"Papermill dataset name is: \" + dataset_parameter_name)\n",
    "print(\"Papermill GPU_ID is: \" + GPU_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "#if hide_colors_parameter is None or hide_shapes_parameter is None:\n",
    "#    sys.exit(\"Bye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from utils.train import convert_to_categories, compute_coverage, compute_coverage_hard\n",
    "from datasets.boia import BOIA\n",
    "from datasets.boia_original import BDDDataset\n",
    "from datasets.boia_original_embedded import FasterBDDOIADataset\n",
    "from datasets.sddoia import SDDOIA\n",
    "from datasets.minikandinsky import MiniKandinsky\n",
    "from datasets.kandinsky import Kandinsky\n",
    "from datasets.shortcutmnist import SHORTMNIST\n",
    "from datasets.clipkandinsky import CLIPKandinsky\n",
    "from datasets.clipshortcutmnist import CLIPSHORTMNIST\n",
    "from datasets.clipboia import CLIPBOIA\n",
    "#from datasets.clipSDDOIA import CLIPSDDOIA\n",
    "from models.boiadpl import BoiaDPL\n",
    "#from models.SDDOIAdpl import SDDOIADPL\n",
    "from models.boialtn import BOIALTN\n",
    "#from models.SDDOIAltn import SDDOIALTN\n",
    "from models.boiann import BOIAnn\n",
    "#from models.SDDOIAnn import SDDOIAnn\n",
    "#from models.SDDOIAcbm import SDDOIACBM\n",
    "from models.boiacbm import BoiaCBM\n",
    "from models.mnistcbm import MnistCBM\n",
    "from models.mnistdpl import MnistDPL\n",
    "from models.mnistltn import MnistLTN\n",
    "from models.mnistnn import MNISTnn\n",
    "from models.mnistsl import MnistSL\n",
    "from models.shieldedmnist import ShieldedMNIST\n",
    "from models.proshieldedmnist import ProShieldedMNIST\n",
    "from models.promnistsl import PROMnistSL\n",
    "from models.promnistltn import PROMnistLTN\n",
    "from models.promnistdpl import PROMnistDPL\n",
    "from models.minikanddpl import MiniKandDPL\n",
    "from models.prokanddpl import ProKandDPL\n",
    "from models.prokandsl import ProKandSL\n",
    "from models.prokandltn import ProKandLTN\n",
    "from models.kanddpl import KandDPL\n",
    "from models.kanddplsinglejoint import KandDPLSingleJoint\n",
    "from models.kanddplsingledisj import KandDPLSingleDisj\n",
    "from models.kandltnsinglejoint import KandLTNSingleJoint\n",
    "from models.kandltnsingledisj import KandLTNSingleDisj\n",
    "from models.kandslsinglejoint import KandSLSingleJoint\n",
    "from models.kandslsingledisj import KandSLSingleDisj\n",
    "from models.kandcbm import KandCBM\n",
    "from models.kandltn import KANDltn\n",
    "from models.kandnn import KANDnn\n",
    "from models.kandsl import KandSL\n",
    "from models.mnistltnrec import MnistLTNRec\n",
    "from models.mnistdplrec import MnistDPLRec\n",
    "from models.mnistslrec import MnistSLRec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shortcut_mitigation.kandinsky.protonet_kand_modules.data_modules.proto_data_creation import get_random_classes\n",
    "from shortcut_mitigation.mnist.protonet_mnist_add_modules.data_modules.proto_data_creation import (\n",
    "    choose_initial_prototypes, \n",
    "    get_augmented_support_query_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.13.0+cu117\n",
      "CUDA version:  11.7\n",
      "Number of GPUs available: 1\n",
      "Device 0: NVIDIA TITAN Xp\n",
      "  Memory Allocated: 0 bytes\n",
      "  Memory Cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "def my_gpu_info():\n",
    "    print(\"Torch version: \", torch.__version__)  \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA version: \", torch.version.cuda)\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)} bytes\")\n",
    "            print(f\"  Memory Cached: {torch.cuda.memory_reserved(i)} bytes\")\n",
    "    else:\n",
    "        print(\"CUDA is not available on this system.\")\n",
    "\n",
    "my_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class containing all the metrics which we are evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        avg_nll,\n",
    "    ):\n",
    "        self.concept_accuracy = concept_accuracy\n",
    "        self.label_accuracy = label_accuracy\n",
    "        self.concept_f1_macro = concept_f1_macro\n",
    "        self.concept_f1_micro = concept_f1_micro\n",
    "        self.concept_f1_weighted = concept_f1_weighted\n",
    "        self.label_f1_macro = label_f1_macro\n",
    "        self.label_f1_micro = label_f1_micro\n",
    "        self.label_f1_weighted = label_f1_weighted\n",
    "        self.collapse = collapse\n",
    "        self.collapse_hard = collapse_hard\n",
    "        self.avg_nll = avg_nll\n",
    "\n",
    "\n",
    "class BOIAMetrics(Metrics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        collapse_forward,\n",
    "        collapse_stop,\n",
    "        collapse_left,\n",
    "        collapse_right,\n",
    "        collapse_hard_forward,\n",
    "        collapse_hard_stop,\n",
    "        collapse_hard_left,\n",
    "        collapse_hard_right,\n",
    "        mean_collapse,\n",
    "        mean_hard_collapse,\n",
    "        avg_nll,\n",
    "    ):\n",
    "        super(BOIAMetrics, self).__init__(\n",
    "            concept_accuracy,\n",
    "            label_accuracy,\n",
    "            concept_f1_macro,\n",
    "            concept_f1_micro,\n",
    "            concept_f1_weighted,\n",
    "            label_f1_macro,\n",
    "            label_f1_micro,\n",
    "            label_f1_weighted,\n",
    "            collapse,\n",
    "            collapse_hard,\n",
    "            avg_nll,\n",
    "        )\n",
    "        self.collapse_forward = collapse_forward\n",
    "        self.collapse_stop = collapse_stop\n",
    "        self.collapse_left = collapse_left\n",
    "        self.collapse_right = collapse_right\n",
    "        self.collapse_hard_forward = collapse_hard_forward\n",
    "        self.collapse_hard_stop = collapse_hard_stop\n",
    "        self.collapse_hard_left = collapse_hard_left\n",
    "        self.collapse_hard_right = collapse_hard_right\n",
    "        self.mean_collapse = mean_collapse\n",
    "        self.mean_hard_collapse = mean_hard_collapse\n",
    "\n",
    "\n",
    "class KandMetrics(Metrics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_accuracy,\n",
    "        label_accuracy,\n",
    "        concept_f1_macro,\n",
    "        concept_f1_micro,\n",
    "        concept_f1_weighted,\n",
    "        label_f1_macro,\n",
    "        label_f1_micro,\n",
    "        label_f1_weighted,\n",
    "        collapse,\n",
    "        collapse_hard,\n",
    "        avg_nll,\n",
    "        collapse_shapes,\n",
    "        collapse_hard_shapes,\n",
    "        collapse_color,\n",
    "        collapse_hard_color,\n",
    "        mean_collapse,\n",
    "        mean_collapse_hard,\n",
    "    ):\n",
    "        super(KandMetrics, self).__init__(\n",
    "            concept_accuracy,\n",
    "            label_accuracy,\n",
    "            concept_f1_macro,\n",
    "            concept_f1_micro,\n",
    "            concept_f1_weighted,\n",
    "            label_f1_macro,\n",
    "            label_f1_micro,\n",
    "            label_f1_weighted,\n",
    "            collapse,\n",
    "            collapse_hard,\n",
    "            avg_nll,\n",
    "        )\n",
    "        self.collapse_shapes = collapse_shapes\n",
    "        self.collapse_hard_shapes = collapse_hard_shapes\n",
    "        self.collapse_color = collapse_color\n",
    "        self.collapse_hard_color = collapse_hard_color\n",
    "        self.mean_collapse = mean_collapse\n",
    "        self.mean_collapse_hard = mean_collapse_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute the concept collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concept_collapse(true_concepts, predicted_concepts, multilabel=False):\n",
    "    if multilabel:\n",
    "        true_concepts = convert_to_categories(true_concepts.astype(int))\n",
    "        predicted_concepts = convert_to_categories(predicted_concepts.astype(int))\n",
    "\n",
    "    return 1 - compute_coverage(confusion_matrix(true_concepts, predicted_concepts))\n",
    "\n",
    "\n",
    "def compute_hard_concept_collapse(true_concepts, predicted_concepts, multilabel=False):\n",
    "    if multilabel:\n",
    "        true_concepts = convert_to_categories(true_concepts.astype(int))\n",
    "        predicted_concepts = convert_to_categories(predicted_concepts.astype(int))\n",
    "\n",
    "    return 1 - compute_coverage_hard(\n",
    "        confusion_matrix(true_concepts, predicted_concepts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=None,\n",
    "    is_boia=False,\n",
    "    cmap=plt.cm.Oranges,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = np.zeros((len(classes), len(classes)))\n",
    "    for i in range(len(true_labels)):\n",
    "        cm[true_labels[i], predicted_labels[i]] += 1\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\")\n",
    "        row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.where(row_sums == 0, 0, cm / row_sums)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.8)\n",
    "    red_yellow_palette = sns.color_palette(\"OrRd\", as_cmap=True)\n",
    "    heatmap = sns.heatmap(\n",
    "        cm,\n",
    "        annot=False,\n",
    "        fmt=\".2f\" if normalize else \"d\",\n",
    "        cmap=red_yellow_palette,\n",
    "        cbar=True,\n",
    "    )\n",
    "\n",
    "    if title:\n",
    "        plt.savefig(title, format=\"pdf\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! new method to express concepts in the confusion matrix for KAND\n",
    "def encode_labels_KAND(tensor):\n",
    "    \"\"\"\n",
    "    Encodes (shape, color) combinations into a single scalar index.\n",
    "    Args:\n",
    "        tensor: Input tensor with shape (N, 6), where the first 3 columns are shapes\n",
    "                and the last 3 columns are colors.\n",
    "    Returns:\n",
    "        Encoded labels as a 1D array.\n",
    "    \"\"\"\n",
    "    shape_indices = tensor[:, :3]  # First 3 columns represent shapes\n",
    "    color_indices = tensor[:, 3:]  # Last 3 columns represent colors\n",
    "\n",
    "    # Combine each shape-color pair into a single label index\n",
    "    encoded_labels = []\n",
    "    for i in range(tensor.shape[0]):  # Loop over each sample\n",
    "        for shape, color in zip(shape_indices[i], color_indices[i]):\n",
    "            # Map (shape, color) -> unique index\n",
    "            label = 3 * shape.item() + color.item()  # Shape: {0,1,2}, Color: {0,1,2}\n",
    "            encoded_labels.append(label)\n",
    "    return np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_KAND(true_concepts, predicted_concepts, classes, class_labels, normalize=True,\n",
    "                               title=None, cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix with custom class labels.\n",
    "\n",
    "    Args:\n",
    "        true_concepts: Array of true concept indices.\n",
    "        predicted_concepts: Array of predicted concept indices.\n",
    "        classes: List (or array) of unique classes.\n",
    "        class_labels: List of custom class labels corresponding to the classes.\n",
    "        normalize: Whether to normalize the values.\n",
    "        title: Optional title for the confusion matrix. If provided, the figure is saved as a PDF.\n",
    "        cmap: Colormap to use for the heatmap.\n",
    "    \"\"\"\n",
    "    # Create the confusion matrix\n",
    "    cm = np.zeros((len(classes), len(classes)))\n",
    "    for i in range(len(true_concepts)):\n",
    "        cm[true_concepts[i], predicted_concepts[i]] += 1\n",
    "\n",
    "    # Normalize if needed\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\")\n",
    "        row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.where(row_sums == 0, 0, cm / row_sums)\n",
    "    \n",
    "    # Ensure that only an extra empty row/column is removed while keeping all valid labels\n",
    "    if cm.shape[0] > len(class_labels):  # If there's an extra blank row/column\n",
    "        if np.all(cm[-1, :] == 0) and np.all(cm[:, -1] == 0):  # Check if last row and column are empty\n",
    "            cm = cm[:-1, :-1]  # Remove the last row and column\n",
    "\n",
    "    # Plotting setup\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    heatmap = sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\".2f\" if normalize else \"d\",\n",
    "        cmap=cmap,\n",
    "        cbar=True,\n",
    "        xticklabels=class_labels[:cm.shape[1]],  # Keep all valid labels\n",
    "        yticklabels=class_labels[:cm.shape[0]],\n",
    "        linewidths=0.5,\n",
    "        linecolor='lightgrey'\n",
    "    )\n",
    "    # Rotate x-axis labels to 45° for better readability\n",
    "    heatmap.set_xticklabels(class_labels[:cm.shape[1]], rotation=45, fontsize=10)\n",
    "    heatmap.set_yticklabels(class_labels[:cm.shape[0]], rotation=0, fontsize=10)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title, fontsize=12)\n",
    "        # Save the figure with tight bounding box to prevent label cutoff\n",
    "        plt.savefig(title, format=\"pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    # Increase bottom margin to ensure x labels are visible\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    true_concepts,\n",
    "    predicted_concepts,\n",
    "    avg_nll,\n",
    "    dataset_name,\n",
    "    model_name,\n",
    "    seed,\n",
    "    save_name\n",
    "):\n",
    "\n",
    "    # multilabel or not\n",
    "    multilabel_concept = False\n",
    "    multilabel_label = False\n",
    "\n",
    "    if dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        multilabel_concept = True\n",
    "        multilabel_label = True\n",
    "\n",
    "    if dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "        collapse_true_concepts_list = torch.tensor(true_concepts)\n",
    "        collapse_true_concepts_list = torch.split(collapse_true_concepts_list, 3, dim=1)\n",
    "        collapse_pred_concepts_list = torch.tensor(predicted_concepts)\n",
    "        collapse_pred_concepts_list = torch.split(collapse_pred_concepts_list, 3, dim=1)\n",
    "\n",
    "        collapse_true_concepts_1 = collapse_true_concepts_list[0].flatten()\n",
    "        collapse_true_concepts_2 = collapse_true_concepts_list[1].flatten()\n",
    "        collapse_true_concepts = torch.stack(\n",
    "            (collapse_true_concepts_1, collapse_true_concepts_2), dim=1\n",
    "        )\n",
    "        # to int\n",
    "        collapse_true_concepts = (\n",
    "            collapse_true_concepts[:, 0] * 3 + collapse_true_concepts[:, 1]\n",
    "        )\n",
    "        collapse_true_concepts = collapse_true_concepts.detach().numpy()\n",
    "\n",
    "        collapse_pred_concepts_1 = collapse_pred_concepts_list[0].flatten()\n",
    "        collapse_pred_concepts_2 = collapse_pred_concepts_list[1].flatten()\n",
    "        collapse_pred_concepts = torch.stack(\n",
    "            (collapse_pred_concepts_1, collapse_pred_concepts_2), dim=1\n",
    "        )\n",
    "        # to int\n",
    "        collapse_pred_concepts = (\n",
    "            collapse_pred_concepts[:, 0] * 3 + collapse_pred_concepts[:, 1]\n",
    "        )\n",
    "        collapse_pred_concepts = collapse_pred_concepts.detach().numpy()\n",
    "\n",
    "        # total collapse\n",
    "        collapse = compute_concept_collapse(\n",
    "            collapse_true_concepts, collapse_pred_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "        collapse_hard = compute_hard_concept_collapse(\n",
    "            collapse_true_concepts, collapse_pred_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # total collapse\n",
    "        collapse = compute_concept_collapse(\n",
    "            true_concepts, predicted_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "        collapse_hard = compute_hard_concept_collapse(\n",
    "            true_concepts, predicted_concepts, multilabel_concept\n",
    "        )\n",
    "\n",
    "    if dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        # additional metrics for boia and sddoia\n",
    "        collapse_forward, collapse_hard_forward = compute_concept_collapse(\n",
    "            true_concepts[:, :3], predicted_concepts[:, :3], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, :3], predicted_concepts[:, :3], True\n",
    "        )\n",
    "        collapse_stop, collapse_hard_stop = compute_concept_collapse(\n",
    "            true_concepts[:, 3:9], predicted_concepts[:, 3:9], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 3:9], predicted_concepts[:, 3:9], True\n",
    "        )\n",
    "        collapse_left, collapse_hard_left = compute_concept_collapse(\n",
    "            true_concepts[:, 9:15], predicted_concepts[:, 9:15], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 9:15], predicted_concepts[:, 9:15], True\n",
    "        )\n",
    "        collapse_right, collapse_hard_right = compute_concept_collapse(\n",
    "            true_concepts[:, 15:21], predicted_concepts[:, 15:21], True\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 15:21], predicted_concepts[:, 15:21], True\n",
    "        )\n",
    "\n",
    "        mean_collapse, mean_hard_collapse = np.mean(\n",
    "            [collapse_forward, collapse_stop, collapse_left, collapse_right]\n",
    "        ), np.mean(\n",
    "            [\n",
    "                collapse_hard_forward,\n",
    "                collapse_hard_stop,\n",
    "                collapse_hard_left,\n",
    "                collapse_hard_right,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    elif dataset_name in [\"minikandinsky\", \"kandinsky\", \"clipkandinsky\"]:\n",
    "        # additional metrics for boia and sddoia\n",
    "        collapse_color, collapse_hard_color = compute_concept_collapse(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            False,\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            False,\n",
    "        )\n",
    "        collapse_shapes, collapse_hard_shapes = compute_concept_collapse(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            False,\n",
    "        ), compute_hard_concept_collapse(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        mean_collapse, mean_collapse_hard = np.mean(\n",
    "            [collapse_color, collapse_shapes]\n",
    "        ), np.mean([collapse_hard_color, collapse_hard_shapes])\n",
    "\n",
    "    if multilabel_concept:\n",
    "        concept_accuracy, concept_f1_macro, concept_f1_micro, concept_f1_weighted = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        for i in range(true_concepts.shape[1]):\n",
    "            print(true_concepts[i])\n",
    "            print(predicted_concepts[i])\n",
    "            #raise RuntimeError(\"Multilabel concept not implemented\")\n",
    "            concept_accuracy += accuracy_score(true_concepts[i], predicted_concepts[i])\n",
    "            concept_f1_macro += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"macro\"\n",
    "            )\n",
    "            concept_f1_micro += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"micro\"\n",
    "            )\n",
    "            concept_f1_weighted += f1_score(\n",
    "                true_concepts[i], predicted_concepts[i], average=\"weighted\"\n",
    "            )\n",
    "\n",
    "        print(true_concepts.shape[1])\n",
    "        concept_accuracy = concept_accuracy / true_concepts.shape[1]\n",
    "        concept_f1_macro = concept_f1_macro / true_concepts.shape[1]\n",
    "        concept_f1_micro = concept_f1_micro / true_concepts.shape[1]\n",
    "        concept_f1_weighted = concept_f1_weighted / true_concepts.shape[1]\n",
    "\n",
    "        label_accuracy, label_f1_macro, label_f1_micro, label_f1_weighted = 0, 0, 0, 0\n",
    "    elif dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "        concept_accuracy_color = accuracy_score(\n",
    "            true_concepts[:, 3:6].reshape(-1), predicted_concepts[:, 3:6].reshape(-1)\n",
    "        )\n",
    "        concept_f1_macro_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        concept_f1_micro_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"micro\",\n",
    "        )\n",
    "        concept_f1_weighted_color = f1_score(\n",
    "            true_concepts[:, 3:6].reshape(-1),\n",
    "            predicted_concepts[:, 3:6].reshape(-1),\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        concept_accuracy_shape = accuracy_score(\n",
    "            true_concepts[:, :3].reshape(-1), predicted_concepts[:, :3].reshape(-1)\n",
    "        )\n",
    "        concept_f1_macro_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        concept_f1_micro_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"micro\",\n",
    "        )\n",
    "        concept_f1_weighted_shape = f1_score(\n",
    "            true_concepts[:, :3].reshape(-1),\n",
    "            predicted_concepts[:, :3].reshape(-1),\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        concept_accuracy = np.mean([concept_accuracy_color, concept_accuracy_shape])\n",
    "        concept_f1_macro = np.mean([concept_f1_macro_color, concept_f1_macro_shape])\n",
    "        concept_f1_micro = np.mean([concept_f1_micro_color, concept_f1_micro_shape])\n",
    "        concept_f1_weighted = np.mean(\n",
    "            [concept_f1_weighted_color, concept_f1_weighted_shape]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        concept_accuracy = accuracy_score(true_concepts, predicted_concepts)\n",
    "        concept_f1_macro = f1_score(true_concepts, predicted_concepts, average=\"macro\")\n",
    "        concept_f1_micro = f1_score(true_concepts, predicted_concepts, average=\"micro\")\n",
    "        concept_f1_weighted = f1_score(\n",
    "            true_concepts, predicted_concepts, average=\"weighted\"\n",
    "        )\n",
    "\n",
    "    if multilabel_label:\n",
    "        for i in range(true_labels.shape[1]):\n",
    "            label_accuracy += accuracy_score(true_labels[i], predicted_labels[i])\n",
    "            label_f1_macro += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"macro\"\n",
    "            )\n",
    "            label_f1_micro += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"micro\"\n",
    "            )\n",
    "            label_f1_weighted += f1_score(\n",
    "                true_labels[i], predicted_labels[i], average=\"weighted\"\n",
    "            )\n",
    "\n",
    "        label_accuracy = label_accuracy / true_labels.shape[1]\n",
    "        label_f1_macro = label_f1_macro / true_labels.shape[1]\n",
    "        label_f1_micro = label_f1_micro / true_labels.shape[1]\n",
    "        label_f1_weighted = label_f1_weighted / true_labels.shape[1]\n",
    "    else:\n",
    "        label_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        label_f1_macro = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "        label_f1_micro = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
    "        label_f1_weighted = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    if dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "        metrics = BOIAMetrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            collapse_forward=collapse_forward,\n",
    "            collapse_stop=collapse_stop,\n",
    "            collapse_right=collapse_right,\n",
    "            collapse_left=collapse_left,\n",
    "            collapse_hard_forward=collapse_hard_forward,\n",
    "            collapse_hard_stop=collapse_hard_stop,\n",
    "            collapse_hard_right=collapse_hard_right,\n",
    "            collapse_hard_left=collapse_hard_left,\n",
    "            mean_collapse=mean_collapse,\n",
    "            mean_hard_collapse=mean_hard_collapse,\n",
    "            avg_nll=avg_nll,\n",
    "        )\n",
    "    elif dataset_name in [\"minikandinsky\", \"kandinsky\", \"clipkandinsky\"]:\n",
    "        metrics = KandMetrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            avg_nll=avg_nll,\n",
    "            collapse_shapes=collapse_shapes,\n",
    "            collapse_color=collapse_color,\n",
    "            collapse_hard_shapes=collapse_hard_shapes,\n",
    "            mean_collapse_hard=mean_collapse_hard,\n",
    "            mean_collapse=mean_collapse,\n",
    "            collapse_hard_color=collapse_hard_color,\n",
    "        )\n",
    "    else:\n",
    "        metrics = Metrics(\n",
    "            concept_accuracy=concept_accuracy,\n",
    "            label_accuracy=label_accuracy,\n",
    "            concept_f1_macro=concept_f1_macro,\n",
    "            concept_f1_micro=concept_f1_micro,\n",
    "            concept_f1_weighted=concept_f1_weighted,\n",
    "            label_f1_macro=label_f1_macro,\n",
    "            label_f1_micro=label_f1_micro,\n",
    "            label_f1_weighted=label_f1_weighted,\n",
    "            collapse=collapse,\n",
    "            collapse_hard=collapse_hard,\n",
    "            avg_nll=avg_nll,\n",
    "        )\n",
    "\n",
    "    if dataset_name in [\"shortmnist\"]:\n",
    "        plot_confusion_matrix(\n",
    "            true_concepts,\n",
    "            predicted_concepts,\n",
    "            classes=[i for i in range(10)],\n",
    "            normalize=True,\n",
    "            #title=f\"{model_name}_{dataset_name}_{seed}.pdf\",\n",
    "            title= save_name + f\"_{seed}.pdf\",\n",
    "            is_boia=True,\n",
    "        )\n",
    "    elif dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\"]:\n",
    "\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, :3].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, :3].astype(int)),\n",
    "            [\"\" for i in range(2**3)],\n",
    "            True,\n",
    "            save_name + f\"{model_name}_{dataset_name}_{seed}_forward.pdf\"\n",
    "            #f\"{model_name}_{dataset_name}_{seed}_forward.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 3:9].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 3:9].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            save_name + f\"{model_name}_{dataset_name}_{seed}_stop.pdf\"\n",
    "            #f\"{model_name}_{dataset_name}_{seed}_stop.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 9:15].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 9:15].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            save_name + f\"{model_name}_{dataset_name}_{seed}_left.pdf\"\n",
    "            #f\"{model_name}_{dataset_name}_{seed}_left.pdf\",\n",
    "        )\n",
    "        plot_confusion_matrix(\n",
    "            convert_to_categories(true_concepts[:, 15:21].astype(int)),\n",
    "            convert_to_categories(predicted_concepts[:, 15:21].astype(int)),\n",
    "            [\"\" for i in range(2**6)],\n",
    "            True,\n",
    "            save_name + f\"{model_name}_{dataset_name}_{seed}_right.pdf\"\n",
    "            #f\"{model_name}_{dataset_name}_{seed}_right.pdf\",\n",
    "        )\n",
    "    elif dataset_name in [\"kandinsky\", \"minikandinsky\"]:\n",
    "        shapes = ['Square', 'Circle', 'Triangle']\n",
    "        colors = ['Red', 'Yellow', 'Blue']\n",
    "        class_labels = [f\"({shape}, {color})\" for shape in shapes for color in colors]\n",
    "\n",
    "        plot_confusion_matrix_KAND(\n",
    "            true_concepts=encode_labels_KAND(true_concepts),\n",
    "            predicted_concepts=encode_labels_KAND(predicted_concepts),\n",
    "            classes=[i for i in range(10)],\n",
    "            class_labels=class_labels,\n",
    "            normalize=True,\n",
    "            title=save_name + f\"{model_name}_{dataset_name}_{seed}.pdf\",\n",
    "        )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the right dataset and the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(datasetname, args):\n",
    "    if datasetname.lower() == \"boia\":\n",
    "        return BOIA(args)\n",
    "    if datasetname.lower() == \"boia_original\":\n",
    "        return BDDDataset(args)\n",
    "    if datasetname.lower() == \"boia_original_embedded\":\n",
    "        return FasterBDDOIADataset(args)\n",
    "    if datasetname.lower() == \"sddoia\":\n",
    "        return SDDOIA(args)\n",
    "    if datasetname.lower() == \"minikandinsky\":\n",
    "        return MiniKandinsky(args)\n",
    "    if datasetname.lower() == \"kandinsky\":\n",
    "        return Kandinsky(args)\n",
    "    if datasetname.lower() == \"shortmnist\":\n",
    "        return SHORTMNIST(args)\n",
    "    if datasetname.lower() == \"clipkandinsky\":\n",
    "        return CLIPKandinsky(args)\n",
    "    if datasetname.lower() == \"clipshortmnist\":\n",
    "        return CLIPSHORTMNIST(args)\n",
    "    if datasetname.lower() == \"clipboia\":\n",
    "        return CLIPBOIA(args)\n",
    "    if datasetname.lower() == \"clipSDDOIA\":\n",
    "        return CLIPSDDOIA(args)\n",
    "\n",
    "    raise NotImplementedError(f\"Dataset {datasetname} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(modelname, encoder, args, decoder=None):\n",
    "    if modelname.lower() == \"boiadpl\":\n",
    "        return BoiaDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAdpl\":\n",
    "        return SDDOIADPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boialtn\":\n",
    "        return BOIALTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAltn\":\n",
    "        return SDDOIALTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boiann\":\n",
    "        return BOIAnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAnn\":\n",
    "        return SDDOIAnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"boiacbm\":\n",
    "        return BoiaCBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"SDDOIAcbm\":\n",
    "        return SDDOIACBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"minikanddpl\":\n",
    "        return MiniKandDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandltn\":\n",
    "        return KANDltn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandnn\":\n",
    "        return KANDnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kanddpl\":\n",
    "        return KandDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"prokanddpl\":\n",
    "        return ProKandDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"prokandsl\":\n",
    "        return ProKandSL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"prokandltn\":\n",
    "        return ProKandLTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kanddplsinglejoint\":\n",
    "        return KandDPLSingleJoint(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kanddplsingledisj\":\n",
    "        return KandDPLSingleDisj(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandslsinglejoint\":\n",
    "        return KandSLSingleJoint(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandslsingledisj\":\n",
    "        return KandSLSingleDisj(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandltnsinglejoint\":\n",
    "        return KandLTNSingleJoint(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandltnsingledisj\":\n",
    "        return KandLTNSingleDisj(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"kandcbm\":\n",
    "        return KandCBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == 'kandsl':\n",
    "        return KandSL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == 'promnistsl':\n",
    "        return PROMnistSL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == 'promnistltn':\n",
    "        return PROMnistLTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"promnistdpl\":\n",
    "        return PROMnistDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistdpl\":\n",
    "        return MnistDPL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistltn\":\n",
    "        return MnistLTN(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistsl\":\n",
    "        return MnistSL(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistnn\":\n",
    "        return MNISTnn(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"mnistcbm\":\n",
    "        return MnistCBM(encoder=encoder, args=args)\n",
    "    if modelname.lower() == 'mnistltnrec':\n",
    "        return MnistLTNRec(encoder=encoder, args=args, decoder=decoder)\n",
    "    if modelname.lower() == 'mnistdplrec':\n",
    "        return MnistDPLRec(encoder=encoder, args=args, decoder=decoder)\n",
    "    if modelname.lower() == \"mnistslrec\":\n",
    "        return MnistSLRec(encoder=encoder, args=args, decoder=decoder)\n",
    "    if modelname.lower() == \"shieldedmnist\":\n",
    "        return ShieldedMNIST(encoder=encoder, args=args)\n",
    "    if modelname.lower() == \"proshieldedmnist\":\n",
    "        return ProShieldedMNIST(encoder=encoder, args=args)\n",
    "\n",
    "    raise NotImplementedError(f\"Model {modelname} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ordering for shield: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    backbone=\"conceptizer\",  \n",
    "    preprocess=0,\n",
    "    finetuning=0,\n",
    "    batch_size=8,\n",
    "    n_epochs=10,\n",
    "    validate=0,\n",
    "    dataset=dataset_parameter_name,\n",
    "    lr=0.001,\n",
    "    exp_decay=0.99,\n",
    "    warmup_steps=1,\n",
    "    wandb=None,\n",
    "    task=my_task,\n",
    "    boia_model=\"ce\",\n",
    "    model=model_parameter_name,           \n",
    "    c_sup=0.0,\n",
    "    which_c=[],\n",
    "    joint=False,\n",
    "    boia_ood_knowledge=True,\n",
    "    debug=False,\n",
    "    device='cuda',\n",
    "    GPU_ID=GPU_ID,\n",
    "    hide=hide_parameter,\n",
    "    hide_shapes=hide_shapes_parameter,\n",
    "    hide_colors=hide_colors_parameter,\n",
    "\n",
    "    # & prototypical network\n",
    "    n_support=75,                     # 75\n",
    "    prototypes=False,                  # ^ change this to False to disable prototypical network\n",
    "    prototypical_loss_weight=1.0,\n",
    "    embedding_dim=64,\n",
    "    mlp=True,\n",
    ")\n",
    "\n",
    "# get dataset\n",
    "dataset = get_dataset(args.dataset, args)\n",
    "# get model\n",
    "encoder, decoder = dataset.get_backbone()\n",
    "model = get_model(modelname=args.model, encoder=encoder, args=args, decoder=decoder)\n",
    "\n",
    "model.device = args.device\n",
    "\n",
    "model.to(model.device)\n",
    "if hasattr(model, \"encoder\"):\n",
    "    model.encoder.to(model.device)\n",
    "elif hasattr(model, \"net\"):\n",
    "    model.net.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "# choose the dataset subfolder\n",
    "save_folder = None\n",
    "if args.dataset == 'shortmnist':                                            \n",
    "    save_folder = \"mnadd-even-odd\"\n",
    "if args.dataset == 'kandinsky':                                             \n",
    "    save_folder = \"kandinsky\"      \n",
    "if args.dataset in ['boia', 'boia_original', 'boia_original_embedded']:     \n",
    "    save_folder = \"bddoia\"\n",
    "\n",
    "# choose the model name\n",
    "save_model_name = None\n",
    "if \"ltn\" in args.model:     save_model_name = 'ltn' \n",
    "if \"dpl\" in args.model:     save_model_name = 'dpl'\n",
    "if \"sl\" in args.model:      save_model_name = 'sl' \n",
    "if \"shield\" in args.model:  save_model_name = 'ccn+'\n",
    "\n",
    "save_path = os.path.join(\"..\", \"shortcut_mitigation\", \n",
    "    \"outputs++\",\n",
    "    save_folder, \n",
    "    \"baseline\",\n",
    "    save_model_name,\n",
    "    #f\"episodic-proto-net-pipeline-{uns_parameter_percentage}-HIDE-[]\",\n",
    "    #f\"pretrained-{uns_parameter_percentage}+c\"\n",
    "    \"pretrained-1.0+c\"\n",
    ")\n",
    "\n",
    "model_path = os.path.join(save_path, save_model_name)\n",
    "extractor_path = os.path.join(save_path, \"best\")\n",
    "save_name = model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the dataset and retrive concepts and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_boia(out_labels, out_concepts):\n",
    "    batch_size = out_labels.size(0)\n",
    "\n",
    "    predicted_labels, predicted_concepts = [], []\n",
    "\n",
    "    for idx_batch in range(batch_size):\n",
    "        prob_labels = torch.split(out_labels[idx_batch], 2)\n",
    "        prob_concepts = torch.split(out_concepts[idx_batch], 2)\n",
    "\n",
    "        tmp_lab, tmp_conc = [], []\n",
    "\n",
    "        for l_lab in prob_labels:\n",
    "            tmp_lab.append(torch.argmax(l_lab, dim=0))\n",
    "        for l_conc in prob_concepts:\n",
    "            tmp_conc.append(torch.argmax(l_conc, dim=0))\n",
    "\n",
    "        predicted_labels.append(torch.tensor([tmp_lab]))\n",
    "        predicted_concepts.append(torch.tensor([tmp_conc]))\n",
    "\n",
    "    predicted_labels = torch.concatenate(predicted_labels, dim=0)\n",
    "    predicted_concepts = torch.concatenate(predicted_concepts, dim=0)\n",
    "\n",
    "    return predicted_labels, predicted_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_mnist(\n",
    "    out_labels, out_concepts, true_concepts, is_ood=False\n",
    "):\n",
    "\n",
    "    # filtering out the extended support\n",
    "    if not is_ood:\n",
    "        for i in range(19):\n",
    "            if i in [6, 10, 12]:\n",
    "                continue\n",
    "            out_labels[:, i] = 0\n",
    "\n",
    "    predicted_labels = torch.argmax(out_labels, dim=-1)\n",
    "    predicted_concepts = torch.argmax(out_concepts, dim=-1)\n",
    "\n",
    "    predicted_concepts = predicted_concepts.view(predicted_concepts.numel())\n",
    "    refactored_true_concepts = true_concepts.view(true_concepts.numel())\n",
    "\n",
    "    # print(\"Predicted Labels: \", predicted_labels)\n",
    "    return predicted_labels, predicted_concepts, refactored_true_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_and_labels_kand(out_labels, out_concepts, true_concepts):\n",
    "\n",
    "    # take the prediction\n",
    "    predicted_labels = torch.argmax(out_labels, dim=1)\n",
    "\n",
    "    # stack colors and shapes on top of each other\n",
    "    refactored_true_concepts = torch.split(true_concepts, 1, dim=1)\n",
    "    refactored_true_concepts = torch.concatenate(\n",
    "        refactored_true_concepts, dim=0\n",
    "    ).squeeze(1)\n",
    "\n",
    "    # take the prediction\n",
    "    predicted_concepts_list = torch.split(out_concepts, 3, dim=2)\n",
    "    predicted_concepts = []\n",
    "    # take the argmax\n",
    "    for pc in predicted_concepts_list:\n",
    "        predicted_concepts.append(torch.argmax(pc, dim=2))\n",
    "    predicted_concepts = torch.stack(predicted_concepts, dim=2)\n",
    "\n",
    "    # make them the same dimension as the groundtruth\n",
    "    predicted_concepts = torch.split(predicted_concepts, 1, dim=1)\n",
    "    predicted_concepts = torch.concatenate(predicted_concepts, dim=0).squeeze(1)\n",
    "\n",
    "    return predicted_labels, torch.squeeze(predicted_concepts), refactored_true_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def retrive_concepts_and_labels(model, dataset, dataset_name, \n",
    "        support_images, support_labels, \n",
    "        concept_extractor, resize_transform, args,\n",
    "        total_classes=10, is_ood=False):\n",
    "\n",
    "    true_labels, predicted_labels, true_concepts, predicted_concepts = [], [], [], []\n",
    "\n",
    "    nll_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    for i, data in enumerate(tqdm(dataset)):\n",
    "\n",
    "        # ! BOIA ORIGINAL\n",
    "        images, labels, concepts, imgs_path = None, None, None, None\n",
    "        if args.dataset == 'boia_original':\n",
    "            images, labels, concepts, imgs_path = data\n",
    "        elif args.dataset == 'boia_original_embedded':\n",
    "            images = torch.stack(data['embeddings'])\n",
    "            concepts = torch.stack(data['attr_labels'])\n",
    "            labels = torch.stack(data['class_labels'])[:,:-1]\n",
    "            \n",
    "            assert torch.sum(images == 0) > 0, \"The number of 0 entries in images should be greater than 0\"\n",
    "            assert images.numel() > 0, \"Images tensor is empty\"\n",
    "            assert concepts.numel() > 0, \"Concepts tensor is empty\"\n",
    "            assert labels.numel() > 0, \"Labels tensor is empty\"\n",
    "\n",
    "            images_raw = torch.stack(data['embeddings_raw'])\n",
    "            detected_rois = data['rois']\n",
    "            detected_rois_feats = data['roi_feats']\n",
    "            detection_labels = data['detection_labels']\n",
    "            detection_scores = data['detection_scores']      \n",
    "        else:    \n",
    "            images, labels, concepts = data\n",
    "\n",
    "        images, labels, concepts = (\n",
    "            images.to(model.device),\n",
    "            labels.to(model.device),\n",
    "            concepts.to(model.device),\n",
    "        )\n",
    "\n",
    "        if args.dataset == 'boia_original':\n",
    "            labels = labels[:,:-1]\n",
    "            batch_scene_embs = []\n",
    "            batch_rois = []\n",
    "            batch_rois_feats = []\n",
    "            batch_rois_labels = []\n",
    "            batch_rois_scores = []\n",
    "            for j in range(len(images)):\n",
    "                img_path = imgs_path[j]\n",
    "                save_emb_folder = f\"FASTER-BDDOIA/test/{img_path.split('/')[-1].split('.')[0]}\"\n",
    "\n",
    "                saved_scene_emb_file = os.path.join(save_emb_folder, \"embedded_image.pt\")\n",
    "                #saved_rois_file = os.path.join(save_emb_folder, \"detected_rois.pt\")\n",
    "                #saved_rois_feats_file = os.path.join(save_emb_folder, \"detected_rois_feats.pt\")\n",
    "                #saved_labels_file = os.path.join(save_emb_folder, \"detection_labels.pt\")\n",
    "                #saved_scores_file = os.path.join(save_emb_folder, \"detection_scores.pt\")\n",
    "\n",
    "                saved_scene_emb = torch.load(saved_scene_emb_file)\n",
    "                #saved_rois = torch.load(saved_rois_file)\n",
    "                #saved_rois_feats = torch.load(saved_rois_feats_file)\n",
    "                #saved_rois_labels = torch.load(saved_labels_file)\n",
    "                #saved_rois_scores = torch.load(saved_scores_file)\n",
    "                \n",
    "                batch_scene_embs.append(saved_scene_emb)\n",
    "                #batch_rois.append(saved_rois)\n",
    "                #batch_rois_feats.append(saved_rois_feats)\n",
    "                #batch_rois_labels.append(saved_rois_labels)\n",
    "                #batch_rois_scores.append(saved_rois_scores)\n",
    "                \n",
    "            images = torch.stack(batch_scene_embs).to(model.device)\n",
    "            #batch_rois = torch.stack(batch_rois)\n",
    "            #batch_rois_feats = torch.stack(batch_rois_feats)\n",
    "            #batch_rois_labels = torch.stack(batch_rois_labels)\n",
    "            #batch_rois_scores = torch.stack(batch_rois_scores)\n",
    "\n",
    "            batch_size = images.shape[0]\n",
    "            assert images.shape == (batch_size, 2048), f\"Expected shape [2048], but got {images.shape}\"\n",
    "            assert labels.shape == (batch_size, 4), f\"Expected shape [5], but got {labels.shape}\"\n",
    "            assert concepts.shape == (batch_size, 21), f\"Expected shape [21], but got {concepts.shape}\"\n",
    "\n",
    "\n",
    "        # filtering out the middle rules supervision\n",
    "        if dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "            labels = labels[:, -1]\n",
    "\n",
    "        # ! new for prototypical networks\n",
    "        if args.prototypes and args.task == 'addition':\n",
    "            # Get a random support set.\n",
    "            this_support_images, this_support_labels = get_random_classes(\n",
    "                support_images, support_labels, args.n_support, n_classes=total_classes)\n",
    "            assert this_support_images.shape == (args.n_support * total_classes, 1, 28, 28), \\\n",
    "                f\"Support images shape is not ({args.n_support * total_classes}, 1, 28, 28), but {this_support_images.shape}\"\n",
    "            assert this_support_labels.shape == (args.n_support * total_classes, 1), \\\n",
    "                f\"Support labels shape is not ({args.n_support * total_classes}, 1), but {this_support_labels.shape}\"\n",
    "        \n",
    "            out_dict = model(images, this_support_images, this_support_labels)\n",
    "        elif args.prototypes and args.task == 'patterns':\n",
    "            # Get a random support set.\n",
    "            if NA:\n",
    "                this_support_images, this_support_labels = support_images, support_labels\n",
    "            else:\n",
    "                this_support_images, this_support_labels = get_random_classes(\n",
    "                    support_images, support_labels, args.n_support, n_classes=total_classes)\n",
    "                assert this_support_images.shape == (args.n_support * total_classes, 3, 64, 64), \\\n",
    "                    f\"Support images shape is not ({args.n_support * total_classes}, 3, 64, 64), but {this_support_images.shape}\"\n",
    "                assert this_support_labels.shape == (args.n_support * total_classes, 2), \\\n",
    "                    f\"Support labels shape is not ({args.n_support * total_classes}, 2), but {this_support_labels.shape}\"\n",
    "\n",
    "            out_dict = model(images, concept_extractor, resize_transform, this_support_images, this_support_labels, args)\n",
    "        elif args.model in ['kanddplsinglejoint', 'kanddplsingledisj',\n",
    "                            'kandslsinglejoint',  'kandslsingledisj',\n",
    "                            'kandltnsinglejoint', 'kandltnsingledisj']:\n",
    "            out_dict = model(images, concept_extractor, resize_transform, args)\n",
    "        else:\n",
    "            out_dict = model(images)\n",
    "\n",
    "        out_label, out_concept = None, None\n",
    "        #print(f\"Model logits: {out_dict['YS'][:5]}\")  # First 5 logits\n",
    "        #print(f\"Labels: {labels[:5]}\")  # First 5 labels\n",
    "\n",
    "        if dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "            class_predictions = torch.split(out_dict[\"YS\"], 2, dim=1)\n",
    "            assert len(class_predictions) == 4\n",
    "\n",
    "            loss = 0\n",
    "            for i, _pred in enumerate(class_predictions):\n",
    "                loss += criterion(_pred.float().cpu(), labels[:, i].long().cpu())\n",
    "            loss /= len(class_predictions)\n",
    "        else:\n",
    "            #print(f\"Model output shape: {out_dict['YS'].shape}\")\n",
    "            #print(f\"Target labels shape: {labels.shape}\")\n",
    "            #print(f\"Target labels: {labels}\")\n",
    "            loss = criterion(out_dict[\"YS\"].float().cpu(), labels.long().cpu())\n",
    "\n",
    "        nll_loss += loss.item()\n",
    "\n",
    "        if dataset_name in [\"boia\", \"boia_original\", \"boia_original_embedded\", \"sddoia\", \"clipboia\", \"clipSDDOIA\"]:\n",
    "            out_label, out_concept = get_concepts_and_labels_boia(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"]\n",
    "            )\n",
    "        elif dataset_name in [\"shortmnist\", \"clipshortmnist\"]:\n",
    "            out_label, out_concept, concepts = get_concepts_and_labels_mnist(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"], concepts, is_ood\n",
    "            )\n",
    "        elif dataset_name in [\"kandinsky\", \"minikandinsky\", \"clipkandinsky\"]:\n",
    "            out_label, out_concept, concepts = get_concepts_and_labels_kand(\n",
    "                out_dict[\"YS\"], out_dict[\"pCS\"], concepts\n",
    "            )\n",
    "            #if args.model in [\"kandsl\", \"prokandsl\"]:  #! added for kandsl\n",
    "            #    out_concept = out_concept.view(out_concept.size(0), -1)\n",
    "        \n",
    "        true_labels.append(labels.cpu().numpy())\n",
    "        true_concepts.append(concepts.cpu().numpy())\n",
    "\n",
    "        predicted_labels.append(out_label.detach().cpu().numpy())\n",
    "        predicted_concepts.append(out_concept.cpu().numpy())\n",
    "\n",
    "    # concatenate\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    predicted_labels = np.concatenate(predicted_labels, axis=0)\n",
    "    true_concepts = np.concatenate(true_concepts, axis=0)\n",
    "    predicted_concepts = np.concatenate(predicted_concepts, axis=0)\n",
    "\n",
    "    avg_nll = nll_loss / len(dataset.dataset)\n",
    "\n",
    "    assert true_labels.shape == predicted_labels.shape\n",
    "    assert true_concepts.shape == predicted_concepts.shape\n",
    "\n",
    "    # ^ print(true_concepts.shape) (3000, 6)\n",
    "\n",
    "    return true_labels, predicted_labels, true_concepts, predicted_concepts, avg_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model, test_set, dataset_name, model_name, \n",
    "    support_images, support_labels, \n",
    "    resize_transform, args, use_concept_extractor=False,\n",
    "    total_classes=10, ood_set=None, ood_set_2=None\n",
    "):  # TODO: define attributes\n",
    "\n",
    "    if args.prototypes:\n",
    "        support_images = support_images.to(model.device)\n",
    "        support_labels = support_labels.to(model.device)\n",
    "    \n",
    "    file_name = save_name + \"_results\" + \".txt\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "\n",
    "        # List of metics\n",
    "        in_metrics_list = []\n",
    "        ood_metrics_list = []\n",
    "        ood_metrics_2_list = []\n",
    "\n",
    "        n_files = 0\n",
    "\n",
    "        # Loop through seeds\n",
    "        for seed in seeds:\n",
    "            print(\"Doing\", seed, \"...\")\n",
    "\n",
    "            to_add = \"\"  # \"_joint\" # \"\"\n",
    "            rec = \"\"\n",
    "            print(\"TO ADD\", to_add)\n",
    "\n",
    "            if args.model in ['mnistdplrec', 'mnistltnrec', 'mnistslrec']:\n",
    "                rec = \"rec\"\n",
    "            current_model_path = f\"{model_path}{rec}_{seed}{to_add}.pth\"\n",
    "            \n",
    "            if not os.path.exists(current_model_path):\n",
    "                print(f\"{current_model_path} is missing...\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Loading {current_model_path}...\")\n",
    "\n",
    "            n_files += 1\n",
    "\n",
    "            try:\n",
    "                # retrieve the status dict\n",
    "                model_state_dict = torch.load(current_model_path)\n",
    "                \n",
    "                '''\n",
    "                if args.dataset == 'shortmnist' and hide_parameter:\n",
    "                    model.encoder.missing_classes = args.hide\n",
    "                    model.encoder.num_hidden = len(args.hide)\n",
    "                    model.encoder.unknown_prototypes = model_state_dict['encoder.unknown_prototypes']\n",
    "                    model.encoder.unknown_prototypes.data.copy_(model_state_dict['encoder.unknown_prototypes'])\n",
    "                    model.encoder.unknown_prototypes = model.encoder.unknown_prototypes.to(model.device)\n",
    "                    print(\"Trained unknown prototypes successfully retrieved and loaded\")\n",
    "                    model_state_dict.pop('encoder.unknown_prototypes', None)\n",
    "                '''\n",
    "\n",
    "                if args.dataset == 'kandinsky' and hide_shapes_parameter:\n",
    "                    model.encoder[0].missing_classes = args.hide_shapes\n",
    "                    model.encoder[0].num_hidden = len(args.hide_shapes)\n",
    "                    model.encoder[0].unknown_prototypes = model_state_dict['encoder.0.unknown_prototypes']\n",
    "                    model.encoder[0].unknown_prototypes = model.encoder[0].unknown_prototypes.to(model.device)\n",
    "                    print(\"Trained shapes unknown prototypes successfully retrieved and loaded\")\n",
    "                    model_state_dict.pop('encoder.0.unknown_prototypes', None)\n",
    "                    \n",
    "                if args.dataset == 'kandinsky' and hide_colors_parameter:\n",
    "                    model.encoder[1].missing_classes = args.hide_colors\n",
    "                    model.encoder[1].num_hidden = len(args.hide_colors)\n",
    "                    model.encoder[1].unknown_prototypes = model_state_dict['encoder.1.unknown_prototypes']\n",
    "                    model.encoder[1].unknown_prototypes = model.encoder[1].unknown_prototypes.to(model.device)\n",
    "                    print(\"Trained colors unknown prototypes successfully retrieved and loaded\")\n",
    "                                        \n",
    "                    model_state_dict.pop('encoder.1.unknown_prototypes', None)\n",
    "\n",
    "                # Load the model status dict\n",
    "                if 'shield' in args.model:\n",
    "                    _ = model.shield_layer\n",
    "                    model._shield_layer.to(args.device)\n",
    "                    model.load_state_dict(model_state_dict, strict=True)\n",
    "                else:\n",
    "                    model.load_state_dict(model_state_dict)\n",
    "                    print(model_state_dict.keys())\n",
    "                \n",
    "                concept_extractor = None\n",
    "                if use_concept_extractor:\n",
    "                    current_extractor_path = f\"{extractor_path}_{seed}{to_add}.pt\"\n",
    "                    concept_extractor = YOLO(current_extractor_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if dataset_name == \"shortmnist\":\n",
    "                model = model.float()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            ind_data = retrive_concepts_and_labels(model, test_set, dataset_name, support_images, support_labels, \n",
    "                                concept_extractor, resize_transform, args, total_classes)\n",
    "\n",
    "            if ood_set is not None:\n",
    "                out_data = retrive_concepts_and_labels(\n",
    "                    model, ood_set, dataset_name, is_ood=True\n",
    "                )\n",
    "\n",
    "            if ood_set_2 is not None:\n",
    "                out_data_2 = retrive_concepts_and_labels(\n",
    "                    model, ood_set_2, dataset_name, is_ood=True\n",
    "                )\n",
    "\n",
    "            in_metrics = compute_metrics(*ind_data, dataset_name, model_name, seed, save_name)\n",
    "            in_metrics_list.append(in_metrics)\n",
    "\n",
    "            if ood_set is not None:\n",
    "                ood_metrics = compute_metrics(*out_data, dataset_name, model_name, seed, save_name)\n",
    "                ood_metrics_list.append(ood_metrics)\n",
    "\n",
    "            if ood_set_2 is not None:\n",
    "                ood_metrics_2 = compute_metrics(*out_data_2, dataset_name, model_name, seed, save_name)\n",
    "                ood_metrics_2_list.append(ood_metrics_2)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        assert n_files > 1, \"At least 2 files to compare\"\n",
    "\n",
    "        # Compute standard deviation for each metric\n",
    "        for key in vars(in_metrics_list[0]):  # the key are always the same\n",
    "            # skip hidden elements\n",
    "            if not key.startswith(\"_\"):\n",
    "                # retrieve the list of values\n",
    "                in_metric_values = [getattr(metrics, key) for metrics in in_metrics_list]\n",
    "                ood_metric_values = [getattr(metrics, key) for metrics in ood_metrics_list]\n",
    "                ood_metric_2_values = [\n",
    "                    getattr(metrics, key) for metrics in ood_metrics_2_list\n",
    "                ]\n",
    "\n",
    "                # convert lists to NumPy arrays\n",
    "                in_metric_values_arr = np.array(in_metric_values)\n",
    "                ood_metric_values_arr = np.array(ood_metric_values)\n",
    "                ood_metric_values_2_arr = np.array(ood_metric_2_values)\n",
    "\n",
    "                # Compute the standard deviation\n",
    "                in_metric_std_dev = np.std(in_metric_values_arr)\n",
    "                ood_metric_std_dev = np.std(ood_metric_values_arr)\n",
    "                ood_metric_2_std_dev = np.std(ood_metric_values_2_arr)\n",
    "\n",
    "                # Compute the mean\n",
    "                in_metric_std_mean = np.mean(in_metric_values_arr)\n",
    "                ood_metric_std_mean = np.mean(ood_metric_values_arr)\n",
    "                ood_metric_2_std_mean = np.mean(ood_metric_values_2_arr)\n",
    "\n",
    "                file.write(\n",
    "                    \"\\n{} (In): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                        key.replace(\"_\", \" \").title(),\n",
    "                        round(in_metric_std_mean, 2),\n",
    "                        round(in_metric_std_dev, 2),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if ood_set is not None:\n",
    "                    file.write(\n",
    "                        \"{} (OOD): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                            key.replace(\"_\", \" \").title(),\n",
    "                            round(ood_metric_std_mean, 2),\n",
    "                            round(ood_metric_std_dev, 2),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if ood_set_2 is not None:\n",
    "                    file.write(\n",
    "                        \"{} (OOD 2): ${:.2f} \\pm {:.2f}$\".format(\n",
    "                            key.replace(\"_\", \" \").title(),\n",
    "                            round(ood_metric_2_std_mean, 2),\n",
    "                            round(ood_metric_2_std_dev, 2),\n",
    "                        )\n",
    "                    )\n",
    "    file.close()\n",
    "    print(\"Results saved in \", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data\n",
      "Loading val data\n",
      "Loading test data\n",
      "6000 5040 6000 (6000,)\n",
      "6000 420 6000 (6000,)\n",
      "Doing 0 ...\n",
      "TO ADD \n",
      "Loading ../shortcut_mitigation/outputs++/mnadd-even-odd/baseline/ccn+/supervisions-via-augmentations-1.0/ccn+_0.pth...\n",
      "Custom ordering for shield: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38\n",
      "Using auto mode ::: Detected propositional requirements!\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637ca700>]\n",
      "head 38 with old rules 1\n",
      "head 38 with new rules 1\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637ca940>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637caa60>]\n",
      "head 37 with old rules 2\n",
      "head 37 with new rules 3\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637cad00>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637cae20>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637caf40>]\n",
      "head 36 with old rules 3\n",
      "head 36 with new rules 5\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2370>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2490>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d25b0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d26d0>]\n",
      "head 35 with old rules 4\n",
      "head 35 with new rules 7\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2b80>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2ca0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2dc0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d2ee0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4040>]\n",
      "head 34 with old rules 5\n",
      "head 34 with new rules 9\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4640>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4760>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4880>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d49a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4ac0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d4be0>]\n",
      "head 33 with old rules 6\n",
      "head 33 with new rules 11\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d45e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d7460>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d7580>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d76a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d77c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d78e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d7a00>]\n",
      "head 32 with old rules 7\n",
      "head 32 with new rules 13\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637d7a30>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a3a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a4c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a5e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a700>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a820>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a940>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375aa60>]\n",
      "head 31 with old rules 8\n",
      "head 31 with new rules 15\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246375a970>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375d520>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375d640>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375d760>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375d880>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375d9a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375dac0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375dbe0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375dd00>]\n",
      "head 30 with old rules 9\n",
      "head 30 with new rules 17\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246375daf0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375f8e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fa00>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fb20>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fc40>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fd60>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fe80>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246375ffa0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463762100>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463762220>]\n",
      "head 29 with old rules 10\n",
      "head 29 with new rules 19\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246375fe80>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463762220>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463765040>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463765160>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463765280>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637653a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637654c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637655e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463765700>]\n",
      "head 28 with old rules 9\n",
      "head 28 with new rules 17\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f2463765700>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637672e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463767400>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463767520>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463767640>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463767760>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463767880>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637679a0>]\n",
      "head 27 with old rules 8\n",
      "head 27 with new rules 15\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f24637678b0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463769460>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463769580>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637696a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637697c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f24637698e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f2463769a00>]\n",
      "head 26 with old rules 7\n",
      "head 26 with new rules 13\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f2463769a30>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b3a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b4c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b5e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b700>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b820>]\n",
      "head 25 with old rules 6\n",
      "head 25 with new rules 11\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246376b2b0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376f0a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376f1c0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376f2e0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376f400>]\n",
      "head 24 with old rules 5\n",
      "head 24 with new rules 9\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246376fa00>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376fb20>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376fc40>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376fd60>]\n",
      "head 23 with old rules 4\n",
      "head 23 with new rules 7\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246376c280>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376c3a0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376c4c0>]\n",
      "head 22 with old rules 3\n",
      "head 22 with new rules 5\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246376c8b0>, <pishield.propositional_requirements.constraint.Constraint object at 0x7f246376c9d0>]\n",
      "head 21 with old rules 2\n",
      "head 21 with new rules 3\n",
      "[] [<pishield.propositional_requirements.constraint.Constraint object at 0x7f246376cc40>]\n",
      "head 20 with old rules 1\n",
      "head 20 with new rules 1\n",
      "Generated 1 strata of constraints with [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38] centrality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:09<00:00,  1.72it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 128 ...\n",
      "TO ADD \n",
      "Loading ../shortcut_mitigation/outputs++/mnadd-even-odd/baseline/ccn+/supervisions-via-augmentations-1.0/ccn+_128.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:09<00:00,  1.73it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in  ../shortcut_mitigation/outputs++/mnadd-even-odd/baseline/ccn+/supervisions-via-augmentations-1.0/ccn+_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/users-1/eleonora/anaconda3/envs/r4rr/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Get loaders\n",
    "if args.dataset == 'boia_original':\n",
    "    _, _, test_loader = dataset.get_data_loaders(batch_size=16)\n",
    "elif args.dataset == 'boia_original_embedded':\n",
    "    _, _, test_loader = dataset.get_data_loaders(args)\n",
    "else:\n",
    "    _, _, test_loader = dataset.get_data_loaders()\n",
    "    \n",
    "# Get ood set if it exists\n",
    "ood_loader = getattr(dataset, \"ood_loader_ambulance\", None)\n",
    "# ood_ambulance = getattr(dataset, \"ood_loader_2\", None) # getattr(dataset, \"ood_loader_ambulance\", None)\n",
    "use_concept_extractor = False\n",
    "\n",
    "# ! new for prototypes\n",
    "support_images, support_labels, concept_extractor = None, None, None\n",
    "if args.prototypes and args.task == 'addition':        \n",
    "    tr_dataloader = torch.load('data/prototypes/proto_loader_dataset.pth')\n",
    "    result = get_augmented_support_query_set(tr_dataloader, no_augmentations=NA, debug=False)\n",
    "    support_images, support_labels, query_images, query_labels, no_aug = result\n",
    "    if NA:\n",
    "        support_images = torch.cat([support_images, query_images], dim=0)\n",
    "        support_labels = torch.cat([support_labels, query_labels], dim=0)\n",
    "        assert support_images.size() == torch.Size([20, 1, 28, 28]), f\"support_images_aug size is {support_images.size()}, expected [20, 1, 28, 28]\"\n",
    "        assert support_labels.size() == torch.Size([20, 1]), f\"support_labels_aug size is {support_labels.size()}, expected [20, 1]\"\n",
    "    use_concept_extractor = False\n",
    "    \n",
    "    if args.hide:\n",
    "        # Create a boolean mask using torch.isin (available in recent versions of PyTorch)\n",
    "        mask = ~torch.isin(support_labels, torch.tensor(args.hide))\n",
    "        # Use tensor indexing to filter both images and labels\n",
    "        support_images = support_images[mask]\n",
    "        support_labels = support_labels[mask]\n",
    "\n",
    "        # Ensure the channel dimension is preserved\n",
    "        if support_images.dim() == 3:  # If shape is [batch_filtered, 28, 28]\n",
    "            support_images = support_images.unsqueeze(1)  # Add back the channel dim to get [batch_filtered, 1, 28, 28]\n",
    "        if support_labels.dim() == 1:  # If shape is [batch_filtered]\n",
    "            support_labels = support_labels.unsqueeze(1)  # Add back the channel dim to get [batch_filtered, 1]\n",
    "\n",
    "elif args.prototypes and args.task == 'patterns':\n",
    "    support_images = torch.load('data/kand_annotations/pnet_proto/concept_prototypes.pt')\n",
    "    support_labels = torch.load('data/kand_annotations/pnet_proto/labels_prototypes.pt')\n",
    "    if NA:\n",
    "        print(\"Excluding Augmentations...\")\n",
    "        pairs = {(s, c): (support_labels[:, 0] == s) & (support_labels[:, 1] == c)\n",
    "                for s in range(3) for c in range(3)}\n",
    "        pair_indices = {k: v.nonzero(as_tuple=True)[0].tolist() for k, v in pairs.items()}\n",
    "        selected_indices = []\n",
    "        for s in range(3):\n",
    "            for c in range(3):\n",
    "                idxs = pair_indices[(s, c)]\n",
    "                assert len(idxs) > 0, f\"No samples found for pair ({s},{c})\"\n",
    "                chosen = random.choice(idxs)\n",
    "                selected_indices.append(chosen)\n",
    "        support_images = support_images[selected_indices]\n",
    "        support_labels = support_labels[selected_indices]\n",
    "        print(\"Selected indices:\", selected_indices)\n",
    "        print(\"Sampled labels:\\n\", support_labels)\n",
    "        print(support_images.shape)\n",
    "        print(support_labels.shape)\n",
    "    use_concept_extractor = True\n",
    "    my_yolo_project_path = f\"ultralytics-{args.GPU_ID}/\"\n",
    "    args.yolo_folder = my_yolo_project_path\n",
    "    \n",
    "elif args.model in ['kanddplsinglejoint', 'kanddplsingledisj',\n",
    "                    'kandltnsinglejoint', 'kandltnsingledisj',\n",
    "                    'kandslsinglejoint', 'kandslsingledisj']:\n",
    "    use_concept_extractor = True\n",
    "    my_yolo_project_path = f\"ultralytics-{args.GPU_ID}/\"\n",
    "    args.yolo_folder = my_yolo_project_path\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    args.dataset,\n",
    "    model_name=args.model,\n",
    "    support_images=support_images,\n",
    "    support_labels=support_labels,\n",
    "    use_concept_extractor=use_concept_extractor,\n",
    "    resize_transform=T.Resize((64, 64)),\n",
    "    args=args,\n",
    "    ood_set=ood_loader,\n",
    "    total_classes=np.unique(support_labels).size,\n",
    "    ood_set_2=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r4rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
